{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b46fe41"
      },
      "source": [
        "<div align=center>\n",
        "\n",
        "<font size=5>\n",
        "    In the Name of God\n",
        "<font/>\n",
        "<br/>\n",
        "<br/>\n",
        "<font>\n",
        "    Sharif University of Technology - Departmenet of Electrical Engineering\n",
        "</font>\n",
        "<br/>\n",
        "<font>\n",
        "    Introducing with Machine Learing - Dr. S. Amini\n",
        "</font>\n",
        "<br/>\n",
        "<br/>\n",
        "Spring 2023\n",
        "\n",
        "</div>\n",
        "\n",
        "<hr/>\n",
        "<div align=center>\n",
        "<font size=6>\n",
        "    Neural Networks Practical Assignment\n",
        "    \n",
        "    Question 1\n",
        "</font>\n",
        "<br/>\t\t\n",
        "<font size=4>\n",
        "<br/>\n",
        "</div>"
      ],
      "id": "1b46fe41"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24a0fc13"
      },
      "source": [
        "# Personal Data"
      ],
      "id": "24a0fc13"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "44babb65"
      },
      "outputs": [],
      "source": [
        "# Set your student number\n",
        "student_number = 99101608\n",
        "Name = 'Melika'\n",
        "Last_Name = 'Rajabi'"
      ],
      "id": "44babb65"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca4a337a"
      },
      "source": [
        "# Rules\n",
        "- You are not allowed to add or remove cells. You **must use the provided space to write your code**. If you don't follow this rule, **your Practical Assignment won't be graded**.  \n",
        "\n",
        "- Collaboration and using the internet is allowed, but your code **must be written by yourself**. **Copying code** from each other or from available resources will result in a **zero score for the assignment**.\n",
        "\n",
        "- You are not allowed to use `torch.nn`, `torch.optim` and any activation function and loss function implemented in torch. "
      ],
      "id": "ca4a337a"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12b76789",
        "outputId": "9d8d1966-6003-4777-e7b3-5436f72b86df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.0.1+cu118)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision) (16.0.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchvision) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchvision) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install torchvision\n",
        "!pip install torch"
      ],
      "id": "12b76789"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2Hwd3Xckhu4"
      },
      "source": [
        "The exclamation mark (!) is used to run shell commands within the notebook environment. When using the exclamation mark before a command, it indicates that the command should be executed in the underlying system shell rather than being interpreted as a Python command."
      ],
      "id": "g2Hwd3Xckhu4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "886188c7"
      },
      "source": [
        "## Importing Libraries"
      ],
      "id": "886188c7"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "55a0adcc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from typing import Dict"
      ],
      "id": "55a0adcc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18510868"
      },
      "source": [
        "## Datasets and Dataloaders\n",
        "\n",
        "Here, we download and load the train and test `FashionMNIST` dataset with the desired transforms. Then, we define the dataloaders for `train` and `test`."
      ],
      "id": "18510868"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dc8759e2"
      },
      "outputs": [],
      "source": [
        "train_set = FashionMNIST(root='.', train=True, download=True, transform=transforms.ToTensor())\n",
        "test_set = FashionMNIST(root='.', train=False, download=True, transform=transforms.ToTensor())"
      ],
      "id": "dc8759e2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COx1w5lzlcrz"
      },
      "source": [
        "root='.': Specifies the root directory where the dataset will be stored. In this case, it will be stored in the current directory.\n",
        "\n",
        "train=True or False: Indicates that we want to load the training data or the test data.\n",
        "\n",
        "download=True: Specifies that if the dataset is not already downloaded, it should be downloaded from the internet.\n",
        "\n",
        "transform=transforms.ToTensor(): Applies the ToTensor() transformation to the dataset, converting the images to PyTorch tensors, which is the expected format for many machine learning models.\n"
      ],
      "id": "COx1w5lzlcrz"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8f6763e6"
      },
      "outputs": [],
      "source": [
        "image_shape = train_set[0][0].shape\n",
        "input_dim = np.prod(image_shape).item()\n",
        "num_classes = len(FashionMNIST.classes)"
      ],
      "id": "8f6763e6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRLVmJvln0Bb"
      },
      "source": [
        "Train set contains $60000$, $28 \\times 28$ images plus a label for each image. Therefore, train_set is actually $60000 \\times 2$. Each train_set[i][0] (image_shape) is $28 \\times 28$ and train_set[i][1] is an integer indicating the label.\n",
        "\n",
        "np.prod multiplies all the elements in an array and item() converts it into scalar. Therefore input_dim = $28 \\times 28 = 784$.\n",
        "\n",
        "FashionMNIST.classes represents a list of class labels in the FashionMNIST dataset. Therefore, num_classes equals the number of classes in the providing dataset. i.e. $10$."
      ],
      "id": "SRLVmJvln0Bb"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "c695ff60"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_set, 64, shuffle=True)\n",
        "test_loader = DataLoader(test_set, 64, shuffle=False)"
      ],
      "id": "c695ff60"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n3xPa9trKfL"
      },
      "source": [
        "Data loaders are a convenient way to load and iterate over the data in batches during the training or testing phase of a machine learning model. \n",
        "\n",
        "First we determine which dataset is going to be loaded. Then we provide the number that specifies the batch size (n), indicating that the data will be divided into batches of n samples. shuffle=True means that the data will be randomly shuffled before each epoch. Shuffling the data helps in reducing any bias that may arise from the ordering of the data during training."
      ],
      "id": "8n3xPa9trKfL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9dac6c2"
      },
      "source": [
        "## Visualization\n",
        "\n",
        "Visualize 1 random image from each class\n",
        "\n",
        "- **Hint**:  You can use `plt.subplots` for visualization"
      ],
      "id": "f9dac6c2"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "e3d6b0c1",
        "outputId": "bf3f2b58-c89d-42ad-f4e5-c26342885fe6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "## FILL HERE\n",
        "\n",
        "sample_images = {}\n",
        "for image, label in train_set:\n",
        "  if label not in sample_images:\n",
        "    sample_images[label] = image.squeeze()\n",
        "  if len(sample_images) == num_classes:\n",
        "    break\n",
        "\n",
        "figure = plt.figure()\n",
        "rows, cols = 2, 5\n",
        "for label, image in sample_images.items():\n",
        "  figure.add_subplot(rows, cols, label+1)\n",
        "  plt.imshow(image, cmap='gray')\n",
        "  plt.title(FashionMNIST.classes[label])\n",
        "  plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "id": "e3d6b0c1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brNSmy1utnHG"
      },
      "source": [
        "We do the visualization in $2$ steps. First we choose the samples and then plot them. \n",
        "\n",
        "To choose the samples, we iterate over the first elements of the train set and gather elements with new labels. If the number of gathered elements reach the number of classes, we stop the iteration cause that means none of the nonobserved elements has new label. Squeeze() removes single-dimensional entries from the shape of an array.\n",
        "\n",
        "Now by using \"imshow\" command, we plot the images. We also set the color to gray. "
      ],
      "id": "brNSmy1utnHG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a94c5aba"
      },
      "source": [
        "## Initializing model's parameters\n",
        "\n",
        "In this part, we create the model and initialize its parameters and store the values of these parameters in the variable `parameters` which is a dictionary including the weigths and biases of each layer."
      ],
      "id": "a94c5aba"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "e6d40952"
      },
      "outputs": [],
      "source": [
        "\n",
        "def add_linear_layer(parameters: dict, shape, device, i=None):\n",
        "  \"\"\"\n",
        "  This function adds parameters of a linear unit of shape `shape` to the `parameters` dictionary.\n",
        "  \"\"\"\n",
        "\n",
        "  n_in, n_out = shape\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    w = torch.zeros(*shape, device=device)\n",
        "    # Kaiming initialization for ReLU activations:\n",
        "    bound = 1 / np.sqrt(n_in).item()\n",
        "    w.uniform_(-bound, bound)\n",
        "    b = torch.zeros(n_out, device=device)  # No need to (1, n_out). It will broadcast itself.\n",
        "    \n",
        "  w.requires_grad = True\n",
        "  b.requires_grad = True\n",
        "\n",
        "  # `i` is used to give numbers to parameter names\n",
        "  parameters.update({f'w{i}': w, f'b{i}': b})\n",
        "    "
      ],
      "id": "e6d40952"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anCNHzWC9Kjg"
      },
      "source": [
        "This function initializes the weights and biases for a linear unit, and adds them to a dictionary of parameters. It follows the Kaiming initialization strategy for the weights and initializes the biases as zeros.\n",
        "\n",
        "parameters is a dictionary to store the parameters, shape is a tuple representing the shape of the linear unit, device is the device on which the tensors will be created, and an optional parameter i is used to give numbers to parameter names.\n",
        "\n",
        "n_in, n_out = shape $$ Unpacks the shape tuple into two variables. n_in represents the input size, and n_out represents the output size of the linear unit.\n",
        "\n",
        "with torch.no_grad() $$ Starts a context block where operations inside it will not track gradients. This is used for efficiency when initializing the parameters.\n",
        "\n",
        "w = torch.zeros(*shape, device=device) $$ Creates a tensor filled with zeros, with the shape specified by the shape tuple, and places it on the specified device.\n",
        "\n",
        "bound = 1 / np.sqrt(n_in).item() $$ Calculates the bound for the uniform distribution used for initializing the weights.\n",
        "\n",
        "w.uniform_(-bound, bound) $$ Initializes the weights (w) with values drawn from a uniform distribution between -bound and bound. This is a common initialization strategy called Kaiming initialization, often used for ReLU activations.\n",
        "\n",
        "b = torch.zeros(n_out, device=device) $$ Creates a tensor filled with zeros of shape (n_out,), representing the biases for the linear unit.\n",
        "\n",
        "w.requires_grad = True and b.requires_grad = True $$ mark the tensors w and b as requiring gradients, which means that their gradients will be computed during backpropagation.\n",
        "\n",
        "parameters.update({f'w{i}': w, f'b{i}': b}) $$ Updates the parameters dictionary by adding the weight tensor w with the key f'w{i}' (where i is an optional index) and the bias tensor b with the key f'b{i}'."
      ],
      "id": "anCNHzWC9Kjg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce914706"
      },
      "source": [
        "Now we define our neural network with the given layers and add the weights and biases to the dictionary `parameters`. **You are allowed to modify the values of the layers**."
      ],
      "id": "ce914706"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f3867d7",
        "outputId": "88a41d49-f3b2-4da7-af4a-2b180c1c560a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['w0', 'b0', 'w1', 'b1', 'w2', 'b2', 'w3', 'b3', 'w4', 'b4'])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "\n",
        "layers = [\n",
        "    (input_dim, 512),\n",
        "    (512, 256),\n",
        "    (256, 128),\n",
        "    (128, 64),\n",
        "    (64, num_classes)\n",
        "]\n",
        "num_layers = len(layers)\n",
        "parameters = {}\n",
        "\n",
        "# Setting the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Adding the parameters to the dictionary\n",
        "for i, shape in enumerate(layers):\n",
        "  add_linear_layer(parameters, shape, device, i)\n",
        "\n",
        "parameters.keys()\n"
      ],
      "id": "8f3867d7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rk-RLZ9EP8yt"
      },
      "source": [
        "This code sets up a neural network with multiple linear layers based on the provided layer shapes.\n",
        "\n",
        "First it defines a list called layers that contains tuples representing the shape of each layer in the neural network. Each tuple represents the input size and output size of a linear layer.\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") $$ Sets the device variable to \"cuda\" if a CUDA-enabled GPU is available, otherwise it sets it to \"cpu\". This is used to determine the device (GPU or CPU) on which the tensors and computations will be performed. \n",
        "\n",
        "In the next step, it starts a loop that iterates over the layers list and assigns each tuple to the variables i and shape. The enumerate() function provides both the index (i) and the value (shape) of each element in the layers list. And then by using the defined function, it adds the parameters of the linear layer to the parameters dictionary.\n",
        "\n",
        "In the end, it displays the keys of the parameters dictionary, which represent the parameter names."
      ],
      "id": "rk-RLZ9EP8yt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bfd2c8e"
      },
      "source": [
        "## Defining the required functions\n",
        "\n",
        "In this section, we should define the required functions. For each of these functions, the inputs and the desired outputs are given and you should write all or part of the function. **You are not allowed to use the activation functions and the loss functions implemented in torch**."
      ],
      "id": "8bfd2c8e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3b413d8"
      },
      "source": [
        "Computing affine and relu outputs:"
      ],
      "id": "f3b413d8"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "bebeeb0e"
      },
      "outputs": [],
      "source": [
        "\n",
        "def affine_forward(x, w, b):\n",
        "  ## FILL HERE\n",
        "\n",
        "  out = torch.mm(x, w) \n",
        "  out = torch.add(out, b)  \n",
        "  return out\n",
        "\n",
        "\n",
        "def relu(x):\n",
        "  ## FILL HERE\n",
        "  \n",
        "  zeros = torch.zeros_like(x)\n",
        "  out = torch.max(x, zeros)\n",
        "  return out\n"
      ],
      "id": "bebeeb0e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lsc0ZIrUT5Fb"
      },
      "source": [
        "For implementing the affine_forward function, we used torch \"mm\" and \"add\" commands to multiply x and w and then add the result to b. we set the broadcast True to provide size compatibilities.\n",
        "\n",
        "To implement the ReLU function, we first create a zero tensor with the shape like the input x and then by using the element wise maximum between two tensors (input and zeros), we result in the output."
      ],
      "id": "Lsc0ZIrUT5Fb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d9baa5e"
      },
      "source": [
        "Function `model` returns output of the whole model for the input `x` using the parameters:"
      ],
      "id": "5d9baa5e"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "d2562962"
      },
      "outputs": [],
      "source": [
        "\n",
        "def model(x: torch.Tensor, parameters, num_layers=num_layers):\n",
        "  # Number of batches\n",
        "  B = x.shape[0]\n",
        "  x = x.view(B, -1)\n",
        "\n",
        "  # FILL HERE\n",
        "  \n",
        "  output = x\n",
        "  for i in range(num_layers):\n",
        "\n",
        "    w = parameters[f'w{i}']\n",
        "    b = parameters[f'b{i}']\n",
        "\n",
        "    output = affine_forward(output, w, b)\n",
        "\n",
        "    if i != num_layers-1:\n",
        "      output = relu(output)\n",
        "\n",
        "  return output\n"
      ],
      "id": "d2562962"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paMpWi1uL2hx"
      },
      "source": [
        "To implement the model function that returns the output of the model for the given input, we should apply affine_forward and relu functions for each layer based on that layer's parameters. Therefore, we iterate over the layers, find the parameters corresponding to that layer from the parameters tensor, apply affine_forward and relu respectively. Note that for the last layer, we do not apply relu.\n",
        "\n",
        "Note that before starting the algorithm we convert the input images into vectors. "
      ],
      "id": "paMpWi1uL2hx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d17a9b4c"
      },
      "source": [
        "Implementing cross entropy loss:"
      ],
      "id": "d17a9b4c"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6959621c"
      },
      "outputs": [],
      "source": [
        "\n",
        "def cross_entropy_loss(scores, y):\n",
        "  n = len(y)\n",
        "\n",
        "  # FILL HERE\n",
        "\n",
        "  scores_exp = torch.exp(scores)\n",
        "  probs = scores_exp / torch.sum(scores_exp, axis=1, keepdims=True)\n",
        "\n",
        "  loss = -torch.log(probs[torch.arange(n), y])\n",
        "    \n",
        "  return loss\n"
      ],
      "id": "6959621c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJJQ1fqtqSqV"
      },
      "source": [
        "In our context, The cross entropy loss function for each sample equals $-\\log (\\text{predicted probability for the correct class for the sample})$. \n",
        "\n",
        "Here we have the scores which is a $n \\times C$ matrix. Each row represents the  predicted scores for the provided sample to belong to each class from 0 to C-1. To convert the scores into probability, we apply softmax on each row separately: \n",
        "\n",
        "```\n",
        "scores_exp = torch.exp(scores)\n",
        "```\n",
        "This line calculates the $e^{score}$ for each sample and each class.\n",
        "\n",
        "```\n",
        "torch.sum(scores_exp, axis=1, keepdims=True)\n",
        "```\n",
        "This line calculates the sum of each row and returns a $n \\times 1$ matrix.\n",
        "\n",
        "Now if we devide them to each other, we result in the score matrix converted into probability matrix. \n",
        "\n",
        "For accessing the predicted probability of the correct class in each sample, we use \"[torch.arange(n), y]\" indexing. This means that for each row, choose the correct class index. \n",
        "\n",
        "In the end we compute the $-log$ to result in a $n \\times 1$ matrix containing the cross entropy loss for each sample.\n"
      ],
      "id": "tJJQ1fqtqSqV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15a589af"
      },
      "source": [
        "Implementing a function for optimizing paramters and a function to zeroing out their gradients:"
      ],
      "id": "15a589af"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3121c147"
      },
      "outputs": [],
      "source": [
        "\n",
        "def sgd_optimizer(parameters: Dict[str, torch.Tensor], learning_rate=0.001):\n",
        "  '''This function gets the parameters and a learning rate. Then updates the parameters using their\n",
        "  gradient (parameter.grad). Finally, you should zero the gradients of the parameters after updating\n",
        "  the parameter value.'''\n",
        "  ## FILL HERE\n",
        "\n",
        "  for param in parameters.values():\n",
        "\n",
        "    param.data -= learning_rate * param.grad\n",
        "\n",
        "    param.grad.zero_()\n"
      ],
      "id": "3121c147"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPe2sO5-6Ppj"
      },
      "source": [
        "For each parameter in the parameters tensor, we pick the value and then using learning rate and its gradient, we update it. Then by using the \"grad.zero_()\" command, we make its gradient zero."
      ],
      "id": "TPe2sO5-6Ppj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e17b4cf8"
      },
      "source": [
        "Training functions:"
      ],
      "id": "e17b4cf8"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "76c0f03b"
      },
      "outputs": [],
      "source": [
        "\n",
        "def accuracy(y_pred: np.ndarray, y_true: np.ndarray):\n",
        "  ## FILL HERE\n",
        "\n",
        "  correct = np.equal(y_pred, y_true)\n",
        "  acc = np.sum(correct) / len(y_true)\n",
        "\n",
        "  return acc\n",
        "\n",
        "\n",
        "def train(train_loader, learning_rate=0.001, epoch=None):\n",
        "  '''This function implements the training loop for a single epoch. For each batch you should do the following:\n",
        "      1- Calculate the output of the model to the given input batch\n",
        "      2- Calculate the loss based on the model output\n",
        "      3- Update the gradients using loss.backward() method\n",
        "      4- Optimize the model parameters using the sgd_optimizer function defined previously\n",
        "      5- Print the train loss (Show the epoch and batch as well)\n",
        "      '''\n",
        "\n",
        "  train_loss = 0\n",
        "  N_train = len(train_loader.dataset)\n",
        "   \n",
        "  # Creating empty lists Y and Y_pred to store the labels and predictions of each batch\n",
        "  # for calculateing the accuracy later\n",
        "  Y = []\n",
        "  Y_pred = []\n",
        "    \n",
        "    \n",
        "  for i, (x, y) in enumerate(train_loader):\n",
        "\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    p = model(x, parameters)\n",
        "\n",
        "    ## FILL HERE\n",
        "\n",
        "    loss = cross_entropy_loss(p, y)\n",
        "    train_loss += loss.mean().item()\n",
        "\n",
        "    loss.mean().backward()\n",
        "\n",
        "    sgd_optimizer(parameters, learning_rate=0.001)\n",
        "        \n",
        "    y_pred = p.argmax(dim=-1)\n",
        "\n",
        "    Y.append(y.cpu().numpy())\n",
        "    Y_pred.append(y_pred.cpu().numpy())\n",
        "\n",
        "    print(f\"Epoch [{epoch}], Batch [{i+1}], Train Loss: {loss.mean().item()}\")\n",
        "\n",
        "  Y = np.concatenate(Y)\n",
        "  Y_pred = np.concatenate(Y_pred)\n",
        "\n",
        "  train_loss /= N_train\n",
        "\n",
        "  acc = accuracy(Y_pred, Y)\n",
        "\n",
        "  print(f'Accuracy of train set: {acc}')\n",
        "\n",
        "  return train_loss, acc\n",
        "\n",
        "\n",
        "def validate(loader, epoch=None, set_name=None):\n",
        "  '''This function validates the model on the test dataloader. The function goes through each batch and does\n",
        "  the following on each batch:\n",
        "    1- Calculate the model output\n",
        "    2- Calculate the loss using the model output\n",
        "    3- Print the loss for each batch and epoch\n",
        "    \n",
        "  Finally the function calculates the model accuracy.'''\n",
        "\n",
        "  total_loss = 0\n",
        "  N = len(loader.dataset)\n",
        "    \n",
        "  # Creating empty lists Y and Y_pred to store the labels and predictions of each batch\n",
        "  # for calculateing the accuracy later\n",
        "  Y = []\n",
        "  Y_pred = []\n",
        "\n",
        "\n",
        "  for i, (x, y) in enumerate(loader):\n",
        "\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    p = model(x, parameters)\n",
        "\n",
        "    ## FILL HERE\n",
        "\n",
        "    loss = cross_entropy_loss(p, y)\n",
        "    total_loss += loss.mean().item()\n",
        "        \n",
        "    y_pred = p.argmax(dim=-1)\n",
        "\n",
        "    Y.append(y.cpu().numpy())\n",
        "    Y_pred.append(y_pred.cpu().numpy())\n",
        "        \n",
        "    print(f\"Epoch [{epoch+1}], Batch [{i+1}], Loss: {loss.mean().item()}\")\n",
        "\n",
        "  Y = np.concatenate(Y)\n",
        "  Y_pred = np.concatenate(Y_pred)\n",
        "\n",
        "  total_loss /= N\n",
        "\n",
        "  acc = accuracy(Y_pred, Y)\n",
        "\n",
        "  print(f'Accuracy of {set_name} set: {acc}')\n",
        "\n",
        "  return total_loss, acc\n"
      ],
      "id": "76c0f03b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_tu-Z9o8gqY"
      },
      "source": [
        "For the accuracy function, first we compute the number of correct predicted labels using \"np.equal\". Then by deviding this number to the total samples, we result in the output.\n",
        "\n",
        "In the train function, we iterate over the batches, performing following tasks:\n",
        "\n",
        "1. Caculating the output of the model for each x. x is a tensor including $64$ elements (number of samples in each batch), each containing a $28 \\times 28$ image. \n",
        "\n",
        "2. Storing the y tensor which indicates the labels of these $64$ images\n",
        "\n",
        "3. Calculating the loss using cross entropy function and then calculating the mean of the loss vector and add it to the train loss \n",
        "\n",
        "4. Updating the gradients using loss.backward() method\n",
        "\n",
        "5. Optimizing the model parameters using the sgd_optimizer function defined previously.\n",
        "\n",
        "6. Finding the predicted labels by using \"argmax\" on the scores.\n",
        "\n",
        "7. Printing the epoch and the batch we are in. And also the loss of each batch we've calculated to add to the train loss.\n",
        "\n",
        "Then we add the predicted and the true labels to the main Y and Y_pred using \"concatenate\" command. In the next step , we compute the accuracy using our defined function. And in the end, we return the total train loss that is the train_loss devided by N_train and the accuracy.\n",
        "\n",
        "In the validate function, we act very similar to the train function but using the test loader and also we won't perform the steps number $4$ and $5$.\n"
      ],
      "id": "H_tu-Z9o8gqY"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "87ebb4b6"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "test_losses = []\n",
        "train_accuracies = []\n",
        "test_accuracies = []"
      ],
      "id": "87ebb4b6"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "28d4eb0b"
      },
      "outputs": [],
      "source": [
        "def train_model(dataloaders, num_epochs, learning_rate=0.001, model_name='pytorch_model'):\n",
        "  '''This function trains the model for the number of epochs given and stores, calculates and prints the train\n",
        "  and test losses and accuracies. Finally, it plots the accuracy and loss history for training and test sets'''\n",
        "  \n",
        "  train_loader, test_loader = dataloaders\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    ## FILL HERE\n",
        "    ## You should calculate the train and test loss and accuracies for each epoch and add them to\n",
        "    ## the lists `train_losses`, `test_losses`, `train_accuracies` and `test_accuracies`\n",
        "\n",
        "    train_loss, train_acc = train(train_loader, learning_rate, epoch)\n",
        "    test_loss, test_acc = validate(test_loader, epoch, set_name=None)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    test_losses.append(test_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    test_accuracies.append(test_acc)\n",
        "\n",
        "  ## Plot the loss history of training and test sets \n",
        "  ## FILL HERE\n",
        "\n",
        "  plt.plot(train_losses, label='Train')\n",
        "  plt.plot(test_losses, label='Test')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.title(f'{model_name} - Loss History')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  ## Plot the accuracy history of training and test sets\n",
        "  ## FILL HERE\n",
        "\n",
        "  plt.plot(train_accuracies, label='Train')\n",
        "  plt.plot(test_accuracies, label='Test')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.title(f'{model_name} - Accuracy History')\n",
        "  plt.legend()\n",
        "  plt.show()\n"
      ],
      "id": "28d4eb0b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI2E_3s-59P_"
      },
      "source": [
        "For implementing this function, first of all using the train and validate functions that we've implemented, we compute the train and test loss and accuracy. Then we add these numbers to the main lists created for each. In the end we plot these four lists that indicate the loss and accuracy hostory of train and test sets. "
      ],
      "id": "fI2E_3s-59P_"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "2ec4bdd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "83cc6803-af73-44b1-d741-30ea5b9babb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch [20], Batch [486], Train Loss: 0.7818667888641357\n",
            "Epoch [20], Batch [487], Train Loss: 0.8088809847831726\n",
            "Epoch [20], Batch [488], Train Loss: 0.7608166933059692\n",
            "Epoch [20], Batch [489], Train Loss: 0.8933385014533997\n",
            "Epoch [20], Batch [490], Train Loss: 1.0034886598587036\n",
            "Epoch [20], Batch [491], Train Loss: 0.7688308954238892\n",
            "Epoch [20], Batch [492], Train Loss: 0.7898273468017578\n",
            "Epoch [20], Batch [493], Train Loss: 0.8562772870063782\n",
            "Epoch [20], Batch [494], Train Loss: 0.8634331822395325\n",
            "Epoch [20], Batch [495], Train Loss: 0.6159061193466187\n",
            "Epoch [20], Batch [496], Train Loss: 0.8876675367355347\n",
            "Epoch [20], Batch [497], Train Loss: 0.9176874756813049\n",
            "Epoch [20], Batch [498], Train Loss: 0.8345860242843628\n",
            "Epoch [20], Batch [499], Train Loss: 0.9353675842285156\n",
            "Epoch [20], Batch [500], Train Loss: 0.7026399970054626\n",
            "Epoch [20], Batch [501], Train Loss: 0.7803460359573364\n",
            "Epoch [20], Batch [502], Train Loss: 0.8276726007461548\n",
            "Epoch [20], Batch [503], Train Loss: 0.8967006206512451\n",
            "Epoch [20], Batch [504], Train Loss: 1.0083976984024048\n",
            "Epoch [20], Batch [505], Train Loss: 0.8149846196174622\n",
            "Epoch [20], Batch [506], Train Loss: 0.7167524099349976\n",
            "Epoch [20], Batch [507], Train Loss: 0.761496901512146\n",
            "Epoch [20], Batch [508], Train Loss: 0.8056510090827942\n",
            "Epoch [20], Batch [509], Train Loss: 0.8049895763397217\n",
            "Epoch [20], Batch [510], Train Loss: 0.6946335434913635\n",
            "Epoch [20], Batch [511], Train Loss: 0.7539917230606079\n",
            "Epoch [20], Batch [512], Train Loss: 0.7682453393936157\n",
            "Epoch [20], Batch [513], Train Loss: 0.9889957904815674\n",
            "Epoch [20], Batch [514], Train Loss: 0.8666792511940002\n",
            "Epoch [20], Batch [515], Train Loss: 0.847524881362915\n",
            "Epoch [20], Batch [516], Train Loss: 1.0222654342651367\n",
            "Epoch [20], Batch [517], Train Loss: 0.7889975309371948\n",
            "Epoch [20], Batch [518], Train Loss: 1.004703164100647\n",
            "Epoch [20], Batch [519], Train Loss: 0.7149946093559265\n",
            "Epoch [20], Batch [520], Train Loss: 0.8339847326278687\n",
            "Epoch [20], Batch [521], Train Loss: 0.8992536067962646\n",
            "Epoch [20], Batch [522], Train Loss: 0.9526050686836243\n",
            "Epoch [20], Batch [523], Train Loss: 0.8592038750648499\n",
            "Epoch [20], Batch [524], Train Loss: 0.7923258543014526\n",
            "Epoch [20], Batch [525], Train Loss: 0.7861875295639038\n",
            "Epoch [20], Batch [526], Train Loss: 0.8948237895965576\n",
            "Epoch [20], Batch [527], Train Loss: 0.8338564038276672\n",
            "Epoch [20], Batch [528], Train Loss: 0.7655743956565857\n",
            "Epoch [20], Batch [529], Train Loss: 0.8100336790084839\n",
            "Epoch [20], Batch [530], Train Loss: 0.7719893455505371\n",
            "Epoch [20], Batch [531], Train Loss: 0.6136651039123535\n",
            "Epoch [20], Batch [532], Train Loss: 0.7343587875366211\n",
            "Epoch [20], Batch [533], Train Loss: 1.0497980117797852\n",
            "Epoch [20], Batch [534], Train Loss: 0.7871451377868652\n",
            "Epoch [20], Batch [535], Train Loss: 0.8129259943962097\n",
            "Epoch [20], Batch [536], Train Loss: 0.7588570713996887\n",
            "Epoch [20], Batch [537], Train Loss: 0.7843430638313293\n",
            "Epoch [20], Batch [538], Train Loss: 0.7860217094421387\n",
            "Epoch [20], Batch [539], Train Loss: 0.7263767719268799\n",
            "Epoch [20], Batch [540], Train Loss: 0.7514411807060242\n",
            "Epoch [20], Batch [541], Train Loss: 0.8272410035133362\n",
            "Epoch [20], Batch [542], Train Loss: 0.866633415222168\n",
            "Epoch [20], Batch [543], Train Loss: 0.8492111563682556\n",
            "Epoch [20], Batch [544], Train Loss: 0.7331359386444092\n",
            "Epoch [20], Batch [545], Train Loss: 1.0147600173950195\n",
            "Epoch [20], Batch [546], Train Loss: 0.9322348237037659\n",
            "Epoch [20], Batch [547], Train Loss: 0.7413058280944824\n",
            "Epoch [20], Batch [548], Train Loss: 0.8951308131217957\n",
            "Epoch [20], Batch [549], Train Loss: 1.0703154802322388\n",
            "Epoch [20], Batch [550], Train Loss: 0.7982102036476135\n",
            "Epoch [20], Batch [551], Train Loss: 0.7724249958992004\n",
            "Epoch [20], Batch [552], Train Loss: 0.9301198720932007\n",
            "Epoch [20], Batch [553], Train Loss: 0.7457317113876343\n",
            "Epoch [20], Batch [554], Train Loss: 0.7479816675186157\n",
            "Epoch [20], Batch [555], Train Loss: 0.7942283153533936\n",
            "Epoch [20], Batch [556], Train Loss: 0.932740330696106\n",
            "Epoch [20], Batch [557], Train Loss: 0.6296482682228088\n",
            "Epoch [20], Batch [558], Train Loss: 0.8548218607902527\n",
            "Epoch [20], Batch [559], Train Loss: 0.8359563946723938\n",
            "Epoch [20], Batch [560], Train Loss: 0.7894752025604248\n",
            "Epoch [20], Batch [561], Train Loss: 0.855786144733429\n",
            "Epoch [20], Batch [562], Train Loss: 0.8970803618431091\n",
            "Epoch [20], Batch [563], Train Loss: 0.9158689975738525\n",
            "Epoch [20], Batch [564], Train Loss: 0.839968204498291\n",
            "Epoch [20], Batch [565], Train Loss: 0.7058675289154053\n",
            "Epoch [20], Batch [566], Train Loss: 0.9580187201499939\n",
            "Epoch [20], Batch [567], Train Loss: 0.7746816873550415\n",
            "Epoch [20], Batch [568], Train Loss: 0.9329251050949097\n",
            "Epoch [20], Batch [569], Train Loss: 0.9683151841163635\n",
            "Epoch [20], Batch [570], Train Loss: 0.7649640440940857\n",
            "Epoch [20], Batch [571], Train Loss: 0.7529174089431763\n",
            "Epoch [20], Batch [572], Train Loss: 0.893280029296875\n",
            "Epoch [20], Batch [573], Train Loss: 0.7557228803634644\n",
            "Epoch [20], Batch [574], Train Loss: 0.7894152402877808\n",
            "Epoch [20], Batch [575], Train Loss: 0.8099684715270996\n",
            "Epoch [20], Batch [576], Train Loss: 0.8924368619918823\n",
            "Epoch [20], Batch [577], Train Loss: 0.7792813777923584\n",
            "Epoch [20], Batch [578], Train Loss: 0.7046339511871338\n",
            "Epoch [20], Batch [579], Train Loss: 0.7149070501327515\n",
            "Epoch [20], Batch [580], Train Loss: 0.8953982591629028\n",
            "Epoch [20], Batch [581], Train Loss: 0.7690917253494263\n",
            "Epoch [20], Batch [582], Train Loss: 0.8435471653938293\n",
            "Epoch [20], Batch [583], Train Loss: 1.0595831871032715\n",
            "Epoch [20], Batch [584], Train Loss: 0.8891699314117432\n",
            "Epoch [20], Batch [585], Train Loss: 0.7459362745285034\n",
            "Epoch [20], Batch [586], Train Loss: 0.8907074332237244\n",
            "Epoch [20], Batch [587], Train Loss: 0.8765226006507874\n",
            "Epoch [20], Batch [588], Train Loss: 0.8965622186660767\n",
            "Epoch [20], Batch [589], Train Loss: 0.8330273628234863\n",
            "Epoch [20], Batch [590], Train Loss: 0.90511554479599\n",
            "Epoch [20], Batch [591], Train Loss: 0.9283496737480164\n",
            "Epoch [20], Batch [592], Train Loss: 0.9216225743293762\n",
            "Epoch [20], Batch [593], Train Loss: 0.9452610611915588\n",
            "Epoch [20], Batch [594], Train Loss: 0.8527140617370605\n",
            "Epoch [20], Batch [595], Train Loss: 0.8525450229644775\n",
            "Epoch [20], Batch [596], Train Loss: 0.6500995755195618\n",
            "Epoch [20], Batch [597], Train Loss: 0.8577367663383484\n",
            "Epoch [20], Batch [598], Train Loss: 1.0876264572143555\n",
            "Epoch [20], Batch [599], Train Loss: 1.1047122478485107\n",
            "Epoch [20], Batch [600], Train Loss: 0.8555407524108887\n",
            "Epoch [20], Batch [601], Train Loss: 0.8723496794700623\n",
            "Epoch [20], Batch [602], Train Loss: 0.6497241854667664\n",
            "Epoch [20], Batch [603], Train Loss: 0.8708511590957642\n",
            "Epoch [20], Batch [604], Train Loss: 0.7925022840499878\n",
            "Epoch [20], Batch [605], Train Loss: 0.8033234477043152\n",
            "Epoch [20], Batch [606], Train Loss: 0.8849189877510071\n",
            "Epoch [20], Batch [607], Train Loss: 0.6200857162475586\n",
            "Epoch [20], Batch [608], Train Loss: 0.8679874539375305\n",
            "Epoch [20], Batch [609], Train Loss: 0.9483411312103271\n",
            "Epoch [20], Batch [610], Train Loss: 0.8685700297355652\n",
            "Epoch [20], Batch [611], Train Loss: 0.7892737984657288\n",
            "Epoch [20], Batch [612], Train Loss: 0.7593783736228943\n",
            "Epoch [20], Batch [613], Train Loss: 0.7930330038070679\n",
            "Epoch [20], Batch [614], Train Loss: 0.7697760462760925\n",
            "Epoch [20], Batch [615], Train Loss: 1.0742466449737549\n",
            "Epoch [20], Batch [616], Train Loss: 0.8793824911117554\n",
            "Epoch [20], Batch [617], Train Loss: 0.8727190494537354\n",
            "Epoch [20], Batch [618], Train Loss: 0.7411794662475586\n",
            "Epoch [20], Batch [619], Train Loss: 0.8384577035903931\n",
            "Epoch [20], Batch [620], Train Loss: 0.9139570593833923\n",
            "Epoch [20], Batch [621], Train Loss: 0.7716435194015503\n",
            "Epoch [20], Batch [622], Train Loss: 0.835116446018219\n",
            "Epoch [20], Batch [623], Train Loss: 0.7753595113754272\n",
            "Epoch [20], Batch [624], Train Loss: 0.8195846080780029\n",
            "Epoch [20], Batch [625], Train Loss: 0.8626302480697632\n",
            "Epoch [20], Batch [626], Train Loss: 0.9935613870620728\n",
            "Epoch [20], Batch [627], Train Loss: 0.8784818053245544\n",
            "Epoch [20], Batch [628], Train Loss: 0.7516806125640869\n",
            "Epoch [20], Batch [629], Train Loss: 0.6541669964790344\n",
            "Epoch [20], Batch [630], Train Loss: 0.8906845450401306\n",
            "Epoch [20], Batch [631], Train Loss: 0.6206148266792297\n",
            "Epoch [20], Batch [632], Train Loss: 0.7263417840003967\n",
            "Epoch [20], Batch [633], Train Loss: 0.7751964926719666\n",
            "Epoch [20], Batch [634], Train Loss: 1.122336745262146\n",
            "Epoch [20], Batch [635], Train Loss: 0.6713437438011169\n",
            "Epoch [20], Batch [636], Train Loss: 0.8274457454681396\n",
            "Epoch [20], Batch [637], Train Loss: 0.6625562310218811\n",
            "Epoch [20], Batch [638], Train Loss: 0.7711611390113831\n",
            "Epoch [20], Batch [639], Train Loss: 0.7846198678016663\n",
            "Epoch [20], Batch [640], Train Loss: 0.9615007638931274\n",
            "Epoch [20], Batch [641], Train Loss: 0.8331563472747803\n",
            "Epoch [20], Batch [642], Train Loss: 0.6335350275039673\n",
            "Epoch [20], Batch [643], Train Loss: 0.6871291399002075\n",
            "Epoch [20], Batch [644], Train Loss: 1.0048733949661255\n",
            "Epoch [20], Batch [645], Train Loss: 0.6111987829208374\n",
            "Epoch [20], Batch [646], Train Loss: 0.8726658821105957\n",
            "Epoch [20], Batch [647], Train Loss: 0.8645050525665283\n",
            "Epoch [20], Batch [648], Train Loss: 0.8403196930885315\n",
            "Epoch [20], Batch [649], Train Loss: 1.0698226690292358\n",
            "Epoch [20], Batch [650], Train Loss: 0.8716291189193726\n",
            "Epoch [20], Batch [651], Train Loss: 1.0390398502349854\n",
            "Epoch [20], Batch [652], Train Loss: 0.8267245888710022\n",
            "Epoch [20], Batch [653], Train Loss: 0.7873233556747437\n",
            "Epoch [20], Batch [654], Train Loss: 0.8842836022377014\n",
            "Epoch [20], Batch [655], Train Loss: 0.7303617000579834\n",
            "Epoch [20], Batch [656], Train Loss: 0.831961452960968\n",
            "Epoch [20], Batch [657], Train Loss: 1.0277810096740723\n",
            "Epoch [20], Batch [658], Train Loss: 0.8878569602966309\n",
            "Epoch [20], Batch [659], Train Loss: 0.9016170501708984\n",
            "Epoch [20], Batch [660], Train Loss: 0.8803885579109192\n",
            "Epoch [20], Batch [661], Train Loss: 0.8923261165618896\n",
            "Epoch [20], Batch [662], Train Loss: 0.8483665585517883\n",
            "Epoch [20], Batch [663], Train Loss: 0.7640579342842102\n",
            "Epoch [20], Batch [664], Train Loss: 0.7228634357452393\n",
            "Epoch [20], Batch [665], Train Loss: 0.9715695381164551\n",
            "Epoch [20], Batch [666], Train Loss: 0.8599075078964233\n",
            "Epoch [20], Batch [667], Train Loss: 0.7625973224639893\n",
            "Epoch [20], Batch [668], Train Loss: 0.7478295564651489\n",
            "Epoch [20], Batch [669], Train Loss: 0.9743132591247559\n",
            "Epoch [20], Batch [670], Train Loss: 0.9379746317863464\n",
            "Epoch [20], Batch [671], Train Loss: 0.6993739604949951\n",
            "Epoch [20], Batch [672], Train Loss: 0.6884498596191406\n",
            "Epoch [20], Batch [673], Train Loss: 0.8948484659194946\n",
            "Epoch [20], Batch [674], Train Loss: 0.9553768038749695\n",
            "Epoch [20], Batch [675], Train Loss: 0.6907558441162109\n",
            "Epoch [20], Batch [676], Train Loss: 0.925156831741333\n",
            "Epoch [20], Batch [677], Train Loss: 0.8459665775299072\n",
            "Epoch [20], Batch [678], Train Loss: 0.7448999285697937\n",
            "Epoch [20], Batch [679], Train Loss: 0.7267690896987915\n",
            "Epoch [20], Batch [680], Train Loss: 0.8684574365615845\n",
            "Epoch [20], Batch [681], Train Loss: 0.9361672401428223\n",
            "Epoch [20], Batch [682], Train Loss: 0.885913074016571\n",
            "Epoch [20], Batch [683], Train Loss: 1.1088007688522339\n",
            "Epoch [20], Batch [684], Train Loss: 0.7761658430099487\n",
            "Epoch [20], Batch [685], Train Loss: 1.049984335899353\n",
            "Epoch [20], Batch [686], Train Loss: 0.9176939129829407\n",
            "Epoch [20], Batch [687], Train Loss: 0.755202054977417\n",
            "Epoch [20], Batch [688], Train Loss: 0.8123562335968018\n",
            "Epoch [20], Batch [689], Train Loss: 1.0568881034851074\n",
            "Epoch [20], Batch [690], Train Loss: 0.6930084228515625\n",
            "Epoch [20], Batch [691], Train Loss: 0.7453789114952087\n",
            "Epoch [20], Batch [692], Train Loss: 0.7304373979568481\n",
            "Epoch [20], Batch [693], Train Loss: 0.7725129723548889\n",
            "Epoch [20], Batch [694], Train Loss: 0.7497588992118835\n",
            "Epoch [20], Batch [695], Train Loss: 0.8792046904563904\n",
            "Epoch [20], Batch [696], Train Loss: 0.8591564297676086\n",
            "Epoch [20], Batch [697], Train Loss: 0.9421967267990112\n",
            "Epoch [20], Batch [698], Train Loss: 0.7488442659378052\n",
            "Epoch [20], Batch [699], Train Loss: 0.9954947233200073\n",
            "Epoch [20], Batch [700], Train Loss: 1.0191445350646973\n",
            "Epoch [20], Batch [701], Train Loss: 0.7723832130432129\n",
            "Epoch [20], Batch [702], Train Loss: 0.7789164781570435\n",
            "Epoch [20], Batch [703], Train Loss: 0.8042198419570923\n",
            "Epoch [20], Batch [704], Train Loss: 0.8116424083709717\n",
            "Epoch [20], Batch [705], Train Loss: 0.8121061325073242\n",
            "Epoch [20], Batch [706], Train Loss: 0.8596192598342896\n",
            "Epoch [20], Batch [707], Train Loss: 0.7018943428993225\n",
            "Epoch [20], Batch [708], Train Loss: 0.7834014892578125\n",
            "Epoch [20], Batch [709], Train Loss: 0.7349525690078735\n",
            "Epoch [20], Batch [710], Train Loss: 0.8883991837501526\n",
            "Epoch [20], Batch [711], Train Loss: 0.884048342704773\n",
            "Epoch [20], Batch [712], Train Loss: 0.765267550945282\n",
            "Epoch [20], Batch [713], Train Loss: 0.806973397731781\n",
            "Epoch [20], Batch [714], Train Loss: 0.8165698647499084\n",
            "Epoch [20], Batch [715], Train Loss: 0.7950478792190552\n",
            "Epoch [20], Batch [716], Train Loss: 0.8802276849746704\n",
            "Epoch [20], Batch [717], Train Loss: 0.7551552653312683\n",
            "Epoch [20], Batch [718], Train Loss: 0.8907595872879028\n",
            "Epoch [20], Batch [719], Train Loss: 0.6965898275375366\n",
            "Epoch [20], Batch [720], Train Loss: 0.6795519590377808\n",
            "Epoch [20], Batch [721], Train Loss: 0.7499489784240723\n",
            "Epoch [20], Batch [722], Train Loss: 0.8211945295333862\n",
            "Epoch [20], Batch [723], Train Loss: 0.7458298802375793\n",
            "Epoch [20], Batch [724], Train Loss: 0.7538527846336365\n",
            "Epoch [20], Batch [725], Train Loss: 0.771615207195282\n",
            "Epoch [20], Batch [726], Train Loss: 0.6923866868019104\n",
            "Epoch [20], Batch [727], Train Loss: 0.8010267019271851\n",
            "Epoch [20], Batch [728], Train Loss: 0.879916787147522\n",
            "Epoch [20], Batch [729], Train Loss: 0.9684290289878845\n",
            "Epoch [20], Batch [730], Train Loss: 1.0747525691986084\n",
            "Epoch [20], Batch [731], Train Loss: 0.7677785158157349\n",
            "Epoch [20], Batch [732], Train Loss: 0.6808342337608337\n",
            "Epoch [20], Batch [733], Train Loss: 0.772637128829956\n",
            "Epoch [20], Batch [734], Train Loss: 0.8769276142120361\n",
            "Epoch [20], Batch [735], Train Loss: 0.8485722541809082\n",
            "Epoch [20], Batch [736], Train Loss: 0.768877387046814\n",
            "Epoch [20], Batch [737], Train Loss: 1.1453959941864014\n",
            "Epoch [20], Batch [738], Train Loss: 1.0515624284744263\n",
            "Epoch [20], Batch [739], Train Loss: 0.91124027967453\n",
            "Epoch [20], Batch [740], Train Loss: 0.7018114328384399\n",
            "Epoch [20], Batch [741], Train Loss: 0.7839499711990356\n",
            "Epoch [20], Batch [742], Train Loss: 0.8192880749702454\n",
            "Epoch [20], Batch [743], Train Loss: 0.7935473918914795\n",
            "Epoch [20], Batch [744], Train Loss: 0.6968162655830383\n",
            "Epoch [20], Batch [745], Train Loss: 0.8069220781326294\n",
            "Epoch [20], Batch [746], Train Loss: 0.9212675094604492\n",
            "Epoch [20], Batch [747], Train Loss: 0.7463288307189941\n",
            "Epoch [20], Batch [748], Train Loss: 0.6894013285636902\n",
            "Epoch [20], Batch [749], Train Loss: 1.1384332180023193\n",
            "Epoch [20], Batch [750], Train Loss: 0.8339994549751282\n",
            "Epoch [20], Batch [751], Train Loss: 0.904064416885376\n",
            "Epoch [20], Batch [752], Train Loss: 1.0483943223953247\n",
            "Epoch [20], Batch [753], Train Loss: 1.0467078685760498\n",
            "Epoch [20], Batch [754], Train Loss: 1.099335789680481\n",
            "Epoch [20], Batch [755], Train Loss: 0.7243011593818665\n",
            "Epoch [20], Batch [756], Train Loss: 0.8882104754447937\n",
            "Epoch [20], Batch [757], Train Loss: 0.6716532707214355\n",
            "Epoch [20], Batch [758], Train Loss: 0.9467304348945618\n",
            "Epoch [20], Batch [759], Train Loss: 1.145173192024231\n",
            "Epoch [20], Batch [760], Train Loss: 1.0476679801940918\n",
            "Epoch [20], Batch [761], Train Loss: 0.8036550283432007\n",
            "Epoch [20], Batch [762], Train Loss: 0.8759588599205017\n",
            "Epoch [20], Batch [763], Train Loss: 0.5967686176300049\n",
            "Epoch [20], Batch [764], Train Loss: 0.8690805435180664\n",
            "Epoch [20], Batch [765], Train Loss: 0.8927367925643921\n",
            "Epoch [20], Batch [766], Train Loss: 0.9429138898849487\n",
            "Epoch [20], Batch [767], Train Loss: 1.0784553289413452\n",
            "Epoch [20], Batch [768], Train Loss: 0.7082392573356628\n",
            "Epoch [20], Batch [769], Train Loss: 0.712628960609436\n",
            "Epoch [20], Batch [770], Train Loss: 0.728588879108429\n",
            "Epoch [20], Batch [771], Train Loss: 0.7755360007286072\n",
            "Epoch [20], Batch [772], Train Loss: 0.7806552648544312\n",
            "Epoch [20], Batch [773], Train Loss: 0.8245958685874939\n",
            "Epoch [20], Batch [774], Train Loss: 0.6802261471748352\n",
            "Epoch [20], Batch [775], Train Loss: 0.825473427772522\n",
            "Epoch [20], Batch [776], Train Loss: 0.7263716459274292\n",
            "Epoch [20], Batch [777], Train Loss: 1.0947943925857544\n",
            "Epoch [20], Batch [778], Train Loss: 0.8845700621604919\n",
            "Epoch [20], Batch [779], Train Loss: 0.9382275938987732\n",
            "Epoch [20], Batch [780], Train Loss: 0.6864588260650635\n",
            "Epoch [20], Batch [781], Train Loss: 0.7262825965881348\n",
            "Epoch [20], Batch [782], Train Loss: 0.7128480076789856\n",
            "Epoch [20], Batch [783], Train Loss: 1.1661721467971802\n",
            "Epoch [20], Batch [784], Train Loss: 0.8839839100837708\n",
            "Epoch [20], Batch [785], Train Loss: 0.8590059280395508\n",
            "Epoch [20], Batch [786], Train Loss: 0.9481075406074524\n",
            "Epoch [20], Batch [787], Train Loss: 0.6531950831413269\n",
            "Epoch [20], Batch [788], Train Loss: 0.8140979409217834\n",
            "Epoch [20], Batch [789], Train Loss: 0.7910798788070679\n",
            "Epoch [20], Batch [790], Train Loss: 0.8796921372413635\n",
            "Epoch [20], Batch [791], Train Loss: 0.9038131237030029\n",
            "Epoch [20], Batch [792], Train Loss: 0.9719942212104797\n",
            "Epoch [20], Batch [793], Train Loss: 0.9266151189804077\n",
            "Epoch [20], Batch [794], Train Loss: 0.8133252859115601\n",
            "Epoch [20], Batch [795], Train Loss: 0.8899298310279846\n",
            "Epoch [20], Batch [796], Train Loss: 0.7263880372047424\n",
            "Epoch [20], Batch [797], Train Loss: 0.8656406402587891\n",
            "Epoch [20], Batch [798], Train Loss: 0.885190486907959\n",
            "Epoch [20], Batch [799], Train Loss: 0.8270245790481567\n",
            "Epoch [20], Batch [800], Train Loss: 0.8306459784507751\n",
            "Epoch [20], Batch [801], Train Loss: 0.7867817282676697\n",
            "Epoch [20], Batch [802], Train Loss: 0.8899044990539551\n",
            "Epoch [20], Batch [803], Train Loss: 0.8436329364776611\n",
            "Epoch [20], Batch [804], Train Loss: 0.9373545050621033\n",
            "Epoch [20], Batch [805], Train Loss: 0.7610768675804138\n",
            "Epoch [20], Batch [806], Train Loss: 0.830901026725769\n",
            "Epoch [20], Batch [807], Train Loss: 0.8151721954345703\n",
            "Epoch [20], Batch [808], Train Loss: 0.8766482472419739\n",
            "Epoch [20], Batch [809], Train Loss: 0.7071442008018494\n",
            "Epoch [20], Batch [810], Train Loss: 0.8904531598091125\n",
            "Epoch [20], Batch [811], Train Loss: 0.7109324336051941\n",
            "Epoch [20], Batch [812], Train Loss: 0.7440075874328613\n",
            "Epoch [20], Batch [813], Train Loss: 0.8113246560096741\n",
            "Epoch [20], Batch [814], Train Loss: 0.6776103973388672\n",
            "Epoch [20], Batch [815], Train Loss: 1.0378456115722656\n",
            "Epoch [20], Batch [816], Train Loss: 0.9158338308334351\n",
            "Epoch [20], Batch [817], Train Loss: 0.9416935443878174\n",
            "Epoch [20], Batch [818], Train Loss: 0.6424646377563477\n",
            "Epoch [20], Batch [819], Train Loss: 0.8424740433692932\n",
            "Epoch [20], Batch [820], Train Loss: 0.8316259384155273\n",
            "Epoch [20], Batch [821], Train Loss: 0.9651197791099548\n",
            "Epoch [20], Batch [822], Train Loss: 0.7774941325187683\n",
            "Epoch [20], Batch [823], Train Loss: 0.8116915822029114\n",
            "Epoch [20], Batch [824], Train Loss: 0.7963382601737976\n",
            "Epoch [20], Batch [825], Train Loss: 0.8594614863395691\n",
            "Epoch [20], Batch [826], Train Loss: 0.9173589944839478\n",
            "Epoch [20], Batch [827], Train Loss: 0.9588876962661743\n",
            "Epoch [20], Batch [828], Train Loss: 0.7035390138626099\n",
            "Epoch [20], Batch [829], Train Loss: 0.8038536906242371\n",
            "Epoch [20], Batch [830], Train Loss: 0.9855769872665405\n",
            "Epoch [20], Batch [831], Train Loss: 0.7449584007263184\n",
            "Epoch [20], Batch [832], Train Loss: 0.9177359342575073\n",
            "Epoch [20], Batch [833], Train Loss: 0.7803674340248108\n",
            "Epoch [20], Batch [834], Train Loss: 0.7625935077667236\n",
            "Epoch [20], Batch [835], Train Loss: 0.8351941704750061\n",
            "Epoch [20], Batch [836], Train Loss: 0.7484936714172363\n",
            "Epoch [20], Batch [837], Train Loss: 0.8148216009140015\n",
            "Epoch [20], Batch [838], Train Loss: 0.8257324695587158\n",
            "Epoch [20], Batch [839], Train Loss: 0.8205705285072327\n",
            "Epoch [20], Batch [840], Train Loss: 0.871657133102417\n",
            "Epoch [20], Batch [841], Train Loss: 0.7870286703109741\n",
            "Epoch [20], Batch [842], Train Loss: 0.6839961409568787\n",
            "Epoch [20], Batch [843], Train Loss: 0.7741998434066772\n",
            "Epoch [20], Batch [844], Train Loss: 0.7929836511611938\n",
            "Epoch [20], Batch [845], Train Loss: 0.8032064437866211\n",
            "Epoch [20], Batch [846], Train Loss: 0.7693270444869995\n",
            "Epoch [20], Batch [847], Train Loss: 0.9640406370162964\n",
            "Epoch [20], Batch [848], Train Loss: 0.873740553855896\n",
            "Epoch [20], Batch [849], Train Loss: 0.798747718334198\n",
            "Epoch [20], Batch [850], Train Loss: 0.7901163101196289\n",
            "Epoch [20], Batch [851], Train Loss: 0.911855161190033\n",
            "Epoch [20], Batch [852], Train Loss: 0.9395879507064819\n",
            "Epoch [20], Batch [853], Train Loss: 0.769887387752533\n",
            "Epoch [20], Batch [854], Train Loss: 0.8611052632331848\n",
            "Epoch [20], Batch [855], Train Loss: 0.8034371733665466\n",
            "Epoch [20], Batch [856], Train Loss: 0.9260143637657166\n",
            "Epoch [20], Batch [857], Train Loss: 0.7561081051826477\n",
            "Epoch [20], Batch [858], Train Loss: 0.764214038848877\n",
            "Epoch [20], Batch [859], Train Loss: 0.9943420886993408\n",
            "Epoch [20], Batch [860], Train Loss: 0.7037912607192993\n",
            "Epoch [20], Batch [861], Train Loss: 0.6897447109222412\n",
            "Epoch [20], Batch [862], Train Loss: 1.126760482788086\n",
            "Epoch [20], Batch [863], Train Loss: 0.863440752029419\n",
            "Epoch [20], Batch [864], Train Loss: 0.889762818813324\n",
            "Epoch [20], Batch [865], Train Loss: 0.8260857462882996\n",
            "Epoch [20], Batch [866], Train Loss: 0.7147574424743652\n",
            "Epoch [20], Batch [867], Train Loss: 0.8488724231719971\n",
            "Epoch [20], Batch [868], Train Loss: 0.8634343147277832\n",
            "Epoch [20], Batch [869], Train Loss: 0.8678700923919678\n",
            "Epoch [20], Batch [870], Train Loss: 1.0022010803222656\n",
            "Epoch [20], Batch [871], Train Loss: 0.7593984007835388\n",
            "Epoch [20], Batch [872], Train Loss: 0.867214024066925\n",
            "Epoch [20], Batch [873], Train Loss: 0.6436547040939331\n",
            "Epoch [20], Batch [874], Train Loss: 0.7397753596305847\n",
            "Epoch [20], Batch [875], Train Loss: 0.7226930856704712\n",
            "Epoch [20], Batch [876], Train Loss: 0.6448578238487244\n",
            "Epoch [20], Batch [877], Train Loss: 0.7058358788490295\n",
            "Epoch [20], Batch [878], Train Loss: 0.7775610685348511\n",
            "Epoch [20], Batch [879], Train Loss: 0.7034778594970703\n",
            "Epoch [20], Batch [880], Train Loss: 1.0313212871551514\n",
            "Epoch [20], Batch [881], Train Loss: 0.7646321058273315\n",
            "Epoch [20], Batch [882], Train Loss: 0.8463606238365173\n",
            "Epoch [20], Batch [883], Train Loss: 0.733629584312439\n",
            "Epoch [20], Batch [884], Train Loss: 0.9676541686058044\n",
            "Epoch [20], Batch [885], Train Loss: 0.9808417558670044\n",
            "Epoch [20], Batch [886], Train Loss: 0.6288217306137085\n",
            "Epoch [20], Batch [887], Train Loss: 0.8765276074409485\n",
            "Epoch [20], Batch [888], Train Loss: 0.5848335027694702\n",
            "Epoch [20], Batch [889], Train Loss: 0.7598806023597717\n",
            "Epoch [20], Batch [890], Train Loss: 0.8311795592308044\n",
            "Epoch [20], Batch [891], Train Loss: 0.7326968312263489\n",
            "Epoch [20], Batch [892], Train Loss: 0.7155918478965759\n",
            "Epoch [20], Batch [893], Train Loss: 0.6963303685188293\n",
            "Epoch [20], Batch [894], Train Loss: 0.9604387283325195\n",
            "Epoch [20], Batch [895], Train Loss: 0.7635498642921448\n",
            "Epoch [20], Batch [896], Train Loss: 0.8677392601966858\n",
            "Epoch [20], Batch [897], Train Loss: 0.858840823173523\n",
            "Epoch [20], Batch [898], Train Loss: 0.8825696110725403\n",
            "Epoch [20], Batch [899], Train Loss: 0.6868401765823364\n",
            "Epoch [20], Batch [900], Train Loss: 0.7888194918632507\n",
            "Epoch [20], Batch [901], Train Loss: 1.121600866317749\n",
            "Epoch [20], Batch [902], Train Loss: 0.7593367099761963\n",
            "Epoch [20], Batch [903], Train Loss: 0.8714027404785156\n",
            "Epoch [20], Batch [904], Train Loss: 0.8606712818145752\n",
            "Epoch [20], Batch [905], Train Loss: 0.7026506662368774\n",
            "Epoch [20], Batch [906], Train Loss: 0.802254855632782\n",
            "Epoch [20], Batch [907], Train Loss: 0.7024815082550049\n",
            "Epoch [20], Batch [908], Train Loss: 0.935310959815979\n",
            "Epoch [20], Batch [909], Train Loss: 0.8618944883346558\n",
            "Epoch [20], Batch [910], Train Loss: 0.8774271011352539\n",
            "Epoch [20], Batch [911], Train Loss: 0.8827832341194153\n",
            "Epoch [20], Batch [912], Train Loss: 0.7705745697021484\n",
            "Epoch [20], Batch [913], Train Loss: 0.9191635847091675\n",
            "Epoch [20], Batch [914], Train Loss: 1.0042531490325928\n",
            "Epoch [20], Batch [915], Train Loss: 0.740321695804596\n",
            "Epoch [20], Batch [916], Train Loss: 0.7406725883483887\n",
            "Epoch [20], Batch [917], Train Loss: 0.6914533376693726\n",
            "Epoch [20], Batch [918], Train Loss: 0.8463131189346313\n",
            "Epoch [20], Batch [919], Train Loss: 1.0064524412155151\n",
            "Epoch [20], Batch [920], Train Loss: 0.9946097135543823\n",
            "Epoch [20], Batch [921], Train Loss: 0.8015730977058411\n",
            "Epoch [20], Batch [922], Train Loss: 0.7165001630783081\n",
            "Epoch [20], Batch [923], Train Loss: 0.803196907043457\n",
            "Epoch [20], Batch [924], Train Loss: 0.8736948370933533\n",
            "Epoch [20], Batch [925], Train Loss: 0.8176368474960327\n",
            "Epoch [20], Batch [926], Train Loss: 0.9284838438034058\n",
            "Epoch [20], Batch [927], Train Loss: 0.7888376712799072\n",
            "Epoch [20], Batch [928], Train Loss: 0.9462242126464844\n",
            "Epoch [20], Batch [929], Train Loss: 0.6299415826797485\n",
            "Epoch [20], Batch [930], Train Loss: 0.7101850509643555\n",
            "Epoch [20], Batch [931], Train Loss: 0.8415644764900208\n",
            "Epoch [20], Batch [932], Train Loss: 0.7432923316955566\n",
            "Epoch [20], Batch [933], Train Loss: 0.7257872223854065\n",
            "Epoch [20], Batch [934], Train Loss: 0.8286160230636597\n",
            "Epoch [20], Batch [935], Train Loss: 0.9603238701820374\n",
            "Epoch [20], Batch [936], Train Loss: 0.9300543069839478\n",
            "Epoch [20], Batch [937], Train Loss: 1.0094884634017944\n",
            "Epoch [20], Batch [938], Train Loss: 0.8484513163566589\n",
            "Accuracy of train set: 0.6785666666666667\n",
            "Epoch [21], Batch [1], Loss: 0.7963903546333313\n",
            "Epoch [21], Batch [2], Loss: 0.6471682786941528\n",
            "Epoch [21], Batch [3], Loss: 0.7490229606628418\n",
            "Epoch [21], Batch [4], Loss: 0.7378129363059998\n",
            "Epoch [21], Batch [5], Loss: 0.7788432240486145\n",
            "Epoch [21], Batch [6], Loss: 0.771544337272644\n",
            "Epoch [21], Batch [7], Loss: 0.8211745023727417\n",
            "Epoch [21], Batch [8], Loss: 0.8613131642341614\n",
            "Epoch [21], Batch [9], Loss: 0.826804518699646\n",
            "Epoch [21], Batch [10], Loss: 0.965009331703186\n",
            "Epoch [21], Batch [11], Loss: 0.9881742596626282\n",
            "Epoch [21], Batch [12], Loss: 0.8260880708694458\n",
            "Epoch [21], Batch [13], Loss: 0.6893084049224854\n",
            "Epoch [21], Batch [14], Loss: 0.7072225213050842\n",
            "Epoch [21], Batch [15], Loss: 0.7719939351081848\n",
            "Epoch [21], Batch [16], Loss: 0.9557204246520996\n",
            "Epoch [21], Batch [17], Loss: 0.7710847854614258\n",
            "Epoch [21], Batch [18], Loss: 1.013120174407959\n",
            "Epoch [21], Batch [19], Loss: 0.8773207664489746\n",
            "Epoch [21], Batch [20], Loss: 0.942051351070404\n",
            "Epoch [21], Batch [21], Loss: 0.9736676812171936\n",
            "Epoch [21], Batch [22], Loss: 0.8245896100997925\n",
            "Epoch [21], Batch [23], Loss: 0.72308748960495\n",
            "Epoch [21], Batch [24], Loss: 0.751768171787262\n",
            "Epoch [21], Batch [25], Loss: 0.7786794900894165\n",
            "Epoch [21], Batch [26], Loss: 0.8563792109489441\n",
            "Epoch [21], Batch [27], Loss: 0.8351538181304932\n",
            "Epoch [21], Batch [28], Loss: 0.8633768558502197\n",
            "Epoch [21], Batch [29], Loss: 0.9176725149154663\n",
            "Epoch [21], Batch [30], Loss: 0.5117400288581848\n",
            "Epoch [21], Batch [31], Loss: 0.9018725156784058\n",
            "Epoch [21], Batch [32], Loss: 0.6870166063308716\n",
            "Epoch [21], Batch [33], Loss: 0.8889330625534058\n",
            "Epoch [21], Batch [34], Loss: 0.7304445505142212\n",
            "Epoch [21], Batch [35], Loss: 0.7445769906044006\n",
            "Epoch [21], Batch [36], Loss: 0.7701521515846252\n",
            "Epoch [21], Batch [37], Loss: 0.7083437442779541\n",
            "Epoch [21], Batch [38], Loss: 0.7861669659614563\n",
            "Epoch [21], Batch [39], Loss: 0.8617713451385498\n",
            "Epoch [21], Batch [40], Loss: 0.7927722930908203\n",
            "Epoch [21], Batch [41], Loss: 0.8877960443496704\n",
            "Epoch [21], Batch [42], Loss: 0.9733908176422119\n",
            "Epoch [21], Batch [43], Loss: 1.0179085731506348\n",
            "Epoch [21], Batch [44], Loss: 0.7305634021759033\n",
            "Epoch [21], Batch [45], Loss: 0.9400174021720886\n",
            "Epoch [21], Batch [46], Loss: 1.1066505908966064\n",
            "Epoch [21], Batch [47], Loss: 0.8088264465332031\n",
            "Epoch [21], Batch [48], Loss: 1.0652225017547607\n",
            "Epoch [21], Batch [49], Loss: 0.9158797860145569\n",
            "Epoch [21], Batch [50], Loss: 0.7428920269012451\n",
            "Epoch [21], Batch [51], Loss: 1.1473082304000854\n",
            "Epoch [21], Batch [52], Loss: 0.9187145829200745\n",
            "Epoch [21], Batch [53], Loss: 0.7562607526779175\n",
            "Epoch [21], Batch [54], Loss: 0.9522817134857178\n",
            "Epoch [21], Batch [55], Loss: 1.0751919746398926\n",
            "Epoch [21], Batch [56], Loss: 0.7971823811531067\n",
            "Epoch [21], Batch [57], Loss: 0.9595288038253784\n",
            "Epoch [21], Batch [58], Loss: 1.0015616416931152\n",
            "Epoch [21], Batch [59], Loss: 0.817862331867218\n",
            "Epoch [21], Batch [60], Loss: 1.031203269958496\n",
            "Epoch [21], Batch [61], Loss: 0.820905327796936\n",
            "Epoch [21], Batch [62], Loss: 1.2310364246368408\n",
            "Epoch [21], Batch [63], Loss: 0.8423564434051514\n",
            "Epoch [21], Batch [64], Loss: 0.9879138469696045\n",
            "Epoch [21], Batch [65], Loss: 0.966162919998169\n",
            "Epoch [21], Batch [66], Loss: 0.8163865804672241\n",
            "Epoch [21], Batch [67], Loss: 0.8687942624092102\n",
            "Epoch [21], Batch [68], Loss: 0.6828853487968445\n",
            "Epoch [21], Batch [69], Loss: 0.9489386081695557\n",
            "Epoch [21], Batch [70], Loss: 0.6207714676856995\n",
            "Epoch [21], Batch [71], Loss: 0.624661922454834\n",
            "Epoch [21], Batch [72], Loss: 0.5903302431106567\n",
            "Epoch [21], Batch [73], Loss: 0.8287575840950012\n",
            "Epoch [21], Batch [74], Loss: 0.9138442873954773\n",
            "Epoch [21], Batch [75], Loss: 0.7677562236785889\n",
            "Epoch [21], Batch [76], Loss: 0.8823695182800293\n",
            "Epoch [21], Batch [77], Loss: 0.8604784607887268\n",
            "Epoch [21], Batch [78], Loss: 0.7536868453025818\n",
            "Epoch [21], Batch [79], Loss: 1.1364331245422363\n",
            "Epoch [21], Batch [80], Loss: 0.8883638381958008\n",
            "Epoch [21], Batch [81], Loss: 1.0434966087341309\n",
            "Epoch [21], Batch [82], Loss: 0.7817689776420593\n",
            "Epoch [21], Batch [83], Loss: 0.7781564593315125\n",
            "Epoch [21], Batch [84], Loss: 0.7738450169563293\n",
            "Epoch [21], Batch [85], Loss: 0.7694963216781616\n",
            "Epoch [21], Batch [86], Loss: 0.8999261856079102\n",
            "Epoch [21], Batch [87], Loss: 0.9729616641998291\n",
            "Epoch [21], Batch [88], Loss: 1.0366642475128174\n",
            "Epoch [21], Batch [89], Loss: 0.936874508857727\n",
            "Epoch [21], Batch [90], Loss: 0.7869197726249695\n",
            "Epoch [21], Batch [91], Loss: 0.832293689250946\n",
            "Epoch [21], Batch [92], Loss: 0.6790469288825989\n",
            "Epoch [21], Batch [93], Loss: 0.6229839324951172\n",
            "Epoch [21], Batch [94], Loss: 0.9616667032241821\n",
            "Epoch [21], Batch [95], Loss: 0.7196136713027954\n",
            "Epoch [21], Batch [96], Loss: 0.9176052808761597\n",
            "Epoch [21], Batch [97], Loss: 0.721266508102417\n",
            "Epoch [21], Batch [98], Loss: 0.8467108607292175\n",
            "Epoch [21], Batch [99], Loss: 1.0334370136260986\n",
            "Epoch [21], Batch [100], Loss: 0.7044909000396729\n",
            "Epoch [21], Batch [101], Loss: 0.8510808944702148\n",
            "Epoch [21], Batch [102], Loss: 0.8322100043296814\n",
            "Epoch [21], Batch [103], Loss: 0.7581756711006165\n",
            "Epoch [21], Batch [104], Loss: 0.8238027691841125\n",
            "Epoch [21], Batch [105], Loss: 1.09419584274292\n",
            "Epoch [21], Batch [106], Loss: 0.6739686131477356\n",
            "Epoch [21], Batch [107], Loss: 0.8881932497024536\n",
            "Epoch [21], Batch [108], Loss: 0.8025369644165039\n",
            "Epoch [21], Batch [109], Loss: 0.9164579510688782\n",
            "Epoch [21], Batch [110], Loss: 0.8318617343902588\n",
            "Epoch [21], Batch [111], Loss: 0.759932816028595\n",
            "Epoch [21], Batch [112], Loss: 0.6287919878959656\n",
            "Epoch [21], Batch [113], Loss: 0.9157533049583435\n",
            "Epoch [21], Batch [114], Loss: 0.8318690061569214\n",
            "Epoch [21], Batch [115], Loss: 0.7859627604484558\n",
            "Epoch [21], Batch [116], Loss: 0.8487359285354614\n",
            "Epoch [21], Batch [117], Loss: 0.8076004981994629\n",
            "Epoch [21], Batch [118], Loss: 0.8170989751815796\n",
            "Epoch [21], Batch [119], Loss: 0.8534189462661743\n",
            "Epoch [21], Batch [120], Loss: 0.771525502204895\n",
            "Epoch [21], Batch [121], Loss: 0.8164750933647156\n",
            "Epoch [21], Batch [122], Loss: 0.7087389230728149\n",
            "Epoch [21], Batch [123], Loss: 0.8703900575637817\n",
            "Epoch [21], Batch [124], Loss: 1.2034882307052612\n",
            "Epoch [21], Batch [125], Loss: 0.8696873784065247\n",
            "Epoch [21], Batch [126], Loss: 0.8582335710525513\n",
            "Epoch [21], Batch [127], Loss: 0.7064750790596008\n",
            "Epoch [21], Batch [128], Loss: 0.8969009518623352\n",
            "Epoch [21], Batch [129], Loss: 0.9268712401390076\n",
            "Epoch [21], Batch [130], Loss: 0.8953002691268921\n",
            "Epoch [21], Batch [131], Loss: 0.6947253346443176\n",
            "Epoch [21], Batch [132], Loss: 0.6564033031463623\n",
            "Epoch [21], Batch [133], Loss: 0.7556231617927551\n",
            "Epoch [21], Batch [134], Loss: 0.7904824614524841\n",
            "Epoch [21], Batch [135], Loss: 0.8027002215385437\n",
            "Epoch [21], Batch [136], Loss: 0.732843279838562\n",
            "Epoch [21], Batch [137], Loss: 1.0018877983093262\n",
            "Epoch [21], Batch [138], Loss: 0.7830986976623535\n",
            "Epoch [21], Batch [139], Loss: 0.8670803308486938\n",
            "Epoch [21], Batch [140], Loss: 0.8921197056770325\n",
            "Epoch [21], Batch [141], Loss: 0.7104804515838623\n",
            "Epoch [21], Batch [142], Loss: 0.928943395614624\n",
            "Epoch [21], Batch [143], Loss: 0.9142407178878784\n",
            "Epoch [21], Batch [144], Loss: 0.9179000854492188\n",
            "Epoch [21], Batch [145], Loss: 0.8494148254394531\n",
            "Epoch [21], Batch [146], Loss: 0.8672206401824951\n",
            "Epoch [21], Batch [147], Loss: 0.8416980504989624\n",
            "Epoch [21], Batch [148], Loss: 0.6822903156280518\n",
            "Epoch [21], Batch [149], Loss: 1.001377820968628\n",
            "Epoch [21], Batch [150], Loss: 1.0639814138412476\n",
            "Epoch [21], Batch [151], Loss: 1.128541111946106\n",
            "Epoch [21], Batch [152], Loss: 0.6864709258079529\n",
            "Epoch [21], Batch [153], Loss: 0.7432677149772644\n",
            "Epoch [21], Batch [154], Loss: 0.7588859796524048\n",
            "Epoch [21], Batch [155], Loss: 1.1797672510147095\n",
            "Epoch [21], Batch [156], Loss: 0.7927447557449341\n",
            "Epoch [21], Batch [157], Loss: 0.7009760737419128\n",
            "Accuracy of None set: 0.6705\n",
            "Epoch [21], Batch [1], Train Loss: 0.7596694827079773\n",
            "Epoch [21], Batch [2], Train Loss: 0.8072384595870972\n",
            "Epoch [21], Batch [3], Train Loss: 0.871978223323822\n",
            "Epoch [21], Batch [4], Train Loss: 0.8733552694320679\n",
            "Epoch [21], Batch [5], Train Loss: 0.6777956485748291\n",
            "Epoch [21], Batch [6], Train Loss: 1.0193021297454834\n",
            "Epoch [21], Batch [7], Train Loss: 0.9937055706977844\n",
            "Epoch [21], Batch [8], Train Loss: 0.7651472687721252\n",
            "Epoch [21], Batch [9], Train Loss: 0.7226415872573853\n",
            "Epoch [21], Batch [10], Train Loss: 0.8113954067230225\n",
            "Epoch [21], Batch [11], Train Loss: 0.8926473259925842\n",
            "Epoch [21], Batch [12], Train Loss: 0.710680365562439\n",
            "Epoch [21], Batch [13], Train Loss: 0.7872185111045837\n",
            "Epoch [21], Batch [14], Train Loss: 0.9527965784072876\n",
            "Epoch [21], Batch [15], Train Loss: 0.6770304441452026\n",
            "Epoch [21], Batch [16], Train Loss: 0.9270172715187073\n",
            "Epoch [21], Batch [17], Train Loss: 0.984167218208313\n",
            "Epoch [21], Batch [18], Train Loss: 0.8684058785438538\n",
            "Epoch [21], Batch [19], Train Loss: 0.8053045272827148\n",
            "Epoch [21], Batch [20], Train Loss: 0.6704594492912292\n",
            "Epoch [21], Batch [21], Train Loss: 0.6477131247520447\n",
            "Epoch [21], Batch [22], Train Loss: 0.8139292001724243\n",
            "Epoch [21], Batch [23], Train Loss: 1.002022385597229\n",
            "Epoch [21], Batch [24], Train Loss: 0.902745246887207\n",
            "Epoch [21], Batch [25], Train Loss: 0.6706015467643738\n",
            "Epoch [21], Batch [26], Train Loss: 0.6809186935424805\n",
            "Epoch [21], Batch [27], Train Loss: 0.7775852084159851\n",
            "Epoch [21], Batch [28], Train Loss: 0.8416324853897095\n",
            "Epoch [21], Batch [29], Train Loss: 0.6912322044372559\n",
            "Epoch [21], Batch [30], Train Loss: 0.7576434016227722\n",
            "Epoch [21], Batch [31], Train Loss: 0.8194913864135742\n",
            "Epoch [21], Batch [32], Train Loss: 0.9102669358253479\n",
            "Epoch [21], Batch [33], Train Loss: 0.7640464305877686\n",
            "Epoch [21], Batch [34], Train Loss: 1.0399802923202515\n",
            "Epoch [21], Batch [35], Train Loss: 0.6837696433067322\n",
            "Epoch [21], Batch [36], Train Loss: 0.7375709414482117\n",
            "Epoch [21], Batch [37], Train Loss: 0.8625731468200684\n",
            "Epoch [21], Batch [38], Train Loss: 0.9257089495658875\n",
            "Epoch [21], Batch [39], Train Loss: 1.0030384063720703\n",
            "Epoch [21], Batch [40], Train Loss: 0.7967565655708313\n",
            "Epoch [21], Batch [41], Train Loss: 0.8103287816047668\n",
            "Epoch [21], Batch [42], Train Loss: 0.9700955152511597\n",
            "Epoch [21], Batch [43], Train Loss: 0.8126773834228516\n",
            "Epoch [21], Batch [44], Train Loss: 0.8341643810272217\n",
            "Epoch [21], Batch [45], Train Loss: 0.7245460748672485\n",
            "Epoch [21], Batch [46], Train Loss: 0.9003825783729553\n",
            "Epoch [21], Batch [47], Train Loss: 0.9915969371795654\n",
            "Epoch [21], Batch [48], Train Loss: 0.7438576817512512\n",
            "Epoch [21], Batch [49], Train Loss: 0.7940372824668884\n",
            "Epoch [21], Batch [50], Train Loss: 0.7870381474494934\n",
            "Epoch [21], Batch [51], Train Loss: 0.8075515627861023\n",
            "Epoch [21], Batch [52], Train Loss: 0.8964070081710815\n",
            "Epoch [21], Batch [53], Train Loss: 0.7198212146759033\n",
            "Epoch [21], Batch [54], Train Loss: 0.8000392317771912\n",
            "Epoch [21], Batch [55], Train Loss: 0.7832815647125244\n",
            "Epoch [21], Batch [56], Train Loss: 0.8037761449813843\n",
            "Epoch [21], Batch [57], Train Loss: 1.0212444067001343\n",
            "Epoch [21], Batch [58], Train Loss: 0.8106671571731567\n",
            "Epoch [21], Batch [59], Train Loss: 0.7050701975822449\n",
            "Epoch [21], Batch [60], Train Loss: 0.7589833736419678\n",
            "Epoch [21], Batch [61], Train Loss: 0.9230113625526428\n",
            "Epoch [21], Batch [62], Train Loss: 0.6368706822395325\n",
            "Epoch [21], Batch [63], Train Loss: 0.8805414438247681\n",
            "Epoch [21], Batch [64], Train Loss: 0.8474853038787842\n",
            "Epoch [21], Batch [65], Train Loss: 0.8542481660842896\n",
            "Epoch [21], Batch [66], Train Loss: 0.831736147403717\n",
            "Epoch [21], Batch [67], Train Loss: 0.9074854850769043\n",
            "Epoch [21], Batch [68], Train Loss: 0.8658649325370789\n",
            "Epoch [21], Batch [69], Train Loss: 0.7165166735649109\n",
            "Epoch [21], Batch [70], Train Loss: 0.8400946855545044\n",
            "Epoch [21], Batch [71], Train Loss: 0.6643506288528442\n",
            "Epoch [21], Batch [72], Train Loss: 0.8580436110496521\n",
            "Epoch [21], Batch [73], Train Loss: 0.7956210970878601\n",
            "Epoch [21], Batch [74], Train Loss: 0.9249011278152466\n",
            "Epoch [21], Batch [75], Train Loss: 0.7960455417633057\n",
            "Epoch [21], Batch [76], Train Loss: 0.7757686376571655\n",
            "Epoch [21], Batch [77], Train Loss: 1.1836352348327637\n",
            "Epoch [21], Batch [78], Train Loss: 1.0533174276351929\n",
            "Epoch [21], Batch [79], Train Loss: 0.8341599702835083\n",
            "Epoch [21], Batch [80], Train Loss: 1.0627093315124512\n",
            "Epoch [21], Batch [81], Train Loss: 0.8546987175941467\n",
            "Epoch [21], Batch [82], Train Loss: 0.7496247291564941\n",
            "Epoch [21], Batch [83], Train Loss: 0.9544178247451782\n",
            "Epoch [21], Batch [84], Train Loss: 0.6287199258804321\n",
            "Epoch [21], Batch [85], Train Loss: 0.8114178776741028\n",
            "Epoch [21], Batch [86], Train Loss: 0.9037064909934998\n",
            "Epoch [21], Batch [87], Train Loss: 1.0712825059890747\n",
            "Epoch [21], Batch [88], Train Loss: 0.6847389936447144\n",
            "Epoch [21], Batch [89], Train Loss: 0.799479067325592\n",
            "Epoch [21], Batch [90], Train Loss: 1.0430282354354858\n",
            "Epoch [21], Batch [91], Train Loss: 0.8092740774154663\n",
            "Epoch [21], Batch [92], Train Loss: 0.8540069460868835\n",
            "Epoch [21], Batch [93], Train Loss: 0.862389326095581\n",
            "Epoch [21], Batch [94], Train Loss: 0.8119314312934875\n",
            "Epoch [21], Batch [95], Train Loss: 0.8412687182426453\n",
            "Epoch [21], Batch [96], Train Loss: 0.7990207672119141\n",
            "Epoch [21], Batch [97], Train Loss: 0.6051101088523865\n",
            "Epoch [21], Batch [98], Train Loss: 0.8072925806045532\n",
            "Epoch [21], Batch [99], Train Loss: 0.8591041564941406\n",
            "Epoch [21], Batch [100], Train Loss: 0.7335712313652039\n",
            "Epoch [21], Batch [101], Train Loss: 1.0730986595153809\n",
            "Epoch [21], Batch [102], Train Loss: 0.8411701917648315\n",
            "Epoch [21], Batch [103], Train Loss: 0.7482115626335144\n",
            "Epoch [21], Batch [104], Train Loss: 0.8278912901878357\n",
            "Epoch [21], Batch [105], Train Loss: 0.9214190244674683\n",
            "Epoch [21], Batch [106], Train Loss: 0.7251526713371277\n",
            "Epoch [21], Batch [107], Train Loss: 1.0918145179748535\n",
            "Epoch [21], Batch [108], Train Loss: 1.172478199005127\n",
            "Epoch [21], Batch [109], Train Loss: 0.8045312166213989\n",
            "Epoch [21], Batch [110], Train Loss: 0.6789336800575256\n",
            "Epoch [21], Batch [111], Train Loss: 0.6299117207527161\n",
            "Epoch [21], Batch [112], Train Loss: 0.8637046813964844\n",
            "Epoch [21], Batch [113], Train Loss: 0.6582927703857422\n",
            "Epoch [21], Batch [114], Train Loss: 0.6906704306602478\n",
            "Epoch [21], Batch [115], Train Loss: 0.8428363800048828\n",
            "Epoch [21], Batch [116], Train Loss: 1.035744547843933\n",
            "Epoch [21], Batch [117], Train Loss: 0.6194700598716736\n",
            "Epoch [21], Batch [118], Train Loss: 1.029909372329712\n",
            "Epoch [21], Batch [119], Train Loss: 0.7153134346008301\n",
            "Epoch [21], Batch [120], Train Loss: 0.9108829498291016\n",
            "Epoch [21], Batch [121], Train Loss: 0.8882988691329956\n",
            "Epoch [21], Batch [122], Train Loss: 0.7805121541023254\n",
            "Epoch [21], Batch [123], Train Loss: 0.7066563963890076\n",
            "Epoch [21], Batch [124], Train Loss: 0.7893313765525818\n",
            "Epoch [21], Batch [125], Train Loss: 0.8609836101531982\n",
            "Epoch [21], Batch [126], Train Loss: 0.6639968752861023\n",
            "Epoch [21], Batch [127], Train Loss: 0.7896546125411987\n",
            "Epoch [21], Batch [128], Train Loss: 0.7401694655418396\n",
            "Epoch [21], Batch [129], Train Loss: 0.9067903161048889\n",
            "Epoch [21], Batch [130], Train Loss: 0.8330188989639282\n",
            "Epoch [21], Batch [131], Train Loss: 0.7560693025588989\n",
            "Epoch [21], Batch [132], Train Loss: 0.8744529485702515\n",
            "Epoch [21], Batch [133], Train Loss: 0.8259760737419128\n",
            "Epoch [21], Batch [134], Train Loss: 0.9224677085876465\n",
            "Epoch [21], Batch [135], Train Loss: 0.7904281616210938\n",
            "Epoch [21], Batch [136], Train Loss: 0.8285987377166748\n",
            "Epoch [21], Batch [137], Train Loss: 0.8721541166305542\n",
            "Epoch [21], Batch [138], Train Loss: 0.8329743146896362\n",
            "Epoch [21], Batch [139], Train Loss: 0.8971260786056519\n",
            "Epoch [21], Batch [140], Train Loss: 0.8069062829017639\n",
            "Epoch [21], Batch [141], Train Loss: 1.0088218450546265\n",
            "Epoch [21], Batch [142], Train Loss: 0.6501875519752502\n",
            "Epoch [21], Batch [143], Train Loss: 0.8617986440658569\n",
            "Epoch [21], Batch [144], Train Loss: 0.8710567355155945\n",
            "Epoch [21], Batch [145], Train Loss: 0.7678086757659912\n",
            "Epoch [21], Batch [146], Train Loss: 0.8641747236251831\n",
            "Epoch [21], Batch [147], Train Loss: 0.8280990719795227\n",
            "Epoch [21], Batch [148], Train Loss: 0.7272016406059265\n",
            "Epoch [21], Batch [149], Train Loss: 0.7067217826843262\n",
            "Epoch [21], Batch [150], Train Loss: 0.677585780620575\n",
            "Epoch [21], Batch [151], Train Loss: 0.9640945196151733\n",
            "Epoch [21], Batch [152], Train Loss: 0.892562747001648\n",
            "Epoch [21], Batch [153], Train Loss: 0.907813549041748\n",
            "Epoch [21], Batch [154], Train Loss: 0.6137763261795044\n",
            "Epoch [21], Batch [155], Train Loss: 0.9592294096946716\n",
            "Epoch [21], Batch [156], Train Loss: 0.7460070252418518\n",
            "Epoch [21], Batch [157], Train Loss: 0.928080141544342\n",
            "Epoch [21], Batch [158], Train Loss: 1.070906639099121\n",
            "Epoch [21], Batch [159], Train Loss: 0.9047979116439819\n",
            "Epoch [21], Batch [160], Train Loss: 0.8699312806129456\n",
            "Epoch [21], Batch [161], Train Loss: 0.6955689787864685\n",
            "Epoch [21], Batch [162], Train Loss: 0.8309316039085388\n",
            "Epoch [21], Batch [163], Train Loss: 0.9330935478210449\n",
            "Epoch [21], Batch [164], Train Loss: 0.8059483766555786\n",
            "Epoch [21], Batch [165], Train Loss: 1.017472267150879\n",
            "Epoch [21], Batch [166], Train Loss: 0.8251699805259705\n",
            "Epoch [21], Batch [167], Train Loss: 0.7975159883499146\n",
            "Epoch [21], Batch [168], Train Loss: 0.7560085654258728\n",
            "Epoch [21], Batch [169], Train Loss: 0.7868619561195374\n",
            "Epoch [21], Batch [170], Train Loss: 0.9293237328529358\n",
            "Epoch [21], Batch [171], Train Loss: 0.825674831867218\n",
            "Epoch [21], Batch [172], Train Loss: 0.7473484873771667\n",
            "Epoch [21], Batch [173], Train Loss: 0.6207808256149292\n",
            "Epoch [21], Batch [174], Train Loss: 0.8817152380943298\n",
            "Epoch [21], Batch [175], Train Loss: 0.6230345368385315\n",
            "Epoch [21], Batch [176], Train Loss: 0.8003429770469666\n",
            "Epoch [21], Batch [177], Train Loss: 0.949556291103363\n",
            "Epoch [21], Batch [178], Train Loss: 0.9138020873069763\n",
            "Epoch [21], Batch [179], Train Loss: 0.7924739718437195\n",
            "Epoch [21], Batch [180], Train Loss: 0.722233772277832\n",
            "Epoch [21], Batch [181], Train Loss: 1.1710658073425293\n",
            "Epoch [21], Batch [182], Train Loss: 0.6634368896484375\n",
            "Epoch [21], Batch [183], Train Loss: 0.8074988126754761\n",
            "Epoch [21], Batch [184], Train Loss: 0.7988522052764893\n",
            "Epoch [21], Batch [185], Train Loss: 1.0617402791976929\n",
            "Epoch [21], Batch [186], Train Loss: 0.7077732086181641\n",
            "Epoch [21], Batch [187], Train Loss: 0.8522045612335205\n",
            "Epoch [21], Batch [188], Train Loss: 0.820304811000824\n",
            "Epoch [21], Batch [189], Train Loss: 0.7337596416473389\n",
            "Epoch [21], Batch [190], Train Loss: 0.9940546154975891\n",
            "Epoch [21], Batch [191], Train Loss: 0.7151992321014404\n",
            "Epoch [21], Batch [192], Train Loss: 0.6674367785453796\n",
            "Epoch [21], Batch [193], Train Loss: 0.9256573915481567\n",
            "Epoch [21], Batch [194], Train Loss: 0.8152210712432861\n",
            "Epoch [21], Batch [195], Train Loss: 0.784357488155365\n",
            "Epoch [21], Batch [196], Train Loss: 0.6543407440185547\n",
            "Epoch [21], Batch [197], Train Loss: 0.6809772253036499\n",
            "Epoch [21], Batch [198], Train Loss: 0.6371952295303345\n",
            "Epoch [21], Batch [199], Train Loss: 0.9238438606262207\n",
            "Epoch [21], Batch [200], Train Loss: 0.7995820045471191\n",
            "Epoch [21], Batch [201], Train Loss: 0.7301063537597656\n",
            "Epoch [21], Batch [202], Train Loss: 0.7480056285858154\n",
            "Epoch [21], Batch [203], Train Loss: 1.0598208904266357\n",
            "Epoch [21], Batch [204], Train Loss: 0.9656962752342224\n",
            "Epoch [21], Batch [205], Train Loss: 0.9486067295074463\n",
            "Epoch [21], Batch [206], Train Loss: 0.9469348192214966\n",
            "Epoch [21], Batch [207], Train Loss: 0.7359232902526855\n",
            "Epoch [21], Batch [208], Train Loss: 0.7346803545951843\n",
            "Epoch [21], Batch [209], Train Loss: 0.7804336547851562\n",
            "Epoch [21], Batch [210], Train Loss: 0.9157513976097107\n",
            "Epoch [21], Batch [211], Train Loss: 0.7198960185050964\n",
            "Epoch [21], Batch [212], Train Loss: 0.95792555809021\n",
            "Epoch [21], Batch [213], Train Loss: 0.9417408108711243\n",
            "Epoch [21], Batch [214], Train Loss: 0.8955206871032715\n",
            "Epoch [21], Batch [215], Train Loss: 0.6946512460708618\n",
            "Epoch [21], Batch [216], Train Loss: 0.873833179473877\n",
            "Epoch [21], Batch [217], Train Loss: 0.9209918975830078\n",
            "Epoch [21], Batch [218], Train Loss: 0.7300753593444824\n",
            "Epoch [21], Batch [219], Train Loss: 0.6923085451126099\n",
            "Epoch [21], Batch [220], Train Loss: 0.8884262442588806\n",
            "Epoch [21], Batch [221], Train Loss: 0.8178129196166992\n",
            "Epoch [21], Batch [222], Train Loss: 0.9899446964263916\n",
            "Epoch [21], Batch [223], Train Loss: 0.7064623832702637\n",
            "Epoch [21], Batch [224], Train Loss: 0.8060675859451294\n",
            "Epoch [21], Batch [225], Train Loss: 0.814625084400177\n",
            "Epoch [21], Batch [226], Train Loss: 0.6637734174728394\n",
            "Epoch [21], Batch [227], Train Loss: 0.6797396540641785\n",
            "Epoch [21], Batch [228], Train Loss: 0.792427659034729\n",
            "Epoch [21], Batch [229], Train Loss: 0.9059420824050903\n",
            "Epoch [21], Batch [230], Train Loss: 0.742084801197052\n",
            "Epoch [21], Batch [231], Train Loss: 0.7620488405227661\n",
            "Epoch [21], Batch [232], Train Loss: 0.8341813087463379\n",
            "Epoch [21], Batch [233], Train Loss: 0.8487693667411804\n",
            "Epoch [21], Batch [234], Train Loss: 0.8165298104286194\n",
            "Epoch [21], Batch [235], Train Loss: 0.8132336139678955\n",
            "Epoch [21], Batch [236], Train Loss: 0.9728041291236877\n",
            "Epoch [21], Batch [237], Train Loss: 0.7187243103981018\n",
            "Epoch [21], Batch [238], Train Loss: 0.8253987431526184\n",
            "Epoch [21], Batch [239], Train Loss: 0.780710756778717\n",
            "Epoch [21], Batch [240], Train Loss: 0.9405660033226013\n",
            "Epoch [21], Batch [241], Train Loss: 1.1407544612884521\n",
            "Epoch [21], Batch [242], Train Loss: 0.823996901512146\n",
            "Epoch [21], Batch [243], Train Loss: 0.936302900314331\n",
            "Epoch [21], Batch [244], Train Loss: 1.1828396320343018\n",
            "Epoch [21], Batch [245], Train Loss: 0.965224027633667\n",
            "Epoch [21], Batch [246], Train Loss: 0.7979534864425659\n",
            "Epoch [21], Batch [247], Train Loss: 0.7481124401092529\n",
            "Epoch [21], Batch [248], Train Loss: 0.6180374026298523\n",
            "Epoch [21], Batch [249], Train Loss: 1.261476993560791\n",
            "Epoch [21], Batch [250], Train Loss: 0.9779159426689148\n",
            "Epoch [21], Batch [251], Train Loss: 0.8854482173919678\n",
            "Epoch [21], Batch [252], Train Loss: 0.7617850303649902\n",
            "Epoch [21], Batch [253], Train Loss: 0.8102301955223083\n",
            "Epoch [21], Batch [254], Train Loss: 0.6882063746452332\n",
            "Epoch [21], Batch [255], Train Loss: 0.8711942434310913\n",
            "Epoch [21], Batch [256], Train Loss: 0.6680415868759155\n",
            "Epoch [21], Batch [257], Train Loss: 0.7841168642044067\n",
            "Epoch [21], Batch [258], Train Loss: 0.8342787027359009\n",
            "Epoch [21], Batch [259], Train Loss: 0.8755614161491394\n",
            "Epoch [21], Batch [260], Train Loss: 0.9323700070381165\n",
            "Epoch [21], Batch [261], Train Loss: 0.7231056690216064\n",
            "Epoch [21], Batch [262], Train Loss: 0.7168071269989014\n",
            "Epoch [21], Batch [263], Train Loss: 0.8222847580909729\n",
            "Epoch [21], Batch [264], Train Loss: 0.7980427742004395\n",
            "Epoch [21], Batch [265], Train Loss: 0.8608522415161133\n",
            "Epoch [21], Batch [266], Train Loss: 0.8436627388000488\n",
            "Epoch [21], Batch [267], Train Loss: 0.7447105646133423\n",
            "Epoch [21], Batch [268], Train Loss: 0.6991568207740784\n",
            "Epoch [21], Batch [269], Train Loss: 0.7165048122406006\n",
            "Epoch [21], Batch [270], Train Loss: 0.8750829696655273\n",
            "Epoch [21], Batch [271], Train Loss: 0.9265971779823303\n",
            "Epoch [21], Batch [272], Train Loss: 0.7711647152900696\n",
            "Epoch [21], Batch [273], Train Loss: 0.8447763919830322\n",
            "Epoch [21], Batch [274], Train Loss: 0.7696244120597839\n",
            "Epoch [21], Batch [275], Train Loss: 0.8923567533493042\n",
            "Epoch [21], Batch [276], Train Loss: 0.783270001411438\n",
            "Epoch [21], Batch [277], Train Loss: 0.5951534509658813\n",
            "Epoch [21], Batch [278], Train Loss: 0.693313717842102\n",
            "Epoch [21], Batch [279], Train Loss: 0.8259878754615784\n",
            "Epoch [21], Batch [280], Train Loss: 0.7664107084274292\n",
            "Epoch [21], Batch [281], Train Loss: 0.8211765885353088\n",
            "Epoch [21], Batch [282], Train Loss: 0.7786258459091187\n",
            "Epoch [21], Batch [283], Train Loss: 0.8023877143859863\n",
            "Epoch [21], Batch [284], Train Loss: 0.9720789194107056\n",
            "Epoch [21], Batch [285], Train Loss: 0.8784397840499878\n",
            "Epoch [21], Batch [286], Train Loss: 0.8382328748703003\n",
            "Epoch [21], Batch [287], Train Loss: 1.267684817314148\n",
            "Epoch [21], Batch [288], Train Loss: 0.6429462432861328\n",
            "Epoch [21], Batch [289], Train Loss: 0.9410629272460938\n",
            "Epoch [21], Batch [290], Train Loss: 0.7770472764968872\n",
            "Epoch [21], Batch [291], Train Loss: 0.6082797050476074\n",
            "Epoch [21], Batch [292], Train Loss: 0.7076367139816284\n",
            "Epoch [21], Batch [293], Train Loss: 0.7273449897766113\n",
            "Epoch [21], Batch [294], Train Loss: 0.7757781147956848\n",
            "Epoch [21], Batch [295], Train Loss: 0.7889722585678101\n",
            "Epoch [21], Batch [296], Train Loss: 0.7333027124404907\n",
            "Epoch [21], Batch [297], Train Loss: 0.9343651533126831\n",
            "Epoch [21], Batch [298], Train Loss: 0.8049493432044983\n",
            "Epoch [21], Batch [299], Train Loss: 0.7138750553131104\n",
            "Epoch [21], Batch [300], Train Loss: 0.6948628425598145\n",
            "Epoch [21], Batch [301], Train Loss: 0.6899665594100952\n",
            "Epoch [21], Batch [302], Train Loss: 0.7527715563774109\n",
            "Epoch [21], Batch [303], Train Loss: 0.8666431903839111\n",
            "Epoch [21], Batch [304], Train Loss: 0.7802375555038452\n",
            "Epoch [21], Batch [305], Train Loss: 0.853430986404419\n",
            "Epoch [21], Batch [306], Train Loss: 1.164494276046753\n",
            "Epoch [21], Batch [307], Train Loss: 0.9224147200584412\n",
            "Epoch [21], Batch [308], Train Loss: 0.7141911387443542\n",
            "Epoch [21], Batch [309], Train Loss: 1.0123205184936523\n",
            "Epoch [21], Batch [310], Train Loss: 0.7268941402435303\n",
            "Epoch [21], Batch [311], Train Loss: 0.7095034122467041\n",
            "Epoch [21], Batch [312], Train Loss: 0.8749045133590698\n",
            "Epoch [21], Batch [313], Train Loss: 0.8192753791809082\n",
            "Epoch [21], Batch [314], Train Loss: 1.0005674362182617\n",
            "Epoch [21], Batch [315], Train Loss: 0.8983274102210999\n",
            "Epoch [21], Batch [316], Train Loss: 0.7485886812210083\n",
            "Epoch [21], Batch [317], Train Loss: 0.8348325490951538\n",
            "Epoch [21], Batch [318], Train Loss: 0.591107964515686\n",
            "Epoch [21], Batch [319], Train Loss: 0.8693146705627441\n",
            "Epoch [21], Batch [320], Train Loss: 0.7335831522941589\n",
            "Epoch [21], Batch [321], Train Loss: 0.8057007193565369\n",
            "Epoch [21], Batch [322], Train Loss: 0.775381326675415\n",
            "Epoch [21], Batch [323], Train Loss: 0.7909660339355469\n",
            "Epoch [21], Batch [324], Train Loss: 0.8035800457000732\n",
            "Epoch [21], Batch [325], Train Loss: 0.9244151711463928\n",
            "Epoch [21], Batch [326], Train Loss: 0.8189755082130432\n",
            "Epoch [21], Batch [327], Train Loss: 0.7122325897216797\n",
            "Epoch [21], Batch [328], Train Loss: 0.7273313999176025\n",
            "Epoch [21], Batch [329], Train Loss: 0.7568568587303162\n",
            "Epoch [21], Batch [330], Train Loss: 1.0150530338287354\n",
            "Epoch [21], Batch [331], Train Loss: 0.8063085079193115\n",
            "Epoch [21], Batch [332], Train Loss: 0.8181546926498413\n",
            "Epoch [21], Batch [333], Train Loss: 0.6350927352905273\n",
            "Epoch [21], Batch [334], Train Loss: 0.7462135553359985\n",
            "Epoch [21], Batch [335], Train Loss: 0.8099464178085327\n",
            "Epoch [21], Batch [336], Train Loss: 0.803882896900177\n",
            "Epoch [21], Batch [337], Train Loss: 0.7739766836166382\n",
            "Epoch [21], Batch [338], Train Loss: 0.8323934674263\n",
            "Epoch [21], Batch [339], Train Loss: 0.9363185167312622\n",
            "Epoch [21], Batch [340], Train Loss: 0.7546310424804688\n",
            "Epoch [21], Batch [341], Train Loss: 1.0829042196273804\n",
            "Epoch [21], Batch [342], Train Loss: 0.6964309811592102\n",
            "Epoch [21], Batch [343], Train Loss: 0.9019796848297119\n",
            "Epoch [21], Batch [344], Train Loss: 0.7581883668899536\n",
            "Epoch [21], Batch [345], Train Loss: 0.8369627594947815\n",
            "Epoch [21], Batch [346], Train Loss: 1.1001434326171875\n",
            "Epoch [21], Batch [347], Train Loss: 0.8737576007843018\n",
            "Epoch [21], Batch [348], Train Loss: 0.8018889427185059\n",
            "Epoch [21], Batch [349], Train Loss: 0.8289090991020203\n",
            "Epoch [21], Batch [350], Train Loss: 0.9240615367889404\n",
            "Epoch [21], Batch [351], Train Loss: 0.801198422908783\n",
            "Epoch [21], Batch [352], Train Loss: 0.9797793030738831\n",
            "Epoch [21], Batch [353], Train Loss: 0.9951295256614685\n",
            "Epoch [21], Batch [354], Train Loss: 0.775896430015564\n",
            "Epoch [21], Batch [355], Train Loss: 0.8277409076690674\n",
            "Epoch [21], Batch [356], Train Loss: 0.9421076774597168\n",
            "Epoch [21], Batch [357], Train Loss: 0.732822060585022\n",
            "Epoch [21], Batch [358], Train Loss: 0.8206020593643188\n",
            "Epoch [21], Batch [359], Train Loss: 0.6211276650428772\n",
            "Epoch [21], Batch [360], Train Loss: 0.7176517248153687\n",
            "Epoch [21], Batch [361], Train Loss: 0.8973581194877625\n",
            "Epoch [21], Batch [362], Train Loss: 0.9165379405021667\n",
            "Epoch [21], Batch [363], Train Loss: 0.8132571578025818\n",
            "Epoch [21], Batch [364], Train Loss: 0.7512744069099426\n",
            "Epoch [21], Batch [365], Train Loss: 0.993988573551178\n",
            "Epoch [21], Batch [366], Train Loss: 0.9231081604957581\n",
            "Epoch [21], Batch [367], Train Loss: 0.5142195224761963\n",
            "Epoch [21], Batch [368], Train Loss: 0.6786163449287415\n",
            "Epoch [21], Batch [369], Train Loss: 0.8704544305801392\n",
            "Epoch [21], Batch [370], Train Loss: 1.073193907737732\n",
            "Epoch [21], Batch [371], Train Loss: 0.8385118842124939\n",
            "Epoch [21], Batch [372], Train Loss: 0.8790557980537415\n",
            "Epoch [21], Batch [373], Train Loss: 0.9388682246208191\n",
            "Epoch [21], Batch [374], Train Loss: 0.8338721990585327\n",
            "Epoch [21], Batch [375], Train Loss: 0.7230525016784668\n",
            "Epoch [21], Batch [376], Train Loss: 0.7928399443626404\n",
            "Epoch [21], Batch [377], Train Loss: 0.9115157127380371\n",
            "Epoch [21], Batch [378], Train Loss: 0.8515132665634155\n",
            "Epoch [21], Batch [379], Train Loss: 0.7218013405799866\n",
            "Epoch [21], Batch [380], Train Loss: 0.7072476744651794\n",
            "Epoch [21], Batch [381], Train Loss: 0.6794692873954773\n",
            "Epoch [21], Batch [382], Train Loss: 0.8049303293228149\n",
            "Epoch [21], Batch [383], Train Loss: 0.8867271542549133\n",
            "Epoch [21], Batch [384], Train Loss: 0.681993842124939\n",
            "Epoch [21], Batch [385], Train Loss: 0.8059395551681519\n",
            "Epoch [21], Batch [386], Train Loss: 0.9863775968551636\n",
            "Epoch [21], Batch [387], Train Loss: 0.756885290145874\n",
            "Epoch [21], Batch [388], Train Loss: 1.0716261863708496\n",
            "Epoch [21], Batch [389], Train Loss: 0.6658680438995361\n",
            "Epoch [21], Batch [390], Train Loss: 0.8716399073600769\n",
            "Epoch [21], Batch [391], Train Loss: 0.8509605526924133\n",
            "Epoch [21], Batch [392], Train Loss: 0.6901596188545227\n",
            "Epoch [21], Batch [393], Train Loss: 0.9265943169593811\n",
            "Epoch [21], Batch [394], Train Loss: 0.8225347995758057\n",
            "Epoch [21], Batch [395], Train Loss: 0.8142430782318115\n",
            "Epoch [21], Batch [396], Train Loss: 0.9493171572685242\n",
            "Epoch [21], Batch [397], Train Loss: 0.9857988953590393\n",
            "Epoch [21], Batch [398], Train Loss: 0.7921119928359985\n",
            "Epoch [21], Batch [399], Train Loss: 0.9804904460906982\n",
            "Epoch [21], Batch [400], Train Loss: 0.9294718503952026\n",
            "Epoch [21], Batch [401], Train Loss: 0.8260450959205627\n",
            "Epoch [21], Batch [402], Train Loss: 0.6654183864593506\n",
            "Epoch [21], Batch [403], Train Loss: 0.6244068145751953\n",
            "Epoch [21], Batch [404], Train Loss: 0.9468697309494019\n",
            "Epoch [21], Batch [405], Train Loss: 0.6962708234786987\n",
            "Epoch [21], Batch [406], Train Loss: 0.8161121010780334\n",
            "Epoch [21], Batch [407], Train Loss: 0.6892048120498657\n",
            "Epoch [21], Batch [408], Train Loss: 0.8920745253562927\n",
            "Epoch [21], Batch [409], Train Loss: 0.6948244571685791\n",
            "Epoch [21], Batch [410], Train Loss: 0.7367202639579773\n",
            "Epoch [21], Batch [411], Train Loss: 0.8096415996551514\n",
            "Epoch [21], Batch [412], Train Loss: 0.9056417942047119\n",
            "Epoch [21], Batch [413], Train Loss: 0.9198806285858154\n",
            "Epoch [21], Batch [414], Train Loss: 0.766951322555542\n",
            "Epoch [21], Batch [415], Train Loss: 0.8031755089759827\n",
            "Epoch [21], Batch [416], Train Loss: 0.8433844447135925\n",
            "Epoch [21], Batch [417], Train Loss: 0.9612665772438049\n",
            "Epoch [21], Batch [418], Train Loss: 0.879718005657196\n",
            "Epoch [21], Batch [419], Train Loss: 0.8787386417388916\n",
            "Epoch [21], Batch [420], Train Loss: 0.8066989779472351\n",
            "Epoch [21], Batch [421], Train Loss: 1.0241680145263672\n",
            "Epoch [21], Batch [422], Train Loss: 0.8612072467803955\n",
            "Epoch [21], Batch [423], Train Loss: 0.8644997477531433\n",
            "Epoch [21], Batch [424], Train Loss: 1.0178897380828857\n",
            "Epoch [21], Batch [425], Train Loss: 0.7329544425010681\n",
            "Epoch [21], Batch [426], Train Loss: 1.066920518875122\n",
            "Epoch [21], Batch [427], Train Loss: 0.8293130993843079\n",
            "Epoch [21], Batch [428], Train Loss: 0.8636505007743835\n",
            "Epoch [21], Batch [429], Train Loss: 0.6854285001754761\n",
            "Epoch [21], Batch [430], Train Loss: 0.7656955718994141\n",
            "Epoch [21], Batch [431], Train Loss: 0.788115382194519\n",
            "Epoch [21], Batch [432], Train Loss: 1.0028038024902344\n",
            "Epoch [21], Batch [433], Train Loss: 1.0969902276992798\n",
            "Epoch [21], Batch [434], Train Loss: 0.7165808081626892\n",
            "Epoch [21], Batch [435], Train Loss: 1.0260459184646606\n",
            "Epoch [21], Batch [436], Train Loss: 0.7387279868125916\n",
            "Epoch [21], Batch [437], Train Loss: 0.7203497290611267\n",
            "Epoch [21], Batch [438], Train Loss: 0.744529664516449\n",
            "Epoch [21], Batch [439], Train Loss: 0.753402590751648\n",
            "Epoch [21], Batch [440], Train Loss: 0.8027220964431763\n",
            "Epoch [21], Batch [441], Train Loss: 1.0097075700759888\n",
            "Epoch [21], Batch [442], Train Loss: 0.8434174060821533\n",
            "Epoch [21], Batch [443], Train Loss: 0.6950227618217468\n",
            "Epoch [21], Batch [444], Train Loss: 0.8365956544876099\n",
            "Epoch [21], Batch [445], Train Loss: 0.8971512317657471\n",
            "Epoch [21], Batch [446], Train Loss: 0.8028551936149597\n",
            "Epoch [21], Batch [447], Train Loss: 0.7056480050086975\n",
            "Epoch [21], Batch [448], Train Loss: 0.8120266199111938\n",
            "Epoch [21], Batch [449], Train Loss: 0.7928005456924438\n",
            "Epoch [21], Batch [450], Train Loss: 0.8561561703681946\n",
            "Epoch [21], Batch [451], Train Loss: 0.8018468618392944\n",
            "Epoch [21], Batch [452], Train Loss: 0.7463181614875793\n",
            "Epoch [21], Batch [453], Train Loss: 0.6290709972381592\n",
            "Epoch [21], Batch [454], Train Loss: 0.6799608469009399\n",
            "Epoch [21], Batch [455], Train Loss: 1.0363370180130005\n",
            "Epoch [21], Batch [456], Train Loss: 0.7151272892951965\n",
            "Epoch [21], Batch [457], Train Loss: 0.995588481426239\n",
            "Epoch [21], Batch [458], Train Loss: 0.7940240502357483\n",
            "Epoch [21], Batch [459], Train Loss: 0.9058201909065247\n",
            "Epoch [21], Batch [460], Train Loss: 0.8531187772750854\n",
            "Epoch [21], Batch [461], Train Loss: 0.8089388012886047\n",
            "Epoch [21], Batch [462], Train Loss: 0.9443268775939941\n",
            "Epoch [21], Batch [463], Train Loss: 0.6490342020988464\n",
            "Epoch [21], Batch [464], Train Loss: 0.6893404126167297\n",
            "Epoch [21], Batch [465], Train Loss: 0.7226042747497559\n",
            "Epoch [21], Batch [466], Train Loss: 0.9733860492706299\n",
            "Epoch [21], Batch [467], Train Loss: 0.753951370716095\n",
            "Epoch [21], Batch [468], Train Loss: 0.9733018279075623\n",
            "Epoch [21], Batch [469], Train Loss: 0.7303920984268188\n",
            "Epoch [21], Batch [470], Train Loss: 0.6790011525154114\n",
            "Epoch [21], Batch [471], Train Loss: 0.7658600211143494\n",
            "Epoch [21], Batch [472], Train Loss: 1.0734864473342896\n",
            "Epoch [21], Batch [473], Train Loss: 0.7643068432807922\n",
            "Epoch [21], Batch [474], Train Loss: 0.8853247165679932\n",
            "Epoch [21], Batch [475], Train Loss: 0.8983700275421143\n",
            "Epoch [21], Batch [476], Train Loss: 0.7952317595481873\n",
            "Epoch [21], Batch [477], Train Loss: 0.7319475412368774\n",
            "Epoch [21], Batch [478], Train Loss: 0.7984941601753235\n",
            "Epoch [21], Batch [479], Train Loss: 0.6454129815101624\n",
            "Epoch [21], Batch [480], Train Loss: 0.7271275520324707\n",
            "Epoch [21], Batch [481], Train Loss: 0.8232067823410034\n",
            "Epoch [21], Batch [482], Train Loss: 0.6656281352043152\n",
            "Epoch [21], Batch [483], Train Loss: 0.7718789577484131\n",
            "Epoch [21], Batch [484], Train Loss: 0.6465744376182556\n",
            "Epoch [21], Batch [485], Train Loss: 0.9808050990104675\n",
            "Epoch [21], Batch [486], Train Loss: 0.892561674118042\n",
            "Epoch [21], Batch [487], Train Loss: 1.0021536350250244\n",
            "Epoch [21], Batch [488], Train Loss: 0.6871955990791321\n",
            "Epoch [21], Batch [489], Train Loss: 0.8002088069915771\n",
            "Epoch [21], Batch [490], Train Loss: 0.9505206346511841\n",
            "Epoch [21], Batch [491], Train Loss: 0.7504571080207825\n",
            "Epoch [21], Batch [492], Train Loss: 0.7152012586593628\n",
            "Epoch [21], Batch [493], Train Loss: 0.7918161153793335\n",
            "Epoch [21], Batch [494], Train Loss: 0.8201442956924438\n",
            "Epoch [21], Batch [495], Train Loss: 0.854904055595398\n",
            "Epoch [21], Batch [496], Train Loss: 0.7912554740905762\n",
            "Epoch [21], Batch [497], Train Loss: 0.6821638345718384\n",
            "Epoch [21], Batch [498], Train Loss: 0.8921620845794678\n",
            "Epoch [21], Batch [499], Train Loss: 0.7653211355209351\n",
            "Epoch [21], Batch [500], Train Loss: 1.0263798236846924\n",
            "Epoch [21], Batch [501], Train Loss: 0.7233623266220093\n",
            "Epoch [21], Batch [502], Train Loss: 0.8258167505264282\n",
            "Epoch [21], Batch [503], Train Loss: 0.9402334094047546\n",
            "Epoch [21], Batch [504], Train Loss: 0.7636165618896484\n",
            "Epoch [21], Batch [505], Train Loss: 0.6421413421630859\n",
            "Epoch [21], Batch [506], Train Loss: 0.9245147705078125\n",
            "Epoch [21], Batch [507], Train Loss: 0.8345539569854736\n",
            "Epoch [21], Batch [508], Train Loss: 0.71913743019104\n",
            "Epoch [21], Batch [509], Train Loss: 0.7440036535263062\n",
            "Epoch [21], Batch [510], Train Loss: 0.7911419868469238\n",
            "Epoch [21], Batch [511], Train Loss: 0.6699473857879639\n",
            "Epoch [21], Batch [512], Train Loss: 0.7540213465690613\n",
            "Epoch [21], Batch [513], Train Loss: 0.827994704246521\n",
            "Epoch [21], Batch [514], Train Loss: 0.6893951892852783\n",
            "Epoch [21], Batch [515], Train Loss: 0.8467037677764893\n",
            "Epoch [21], Batch [516], Train Loss: 0.6482635736465454\n",
            "Epoch [21], Batch [517], Train Loss: 0.9357457756996155\n",
            "Epoch [21], Batch [518], Train Loss: 1.011767029762268\n",
            "Epoch [21], Batch [519], Train Loss: 0.8299667835235596\n",
            "Epoch [21], Batch [520], Train Loss: 1.0289204120635986\n",
            "Epoch [21], Batch [521], Train Loss: 0.81483393907547\n",
            "Epoch [21], Batch [522], Train Loss: 0.9904782772064209\n",
            "Epoch [21], Batch [523], Train Loss: 0.7491934299468994\n",
            "Epoch [21], Batch [524], Train Loss: 0.7519603371620178\n",
            "Epoch [21], Batch [525], Train Loss: 0.7534391283988953\n",
            "Epoch [21], Batch [526], Train Loss: 0.8834509253501892\n",
            "Epoch [21], Batch [527], Train Loss: 0.8562130928039551\n",
            "Epoch [21], Batch [528], Train Loss: 0.696501612663269\n",
            "Epoch [21], Batch [529], Train Loss: 0.7647759318351746\n",
            "Epoch [21], Batch [530], Train Loss: 1.1640241146087646\n",
            "Epoch [21], Batch [531], Train Loss: 0.8214780688285828\n",
            "Epoch [21], Batch [532], Train Loss: 0.7776548862457275\n",
            "Epoch [21], Batch [533], Train Loss: 0.8399611711502075\n",
            "Epoch [21], Batch [534], Train Loss: 0.7771753668785095\n",
            "Epoch [21], Batch [535], Train Loss: 0.8140397667884827\n",
            "Epoch [21], Batch [536], Train Loss: 0.8660063743591309\n",
            "Epoch [21], Batch [537], Train Loss: 0.6606191992759705\n",
            "Epoch [21], Batch [538], Train Loss: 0.9014599323272705\n",
            "Epoch [21], Batch [539], Train Loss: 0.906035840511322\n",
            "Epoch [21], Batch [540], Train Loss: 0.8162074685096741\n",
            "Epoch [21], Batch [541], Train Loss: 0.7032343745231628\n",
            "Epoch [21], Batch [542], Train Loss: 0.7404949069023132\n",
            "Epoch [21], Batch [543], Train Loss: 0.9291433095932007\n",
            "Epoch [21], Batch [544], Train Loss: 0.6568637490272522\n",
            "Epoch [21], Batch [545], Train Loss: 0.596666157245636\n",
            "Epoch [21], Batch [546], Train Loss: 0.9796016216278076\n",
            "Epoch [21], Batch [547], Train Loss: 0.9389355182647705\n",
            "Epoch [21], Batch [548], Train Loss: 0.9295749068260193\n",
            "Epoch [21], Batch [549], Train Loss: 0.7991206049919128\n",
            "Epoch [21], Batch [550], Train Loss: 0.776705265045166\n",
            "Epoch [21], Batch [551], Train Loss: 0.9086254835128784\n",
            "Epoch [21], Batch [552], Train Loss: 1.113907814025879\n",
            "Epoch [21], Batch [553], Train Loss: 0.8310908079147339\n",
            "Epoch [21], Batch [554], Train Loss: 0.7295025587081909\n",
            "Epoch [21], Batch [555], Train Loss: 0.7206748723983765\n",
            "Epoch [21], Batch [556], Train Loss: 0.9675179719924927\n",
            "Epoch [21], Batch [557], Train Loss: 0.6178514957427979\n",
            "Epoch [21], Batch [558], Train Loss: 0.751203715801239\n",
            "Epoch [21], Batch [559], Train Loss: 0.7263520359992981\n",
            "Epoch [21], Batch [560], Train Loss: 0.8405451774597168\n",
            "Epoch [21], Batch [561], Train Loss: 0.6819926500320435\n",
            "Epoch [21], Batch [562], Train Loss: 0.72698974609375\n",
            "Epoch [21], Batch [563], Train Loss: 0.6164814233779907\n",
            "Epoch [21], Batch [564], Train Loss: 0.7568843364715576\n",
            "Epoch [21], Batch [565], Train Loss: 0.8833281993865967\n",
            "Epoch [21], Batch [566], Train Loss: 0.9117329716682434\n",
            "Epoch [21], Batch [567], Train Loss: 0.8305569887161255\n",
            "Epoch [21], Batch [568], Train Loss: 0.8753213882446289\n",
            "Epoch [21], Batch [569], Train Loss: 0.9732258319854736\n",
            "Epoch [21], Batch [570], Train Loss: 0.9372611045837402\n",
            "Epoch [21], Batch [571], Train Loss: 0.6955012679100037\n",
            "Epoch [21], Batch [572], Train Loss: 0.6271100044250488\n",
            "Epoch [21], Batch [573], Train Loss: 0.9989468455314636\n",
            "Epoch [21], Batch [574], Train Loss: 0.9404283165931702\n",
            "Epoch [21], Batch [575], Train Loss: 0.7690969705581665\n",
            "Epoch [21], Batch [576], Train Loss: 0.7282440662384033\n",
            "Epoch [21], Batch [577], Train Loss: 0.7060100436210632\n",
            "Epoch [21], Batch [578], Train Loss: 0.7452183365821838\n",
            "Epoch [21], Batch [579], Train Loss: 0.8799454569816589\n",
            "Epoch [21], Batch [580], Train Loss: 0.8526414036750793\n",
            "Epoch [21], Batch [581], Train Loss: 0.7617575526237488\n",
            "Epoch [21], Batch [582], Train Loss: 0.9722657799720764\n",
            "Epoch [21], Batch [583], Train Loss: 0.8817368149757385\n",
            "Epoch [21], Batch [584], Train Loss: 0.822752058506012\n",
            "Epoch [21], Batch [585], Train Loss: 0.8998204469680786\n",
            "Epoch [21], Batch [586], Train Loss: 0.6031379103660583\n",
            "Epoch [21], Batch [587], Train Loss: 0.936399519443512\n",
            "Epoch [21], Batch [588], Train Loss: 0.773273766040802\n",
            "Epoch [21], Batch [589], Train Loss: 0.8306572437286377\n",
            "Epoch [21], Batch [590], Train Loss: 0.9318983554840088\n",
            "Epoch [21], Batch [591], Train Loss: 0.8608123660087585\n",
            "Epoch [21], Batch [592], Train Loss: 0.7230402231216431\n",
            "Epoch [21], Batch [593], Train Loss: 0.7883620858192444\n",
            "Epoch [21], Batch [594], Train Loss: 1.0029147863388062\n",
            "Epoch [21], Batch [595], Train Loss: 0.7647366523742676\n",
            "Epoch [21], Batch [596], Train Loss: 0.6600856184959412\n",
            "Epoch [21], Batch [597], Train Loss: 0.7964218854904175\n",
            "Epoch [21], Batch [598], Train Loss: 0.737351655960083\n",
            "Epoch [21], Batch [599], Train Loss: 0.8037920594215393\n",
            "Epoch [21], Batch [600], Train Loss: 0.7448481321334839\n",
            "Epoch [21], Batch [601], Train Loss: 0.824563205242157\n",
            "Epoch [21], Batch [602], Train Loss: 0.8633819818496704\n",
            "Epoch [21], Batch [603], Train Loss: 0.7599572539329529\n",
            "Epoch [21], Batch [604], Train Loss: 1.1956462860107422\n",
            "Epoch [21], Batch [605], Train Loss: 0.9344205260276794\n",
            "Epoch [21], Batch [606], Train Loss: 0.8667325973510742\n",
            "Epoch [21], Batch [607], Train Loss: 0.6759452819824219\n",
            "Epoch [21], Batch [608], Train Loss: 0.9098985195159912\n",
            "Epoch [21], Batch [609], Train Loss: 0.7734049558639526\n",
            "Epoch [21], Batch [610], Train Loss: 0.7449887990951538\n",
            "Epoch [21], Batch [611], Train Loss: 0.9725070595741272\n",
            "Epoch [21], Batch [612], Train Loss: 0.9742989540100098\n",
            "Epoch [21], Batch [613], Train Loss: 0.8080898523330688\n",
            "Epoch [21], Batch [614], Train Loss: 0.7900044918060303\n",
            "Epoch [21], Batch [615], Train Loss: 0.7574294805526733\n",
            "Epoch [21], Batch [616], Train Loss: 0.7918697595596313\n",
            "Epoch [21], Batch [617], Train Loss: 0.7968685626983643\n",
            "Epoch [21], Batch [618], Train Loss: 0.7306175231933594\n",
            "Epoch [21], Batch [619], Train Loss: 0.7960770130157471\n",
            "Epoch [21], Batch [620], Train Loss: 0.6062827706336975\n",
            "Epoch [21], Batch [621], Train Loss: 0.8646858334541321\n",
            "Epoch [21], Batch [622], Train Loss: 0.6677196621894836\n",
            "Epoch [21], Batch [623], Train Loss: 0.8016895651817322\n",
            "Epoch [21], Batch [624], Train Loss: 0.8393171429634094\n",
            "Epoch [21], Batch [625], Train Loss: 0.7432297468185425\n",
            "Epoch [21], Batch [626], Train Loss: 0.6426485776901245\n",
            "Epoch [21], Batch [627], Train Loss: 0.9567890167236328\n",
            "Epoch [21], Batch [628], Train Loss: 0.8258312940597534\n",
            "Epoch [21], Batch [629], Train Loss: 0.8944408893585205\n",
            "Epoch [21], Batch [630], Train Loss: 0.6862938404083252\n",
            "Epoch [21], Batch [631], Train Loss: 0.7976908683776855\n",
            "Epoch [21], Batch [632], Train Loss: 1.0148310661315918\n",
            "Epoch [21], Batch [633], Train Loss: 0.7775694727897644\n",
            "Epoch [21], Batch [634], Train Loss: 0.841555655002594\n",
            "Epoch [21], Batch [635], Train Loss: 0.8006988167762756\n",
            "Epoch [21], Batch [636], Train Loss: 0.6976097822189331\n",
            "Epoch [21], Batch [637], Train Loss: 0.8431776762008667\n",
            "Epoch [21], Batch [638], Train Loss: 0.7948082089424133\n",
            "Epoch [21], Batch [639], Train Loss: 0.7742631435394287\n",
            "Epoch [21], Batch [640], Train Loss: 0.7754970788955688\n",
            "Epoch [21], Batch [641], Train Loss: 0.8887531757354736\n",
            "Epoch [21], Batch [642], Train Loss: 0.8874462246894836\n",
            "Epoch [21], Batch [643], Train Loss: 0.9158990383148193\n",
            "Epoch [21], Batch [644], Train Loss: 0.7854704856872559\n",
            "Epoch [21], Batch [645], Train Loss: 0.9294706583023071\n",
            "Epoch [21], Batch [646], Train Loss: 0.9634795784950256\n",
            "Epoch [21], Batch [647], Train Loss: 1.0359095335006714\n",
            "Epoch [21], Batch [648], Train Loss: 0.7360677719116211\n",
            "Epoch [21], Batch [649], Train Loss: 0.7652231454849243\n",
            "Epoch [21], Batch [650], Train Loss: 0.8689180612564087\n",
            "Epoch [21], Batch [651], Train Loss: 0.9927380681037903\n",
            "Epoch [21], Batch [652], Train Loss: 0.9376740455627441\n",
            "Epoch [21], Batch [653], Train Loss: 0.7168292999267578\n",
            "Epoch [21], Batch [654], Train Loss: 0.8895531892776489\n",
            "Epoch [21], Batch [655], Train Loss: 0.8429208993911743\n",
            "Epoch [21], Batch [656], Train Loss: 0.768444836139679\n",
            "Epoch [21], Batch [657], Train Loss: 0.7898836135864258\n",
            "Epoch [21], Batch [658], Train Loss: 0.7533499002456665\n",
            "Epoch [21], Batch [659], Train Loss: 0.8789822459220886\n",
            "Epoch [21], Batch [660], Train Loss: 0.9748633503913879\n",
            "Epoch [21], Batch [661], Train Loss: 0.7734611630439758\n",
            "Epoch [21], Batch [662], Train Loss: 0.8221729397773743\n",
            "Epoch [21], Batch [663], Train Loss: 1.054630994796753\n",
            "Epoch [21], Batch [664], Train Loss: 0.7279593348503113\n",
            "Epoch [21], Batch [665], Train Loss: 0.9257912039756775\n",
            "Epoch [21], Batch [666], Train Loss: 0.9517773389816284\n",
            "Epoch [21], Batch [667], Train Loss: 1.0182232856750488\n",
            "Epoch [21], Batch [668], Train Loss: 0.8257100582122803\n",
            "Epoch [21], Batch [669], Train Loss: 0.7773400545120239\n",
            "Epoch [21], Batch [670], Train Loss: 0.7729290723800659\n",
            "Epoch [21], Batch [671], Train Loss: 0.7663412094116211\n",
            "Epoch [21], Batch [672], Train Loss: 0.6097927093505859\n",
            "Epoch [21], Batch [673], Train Loss: 0.6755032539367676\n",
            "Epoch [21], Batch [674], Train Loss: 0.9974937438964844\n",
            "Epoch [21], Batch [675], Train Loss: 0.8613121509552002\n",
            "Epoch [21], Batch [676], Train Loss: 0.7496857643127441\n",
            "Epoch [21], Batch [677], Train Loss: 0.7977160215377808\n",
            "Epoch [21], Batch [678], Train Loss: 0.8402453660964966\n",
            "Epoch [21], Batch [679], Train Loss: 0.8773425817489624\n",
            "Epoch [21], Batch [680], Train Loss: 0.8636597990989685\n",
            "Epoch [21], Batch [681], Train Loss: 0.7508945465087891\n",
            "Epoch [21], Batch [682], Train Loss: 0.8416543006896973\n",
            "Epoch [21], Batch [683], Train Loss: 0.9614543318748474\n",
            "Epoch [21], Batch [684], Train Loss: 0.7786928415298462\n",
            "Epoch [21], Batch [685], Train Loss: 0.8953047394752502\n",
            "Epoch [21], Batch [686], Train Loss: 1.0595299005508423\n",
            "Epoch [21], Batch [687], Train Loss: 0.790999174118042\n",
            "Epoch [21], Batch [688], Train Loss: 0.7849997282028198\n",
            "Epoch [21], Batch [689], Train Loss: 0.7943788766860962\n",
            "Epoch [21], Batch [690], Train Loss: 0.8983453512191772\n",
            "Epoch [21], Batch [691], Train Loss: 0.6936836242675781\n",
            "Epoch [21], Batch [692], Train Loss: 0.630058765411377\n",
            "Epoch [21], Batch [693], Train Loss: 0.7407351136207581\n",
            "Epoch [21], Batch [694], Train Loss: 0.9736081957817078\n",
            "Epoch [21], Batch [695], Train Loss: 0.9049515128135681\n",
            "Epoch [21], Batch [696], Train Loss: 0.8308777213096619\n",
            "Epoch [21], Batch [697], Train Loss: 0.7307806611061096\n",
            "Epoch [21], Batch [698], Train Loss: 0.7620121836662292\n",
            "Epoch [21], Batch [699], Train Loss: 1.045893669128418\n",
            "Epoch [21], Batch [700], Train Loss: 0.7375963926315308\n",
            "Epoch [21], Batch [701], Train Loss: 0.7085704207420349\n",
            "Epoch [21], Batch [702], Train Loss: 0.8306376934051514\n",
            "Epoch [21], Batch [703], Train Loss: 0.7492808699607849\n",
            "Epoch [21], Batch [704], Train Loss: 0.9416495561599731\n",
            "Epoch [21], Batch [705], Train Loss: 1.1885441541671753\n",
            "Epoch [21], Batch [706], Train Loss: 0.7988063097000122\n",
            "Epoch [21], Batch [707], Train Loss: 0.8164063692092896\n",
            "Epoch [21], Batch [708], Train Loss: 0.7225532531738281\n",
            "Epoch [21], Batch [709], Train Loss: 0.8518587946891785\n",
            "Epoch [21], Batch [710], Train Loss: 0.6887009143829346\n",
            "Epoch [21], Batch [711], Train Loss: 0.7511841058731079\n",
            "Epoch [21], Batch [712], Train Loss: 0.8152828216552734\n",
            "Epoch [21], Batch [713], Train Loss: 1.043487787246704\n",
            "Epoch [21], Batch [714], Train Loss: 0.7241732478141785\n",
            "Epoch [21], Batch [715], Train Loss: 0.9292864799499512\n",
            "Epoch [21], Batch [716], Train Loss: 0.8601493239402771\n",
            "Epoch [21], Batch [717], Train Loss: 0.8339571952819824\n",
            "Epoch [21], Batch [718], Train Loss: 0.7663396000862122\n",
            "Epoch [21], Batch [719], Train Loss: 0.7344143986701965\n",
            "Epoch [21], Batch [720], Train Loss: 0.6821126937866211\n",
            "Epoch [21], Batch [721], Train Loss: 0.881898045539856\n",
            "Epoch [21], Batch [722], Train Loss: 0.9688398241996765\n",
            "Epoch [21], Batch [723], Train Loss: 0.7394829988479614\n",
            "Epoch [21], Batch [724], Train Loss: 0.8196086883544922\n",
            "Epoch [21], Batch [725], Train Loss: 0.8223263621330261\n",
            "Epoch [21], Batch [726], Train Loss: 0.8261224627494812\n",
            "Epoch [21], Batch [727], Train Loss: 0.8467368483543396\n",
            "Epoch [21], Batch [728], Train Loss: 0.8632605075836182\n",
            "Epoch [21], Batch [729], Train Loss: 0.8438836336135864\n",
            "Epoch [21], Batch [730], Train Loss: 0.8207982778549194\n",
            "Epoch [21], Batch [731], Train Loss: 1.1002497673034668\n",
            "Epoch [21], Batch [732], Train Loss: 0.7842095494270325\n",
            "Epoch [21], Batch [733], Train Loss: 0.946505606174469\n",
            "Epoch [21], Batch [734], Train Loss: 0.7161677479743958\n",
            "Epoch [21], Batch [735], Train Loss: 0.928555428981781\n",
            "Epoch [21], Batch [736], Train Loss: 0.9003931283950806\n",
            "Epoch [21], Batch [737], Train Loss: 0.7272506356239319\n",
            "Epoch [21], Batch [738], Train Loss: 0.6491764783859253\n",
            "Epoch [21], Batch [739], Train Loss: 0.783491313457489\n",
            "Epoch [21], Batch [740], Train Loss: 0.7582007050514221\n",
            "Epoch [21], Batch [741], Train Loss: 0.8041204810142517\n",
            "Epoch [21], Batch [742], Train Loss: 1.084395408630371\n",
            "Epoch [21], Batch [743], Train Loss: 0.6407321095466614\n",
            "Epoch [21], Batch [744], Train Loss: 0.7315692901611328\n",
            "Epoch [21], Batch [745], Train Loss: 0.7249934673309326\n",
            "Epoch [21], Batch [746], Train Loss: 1.0655064582824707\n",
            "Epoch [21], Batch [747], Train Loss: 0.6927602291107178\n",
            "Epoch [21], Batch [748], Train Loss: 0.746516227722168\n",
            "Epoch [21], Batch [749], Train Loss: 0.7838522791862488\n",
            "Epoch [21], Batch [750], Train Loss: 0.8451549410820007\n",
            "Epoch [21], Batch [751], Train Loss: 1.056605339050293\n",
            "Epoch [21], Batch [752], Train Loss: 0.8958008289337158\n",
            "Epoch [21], Batch [753], Train Loss: 0.729736328125\n",
            "Epoch [21], Batch [754], Train Loss: 0.67778080701828\n",
            "Epoch [21], Batch [755], Train Loss: 0.5637605786323547\n",
            "Epoch [21], Batch [756], Train Loss: 0.6363999843597412\n",
            "Epoch [21], Batch [757], Train Loss: 1.0045757293701172\n",
            "Epoch [21], Batch [758], Train Loss: 0.9145455956459045\n",
            "Epoch [21], Batch [759], Train Loss: 0.8256924748420715\n",
            "Epoch [21], Batch [760], Train Loss: 0.9764413833618164\n",
            "Epoch [21], Batch [761], Train Loss: 0.7533116936683655\n",
            "Epoch [21], Batch [762], Train Loss: 0.8105140328407288\n",
            "Epoch [21], Batch [763], Train Loss: 1.0266377925872803\n",
            "Epoch [21], Batch [764], Train Loss: 0.8339642882347107\n",
            "Epoch [21], Batch [765], Train Loss: 0.5900558829307556\n",
            "Epoch [21], Batch [766], Train Loss: 1.1357345581054688\n",
            "Epoch [21], Batch [767], Train Loss: 0.8526263236999512\n",
            "Epoch [21], Batch [768], Train Loss: 0.840984046459198\n",
            "Epoch [21], Batch [769], Train Loss: 0.9040898084640503\n",
            "Epoch [21], Batch [770], Train Loss: 0.7226122617721558\n",
            "Epoch [21], Batch [771], Train Loss: 0.748316764831543\n",
            "Epoch [21], Batch [772], Train Loss: 0.8777837753295898\n",
            "Epoch [21], Batch [773], Train Loss: 0.9766355156898499\n",
            "Epoch [21], Batch [774], Train Loss: 0.7693972587585449\n",
            "Epoch [21], Batch [775], Train Loss: 0.8432198762893677\n",
            "Epoch [21], Batch [776], Train Loss: 0.7747529149055481\n",
            "Epoch [21], Batch [777], Train Loss: 0.8945761919021606\n",
            "Epoch [21], Batch [778], Train Loss: 0.8470541834831238\n",
            "Epoch [21], Batch [779], Train Loss: 0.6702231764793396\n",
            "Epoch [21], Batch [780], Train Loss: 1.1233726739883423\n",
            "Epoch [21], Batch [781], Train Loss: 0.7066171169281006\n",
            "Epoch [21], Batch [782], Train Loss: 0.8424603343009949\n",
            "Epoch [21], Batch [783], Train Loss: 0.6313456296920776\n",
            "Epoch [21], Batch [784], Train Loss: 0.81591796875\n",
            "Epoch [21], Batch [785], Train Loss: 0.7575398683547974\n",
            "Epoch [21], Batch [786], Train Loss: 0.76548832654953\n",
            "Epoch [21], Batch [787], Train Loss: 0.8611652255058289\n",
            "Epoch [21], Batch [788], Train Loss: 0.8335093855857849\n",
            "Epoch [21], Batch [789], Train Loss: 0.6929783225059509\n",
            "Epoch [21], Batch [790], Train Loss: 0.855086088180542\n",
            "Epoch [21], Batch [791], Train Loss: 0.9784795641899109\n",
            "Epoch [21], Batch [792], Train Loss: 0.5857589840888977\n",
            "Epoch [21], Batch [793], Train Loss: 0.7301502823829651\n",
            "Epoch [21], Batch [794], Train Loss: 0.7694661617279053\n",
            "Epoch [21], Batch [795], Train Loss: 0.7222508192062378\n",
            "Epoch [21], Batch [796], Train Loss: 0.9787538051605225\n",
            "Epoch [21], Batch [797], Train Loss: 0.7334471940994263\n",
            "Epoch [21], Batch [798], Train Loss: 0.8455179333686829\n",
            "Epoch [21], Batch [799], Train Loss: 0.7863890528678894\n",
            "Epoch [21], Batch [800], Train Loss: 0.6632825136184692\n",
            "Epoch [21], Batch [801], Train Loss: 0.8472819328308105\n",
            "Epoch [21], Batch [802], Train Loss: 0.8037054538726807\n",
            "Epoch [21], Batch [803], Train Loss: 0.8703742623329163\n",
            "Epoch [21], Batch [804], Train Loss: 0.7618999481201172\n",
            "Epoch [21], Batch [805], Train Loss: 0.8623862862586975\n",
            "Epoch [21], Batch [806], Train Loss: 0.9388997554779053\n",
            "Epoch [21], Batch [807], Train Loss: 0.8230876922607422\n",
            "Epoch [21], Batch [808], Train Loss: 1.0438698530197144\n",
            "Epoch [21], Batch [809], Train Loss: 0.717633843421936\n",
            "Epoch [21], Batch [810], Train Loss: 0.9222944974899292\n",
            "Epoch [21], Batch [811], Train Loss: 0.7625694870948792\n",
            "Epoch [21], Batch [812], Train Loss: 0.9240385890007019\n",
            "Epoch [21], Batch [813], Train Loss: 0.7635273933410645\n",
            "Epoch [21], Batch [814], Train Loss: 0.8218269348144531\n",
            "Epoch [21], Batch [815], Train Loss: 0.8873231410980225\n",
            "Epoch [21], Batch [816], Train Loss: 0.7359855771064758\n",
            "Epoch [21], Batch [817], Train Loss: 0.7452060580253601\n",
            "Epoch [21], Batch [818], Train Loss: 0.8055369853973389\n",
            "Epoch [21], Batch [819], Train Loss: 0.8165266513824463\n",
            "Epoch [21], Batch [820], Train Loss: 0.7339787483215332\n",
            "Epoch [21], Batch [821], Train Loss: 0.721276044845581\n",
            "Epoch [21], Batch [822], Train Loss: 0.8631673455238342\n",
            "Epoch [21], Batch [823], Train Loss: 0.975772500038147\n",
            "Epoch [21], Batch [824], Train Loss: 0.7717055678367615\n",
            "Epoch [21], Batch [825], Train Loss: 0.809588611125946\n",
            "Epoch [21], Batch [826], Train Loss: 0.8814728260040283\n",
            "Epoch [21], Batch [827], Train Loss: 0.6972209215164185\n",
            "Epoch [21], Batch [828], Train Loss: 0.8745169639587402\n",
            "Epoch [21], Batch [829], Train Loss: 0.7634891867637634\n",
            "Epoch [21], Batch [830], Train Loss: 0.6316291689872742\n",
            "Epoch [21], Batch [831], Train Loss: 0.7966288328170776\n",
            "Epoch [21], Batch [832], Train Loss: 0.758318305015564\n",
            "Epoch [21], Batch [833], Train Loss: 0.7970872521400452\n",
            "Epoch [21], Batch [834], Train Loss: 0.6598556637763977\n",
            "Epoch [21], Batch [835], Train Loss: 0.7728709578514099\n",
            "Epoch [21], Batch [836], Train Loss: 0.6835691928863525\n",
            "Epoch [21], Batch [837], Train Loss: 0.7878033518791199\n",
            "Epoch [21], Batch [838], Train Loss: 0.6773033738136292\n",
            "Epoch [21], Batch [839], Train Loss: 0.7889671921730042\n",
            "Epoch [21], Batch [840], Train Loss: 0.9100505709648132\n",
            "Epoch [21], Batch [841], Train Loss: 0.8009600043296814\n",
            "Epoch [21], Batch [842], Train Loss: 0.8091115951538086\n",
            "Epoch [21], Batch [843], Train Loss: 0.7646968364715576\n",
            "Epoch [21], Batch [844], Train Loss: 0.6831438541412354\n",
            "Epoch [21], Batch [845], Train Loss: 0.5878003835678101\n",
            "Epoch [21], Batch [846], Train Loss: 0.8863065838813782\n",
            "Epoch [21], Batch [847], Train Loss: 0.6724233031272888\n",
            "Epoch [21], Batch [848], Train Loss: 0.8382200002670288\n",
            "Epoch [21], Batch [849], Train Loss: 0.7672196626663208\n",
            "Epoch [21], Batch [850], Train Loss: 0.8303945064544678\n",
            "Epoch [21], Batch [851], Train Loss: 0.7979133725166321\n",
            "Epoch [21], Batch [852], Train Loss: 0.6667842268943787\n",
            "Epoch [21], Batch [853], Train Loss: 0.6778947114944458\n",
            "Epoch [21], Batch [854], Train Loss: 0.7937912344932556\n",
            "Epoch [21], Batch [855], Train Loss: 0.7184085845947266\n",
            "Epoch [21], Batch [856], Train Loss: 0.859153151512146\n",
            "Epoch [21], Batch [857], Train Loss: 0.9846148490905762\n",
            "Epoch [21], Batch [858], Train Loss: 0.8547073602676392\n",
            "Epoch [21], Batch [859], Train Loss: 0.8022959232330322\n",
            "Epoch [21], Batch [860], Train Loss: 0.7621532082557678\n",
            "Epoch [21], Batch [861], Train Loss: 0.8126477003097534\n",
            "Epoch [21], Batch [862], Train Loss: 0.7513192296028137\n",
            "Epoch [21], Batch [863], Train Loss: 0.7779965400695801\n",
            "Epoch [21], Batch [864], Train Loss: 0.6703622341156006\n",
            "Epoch [21], Batch [865], Train Loss: 0.7421793341636658\n",
            "Epoch [21], Batch [866], Train Loss: 0.9182983636856079\n",
            "Epoch [21], Batch [867], Train Loss: 0.7805469036102295\n",
            "Epoch [21], Batch [868], Train Loss: 0.9547027945518494\n",
            "Epoch [21], Batch [869], Train Loss: 0.9339451193809509\n",
            "Epoch [21], Batch [870], Train Loss: 0.7564927339553833\n",
            "Epoch [21], Batch [871], Train Loss: 0.857393741607666\n",
            "Epoch [21], Batch [872], Train Loss: 0.7558590173721313\n",
            "Epoch [21], Batch [873], Train Loss: 0.9831702709197998\n",
            "Epoch [21], Batch [874], Train Loss: 0.9931733012199402\n",
            "Epoch [21], Batch [875], Train Loss: 0.7518972158432007\n",
            "Epoch [21], Batch [876], Train Loss: 0.8456539511680603\n",
            "Epoch [21], Batch [877], Train Loss: 0.6854583024978638\n",
            "Epoch [21], Batch [878], Train Loss: 0.6452930569648743\n",
            "Epoch [21], Batch [879], Train Loss: 0.991902232170105\n",
            "Epoch [21], Batch [880], Train Loss: 0.9523265361785889\n",
            "Epoch [21], Batch [881], Train Loss: 0.7055585980415344\n",
            "Epoch [21], Batch [882], Train Loss: 0.8918314576148987\n",
            "Epoch [21], Batch [883], Train Loss: 1.007745623588562\n",
            "Epoch [21], Batch [884], Train Loss: 0.7634392380714417\n",
            "Epoch [21], Batch [885], Train Loss: 1.0436142683029175\n",
            "Epoch [21], Batch [886], Train Loss: 0.6801780462265015\n",
            "Epoch [21], Batch [887], Train Loss: 0.731482982635498\n",
            "Epoch [21], Batch [888], Train Loss: 0.8361854553222656\n",
            "Epoch [21], Batch [889], Train Loss: 1.1034636497497559\n",
            "Epoch [21], Batch [890], Train Loss: 0.8023141026496887\n",
            "Epoch [21], Batch [891], Train Loss: 0.7133982181549072\n",
            "Epoch [21], Batch [892], Train Loss: 0.7519832253456116\n",
            "Epoch [21], Batch [893], Train Loss: 1.160571575164795\n",
            "Epoch [21], Batch [894], Train Loss: 0.7526859641075134\n",
            "Epoch [21], Batch [895], Train Loss: 0.761090874671936\n",
            "Epoch [21], Batch [896], Train Loss: 0.721498966217041\n",
            "Epoch [21], Batch [897], Train Loss: 0.6398085355758667\n",
            "Epoch [21], Batch [898], Train Loss: 0.9446390271186829\n",
            "Epoch [21], Batch [899], Train Loss: 0.9180282950401306\n",
            "Epoch [21], Batch [900], Train Loss: 0.6780694127082825\n",
            "Epoch [21], Batch [901], Train Loss: 0.9005682468414307\n",
            "Epoch [21], Batch [902], Train Loss: 0.9701271653175354\n",
            "Epoch [21], Batch [903], Train Loss: 0.7581528425216675\n",
            "Epoch [21], Batch [904], Train Loss: 0.944586455821991\n",
            "Epoch [21], Batch [905], Train Loss: 0.7095112800598145\n",
            "Epoch [21], Batch [906], Train Loss: 0.7271114587783813\n",
            "Epoch [21], Batch [907], Train Loss: 0.7353172302246094\n",
            "Epoch [21], Batch [908], Train Loss: 0.8976483345031738\n",
            "Epoch [21], Batch [909], Train Loss: 0.7452773451805115\n",
            "Epoch [21], Batch [910], Train Loss: 0.6195592284202576\n",
            "Epoch [21], Batch [911], Train Loss: 0.7731553316116333\n",
            "Epoch [21], Batch [912], Train Loss: 0.7037288546562195\n",
            "Epoch [21], Batch [913], Train Loss: 1.0702731609344482\n",
            "Epoch [21], Batch [914], Train Loss: 1.0054051876068115\n",
            "Epoch [21], Batch [915], Train Loss: 0.8651199340820312\n",
            "Epoch [21], Batch [916], Train Loss: 0.7772120237350464\n",
            "Epoch [21], Batch [917], Train Loss: 0.8657003045082092\n",
            "Epoch [21], Batch [918], Train Loss: 0.8951587677001953\n",
            "Epoch [21], Batch [919], Train Loss: 0.752766489982605\n",
            "Epoch [21], Batch [920], Train Loss: 0.8197305202484131\n",
            "Epoch [21], Batch [921], Train Loss: 0.8292739987373352\n",
            "Epoch [21], Batch [922], Train Loss: 0.7044479846954346\n",
            "Epoch [21], Batch [923], Train Loss: 0.6880279779434204\n",
            "Epoch [21], Batch [924], Train Loss: 0.5866219997406006\n",
            "Epoch [21], Batch [925], Train Loss: 0.7815969586372375\n",
            "Epoch [21], Batch [926], Train Loss: 1.0245375633239746\n",
            "Epoch [21], Batch [927], Train Loss: 0.8572869300842285\n",
            "Epoch [21], Batch [928], Train Loss: 0.7743184566497803\n",
            "Epoch [21], Batch [929], Train Loss: 0.8340281844139099\n",
            "Epoch [21], Batch [930], Train Loss: 0.6466469764709473\n",
            "Epoch [21], Batch [931], Train Loss: 0.9294201135635376\n",
            "Epoch [21], Batch [932], Train Loss: 0.7228021025657654\n",
            "Epoch [21], Batch [933], Train Loss: 0.8163782358169556\n",
            "Epoch [21], Batch [934], Train Loss: 0.7097045183181763\n",
            "Epoch [21], Batch [935], Train Loss: 0.6870359182357788\n",
            "Epoch [21], Batch [936], Train Loss: 0.7843523025512695\n",
            "Epoch [21], Batch [937], Train Loss: 0.8847765922546387\n",
            "Epoch [21], Batch [938], Train Loss: 0.7037519812583923\n",
            "Accuracy of train set: 0.6871833333333334\n",
            "Epoch [22], Batch [1], Loss: 0.7998706102371216\n",
            "Epoch [22], Batch [2], Loss: 0.6314123868942261\n",
            "Epoch [22], Batch [3], Loss: 0.718641996383667\n",
            "Epoch [22], Batch [4], Loss: 0.7211617231369019\n",
            "Epoch [22], Batch [5], Loss: 0.7631975412368774\n",
            "Epoch [22], Batch [6], Loss: 0.7574968338012695\n",
            "Epoch [22], Batch [7], Loss: 0.8028818964958191\n",
            "Epoch [22], Batch [8], Loss: 0.8449326753616333\n",
            "Epoch [22], Batch [9], Loss: 0.8129144310951233\n",
            "Epoch [22], Batch [10], Loss: 0.9457592964172363\n",
            "Epoch [22], Batch [11], Loss: 0.966070294380188\n",
            "Epoch [22], Batch [12], Loss: 0.8209975361824036\n",
            "Epoch [22], Batch [13], Loss: 0.6802148222923279\n",
            "Epoch [22], Batch [14], Loss: 0.6851673722267151\n",
            "Epoch [22], Batch [15], Loss: 0.7444154024124146\n",
            "Epoch [22], Batch [16], Loss: 0.9465067386627197\n",
            "Epoch [22], Batch [17], Loss: 0.7535037398338318\n",
            "Epoch [22], Batch [18], Loss: 0.9969429969787598\n",
            "Epoch [22], Batch [19], Loss: 0.8670105934143066\n",
            "Epoch [22], Batch [20], Loss: 0.922386646270752\n",
            "Epoch [22], Batch [21], Loss: 0.9633051156997681\n",
            "Epoch [22], Batch [22], Loss: 0.809540867805481\n",
            "Epoch [22], Batch [23], Loss: 0.7077370882034302\n",
            "Epoch [22], Batch [24], Loss: 0.7319890260696411\n",
            "Epoch [22], Batch [25], Loss: 0.7683687210083008\n",
            "Epoch [22], Batch [26], Loss: 0.8462381958961487\n",
            "Epoch [22], Batch [27], Loss: 0.8171694278717041\n",
            "Epoch [22], Batch [28], Loss: 0.8523669838905334\n",
            "Epoch [22], Batch [29], Loss: 0.9016368985176086\n",
            "Epoch [22], Batch [30], Loss: 0.4925202429294586\n",
            "Epoch [22], Batch [31], Loss: 0.9127397537231445\n",
            "Epoch [22], Batch [32], Loss: 0.682314932346344\n",
            "Epoch [22], Batch [33], Loss: 0.8870658278465271\n",
            "Epoch [22], Batch [34], Loss: 0.7212124466896057\n",
            "Epoch [22], Batch [35], Loss: 0.7316275238990784\n",
            "Epoch [22], Batch [36], Loss: 0.7422472238540649\n",
            "Epoch [22], Batch [37], Loss: 0.6925169229507446\n",
            "Epoch [22], Batch [38], Loss: 0.7706058025360107\n",
            "Epoch [22], Batch [39], Loss: 0.8427884578704834\n",
            "Epoch [22], Batch [40], Loss: 0.7709497213363647\n",
            "Epoch [22], Batch [41], Loss: 0.8662654161453247\n",
            "Epoch [22], Batch [42], Loss: 0.9606538414955139\n",
            "Epoch [22], Batch [43], Loss: 1.0030491352081299\n",
            "Epoch [22], Batch [44], Loss: 0.7215245962142944\n",
            "Epoch [22], Batch [45], Loss: 0.9213860630989075\n",
            "Epoch [22], Batch [46], Loss: 1.095561146736145\n",
            "Epoch [22], Batch [47], Loss: 0.8141014575958252\n",
            "Epoch [22], Batch [48], Loss: 1.0493719577789307\n",
            "Epoch [22], Batch [49], Loss: 0.9067237973213196\n",
            "Epoch [22], Batch [50], Loss: 0.7337446808815002\n",
            "Epoch [22], Batch [51], Loss: 1.1424932479858398\n",
            "Epoch [22], Batch [52], Loss: 0.9067461490631104\n",
            "Epoch [22], Batch [53], Loss: 0.7498507499694824\n",
            "Epoch [22], Batch [54], Loss: 0.9438188076019287\n",
            "Epoch [22], Batch [55], Loss: 1.068556308746338\n",
            "Epoch [22], Batch [56], Loss: 0.7702580690383911\n",
            "Epoch [22], Batch [57], Loss: 0.9320126175880432\n",
            "Epoch [22], Batch [58], Loss: 0.996675968170166\n",
            "Epoch [22], Batch [59], Loss: 0.8249484300613403\n",
            "Epoch [22], Batch [60], Loss: 1.0098360776901245\n",
            "Epoch [22], Batch [61], Loss: 0.7954016923904419\n",
            "Epoch [22], Batch [62], Loss: 1.2149097919464111\n",
            "Epoch [22], Batch [63], Loss: 0.8256580829620361\n",
            "Epoch [22], Batch [64], Loss: 0.9866424798965454\n",
            "Epoch [22], Batch [65], Loss: 0.9586052298545837\n",
            "Epoch [22], Batch [66], Loss: 0.8082827925682068\n",
            "Epoch [22], Batch [67], Loss: 0.8650681376457214\n",
            "Epoch [22], Batch [68], Loss: 0.6662284135818481\n",
            "Epoch [22], Batch [69], Loss: 0.9313815832138062\n",
            "Epoch [22], Batch [70], Loss: 0.5933185815811157\n",
            "Epoch [22], Batch [71], Loss: 0.6149788498878479\n",
            "Epoch [22], Batch [72], Loss: 0.5711632966995239\n",
            "Epoch [22], Batch [73], Loss: 0.8136879205703735\n",
            "Epoch [22], Batch [74], Loss: 0.9005904197692871\n",
            "Epoch [22], Batch [75], Loss: 0.7552856206893921\n",
            "Epoch [22], Batch [76], Loss: 0.8732314705848694\n",
            "Epoch [22], Batch [77], Loss: 0.838112473487854\n",
            "Epoch [22], Batch [78], Loss: 0.7570092082023621\n",
            "Epoch [22], Batch [79], Loss: 1.0980591773986816\n",
            "Epoch [22], Batch [80], Loss: 0.8670307397842407\n",
            "Epoch [22], Batch [81], Loss: 1.01780104637146\n",
            "Epoch [22], Batch [82], Loss: 0.7716710567474365\n",
            "Epoch [22], Batch [83], Loss: 0.756445050239563\n",
            "Epoch [22], Batch [84], Loss: 0.7698099613189697\n",
            "Epoch [22], Batch [85], Loss: 0.7635975480079651\n",
            "Epoch [22], Batch [86], Loss: 0.880243182182312\n",
            "Epoch [22], Batch [87], Loss: 0.9788733124732971\n",
            "Epoch [22], Batch [88], Loss: 1.0230433940887451\n",
            "Epoch [22], Batch [89], Loss: 0.9374326467514038\n",
            "Epoch [22], Batch [90], Loss: 0.7697275280952454\n",
            "Epoch [22], Batch [91], Loss: 0.8260206580162048\n",
            "Epoch [22], Batch [92], Loss: 0.667820930480957\n",
            "Epoch [22], Batch [93], Loss: 0.5995239615440369\n",
            "Epoch [22], Batch [94], Loss: 0.9263504147529602\n",
            "Epoch [22], Batch [95], Loss: 0.7034008502960205\n",
            "Epoch [22], Batch [96], Loss: 0.9114116430282593\n",
            "Epoch [22], Batch [97], Loss: 0.7063745260238647\n",
            "Epoch [22], Batch [98], Loss: 0.8427447080612183\n",
            "Epoch [22], Batch [99], Loss: 1.0160411596298218\n",
            "Epoch [22], Batch [100], Loss: 0.6856824159622192\n",
            "Epoch [22], Batch [101], Loss: 0.8380615711212158\n",
            "Epoch [22], Batch [102], Loss: 0.8229268789291382\n",
            "Epoch [22], Batch [103], Loss: 0.7443247437477112\n",
            "Epoch [22], Batch [104], Loss: 0.8054115176200867\n",
            "Epoch [22], Batch [105], Loss: 1.1105598211288452\n",
            "Epoch [22], Batch [106], Loss: 0.6397716999053955\n",
            "Epoch [22], Batch [107], Loss: 0.8872582912445068\n",
            "Epoch [22], Batch [108], Loss: 0.8045230507850647\n",
            "Epoch [22], Batch [109], Loss: 0.907833456993103\n",
            "Epoch [22], Batch [110], Loss: 0.8233970999717712\n",
            "Epoch [22], Batch [111], Loss: 0.7405710220336914\n",
            "Epoch [22], Batch [112], Loss: 0.5988898873329163\n",
            "Epoch [22], Batch [113], Loss: 0.9102104306221008\n",
            "Epoch [22], Batch [114], Loss: 0.8253101110458374\n",
            "Epoch [22], Batch [115], Loss: 0.7749349474906921\n",
            "Epoch [22], Batch [116], Loss: 0.8207336068153381\n",
            "Epoch [22], Batch [117], Loss: 0.7873066067695618\n",
            "Epoch [22], Batch [118], Loss: 0.8076908588409424\n",
            "Epoch [22], Batch [119], Loss: 0.8325212001800537\n",
            "Epoch [22], Batch [120], Loss: 0.7606477737426758\n",
            "Epoch [22], Batch [121], Loss: 0.7921338081359863\n",
            "Epoch [22], Batch [122], Loss: 0.70054030418396\n",
            "Epoch [22], Batch [123], Loss: 0.8513979911804199\n",
            "Epoch [22], Batch [124], Loss: 1.1789157390594482\n",
            "Epoch [22], Batch [125], Loss: 0.8516042232513428\n",
            "Epoch [22], Batch [126], Loss: 0.8420672416687012\n",
            "Epoch [22], Batch [127], Loss: 0.6850372552871704\n",
            "Epoch [22], Batch [128], Loss: 0.8751623034477234\n",
            "Epoch [22], Batch [129], Loss: 0.9193097949028015\n",
            "Epoch [22], Batch [130], Loss: 0.8825606107711792\n",
            "Epoch [22], Batch [131], Loss: 0.6803798675537109\n",
            "Epoch [22], Batch [132], Loss: 0.6495329141616821\n",
            "Epoch [22], Batch [133], Loss: 0.7438337802886963\n",
            "Epoch [22], Batch [134], Loss: 0.7837175726890564\n",
            "Epoch [22], Batch [135], Loss: 0.7881152033805847\n",
            "Epoch [22], Batch [136], Loss: 0.7191395163536072\n",
            "Epoch [22], Batch [137], Loss: 0.9968047738075256\n",
            "Epoch [22], Batch [138], Loss: 0.7801713943481445\n",
            "Epoch [22], Batch [139], Loss: 0.8670603036880493\n",
            "Epoch [22], Batch [140], Loss: 0.8767484426498413\n",
            "Epoch [22], Batch [141], Loss: 0.6991530656814575\n",
            "Epoch [22], Batch [142], Loss: 0.904650092124939\n",
            "Epoch [22], Batch [143], Loss: 0.9013978838920593\n",
            "Epoch [22], Batch [144], Loss: 0.9039120078086853\n",
            "Epoch [22], Batch [145], Loss: 0.8428440690040588\n",
            "Epoch [22], Batch [146], Loss: 0.8404833078384399\n",
            "Epoch [22], Batch [147], Loss: 0.8229416012763977\n",
            "Epoch [22], Batch [148], Loss: 0.6665042042732239\n",
            "Epoch [22], Batch [149], Loss: 0.9927974343299866\n",
            "Epoch [22], Batch [150], Loss: 1.065690279006958\n",
            "Epoch [22], Batch [151], Loss: 1.1207830905914307\n",
            "Epoch [22], Batch [152], Loss: 0.6616796255111694\n",
            "Epoch [22], Batch [153], Loss: 0.7165851593017578\n",
            "Epoch [22], Batch [154], Loss: 0.7434772849082947\n",
            "Epoch [22], Batch [155], Loss: 1.1666892766952515\n",
            "Epoch [22], Batch [156], Loss: 0.7775208950042725\n",
            "Epoch [22], Batch [157], Loss: 0.7022184133529663\n",
            "Accuracy of None set: 0.6791\n",
            "Epoch [22], Batch [1], Train Loss: 0.7961999773979187\n",
            "Epoch [22], Batch [2], Train Loss: 0.6262701749801636\n",
            "Epoch [22], Batch [3], Train Loss: 0.8468567132949829\n",
            "Epoch [22], Batch [4], Train Loss: 0.694110095500946\n",
            "Epoch [22], Batch [5], Train Loss: 0.8095648288726807\n",
            "Epoch [22], Batch [6], Train Loss: 0.8111447095870972\n",
            "Epoch [22], Batch [7], Train Loss: 0.7786237597465515\n",
            "Epoch [22], Batch [8], Train Loss: 0.686672568321228\n",
            "Epoch [22], Batch [9], Train Loss: 0.8172961473464966\n",
            "Epoch [22], Batch [10], Train Loss: 0.7121202349662781\n",
            "Epoch [22], Batch [11], Train Loss: 0.7966163754463196\n",
            "Epoch [22], Batch [12], Train Loss: 0.7789769768714905\n",
            "Epoch [22], Batch [13], Train Loss: 0.8673328161239624\n",
            "Epoch [22], Batch [14], Train Loss: 0.7846070528030396\n",
            "Epoch [22], Batch [15], Train Loss: 0.6567202806472778\n",
            "Epoch [22], Batch [16], Train Loss: 0.7524480819702148\n",
            "Epoch [22], Batch [17], Train Loss: 0.9929825067520142\n",
            "Epoch [22], Batch [18], Train Loss: 0.6719735264778137\n",
            "Epoch [22], Batch [19], Train Loss: 0.9215101599693298\n",
            "Epoch [22], Batch [20], Train Loss: 0.9276654124259949\n",
            "Epoch [22], Batch [21], Train Loss: 0.7220358848571777\n",
            "Epoch [22], Batch [22], Train Loss: 0.7490115761756897\n",
            "Epoch [22], Batch [23], Train Loss: 0.6698458790779114\n",
            "Epoch [22], Batch [24], Train Loss: 0.8867391347885132\n",
            "Epoch [22], Batch [25], Train Loss: 0.7368107438087463\n",
            "Epoch [22], Batch [26], Train Loss: 0.6978276968002319\n",
            "Epoch [22], Batch [27], Train Loss: 0.7532683610916138\n",
            "Epoch [22], Batch [28], Train Loss: 1.0519285202026367\n",
            "Epoch [22], Batch [29], Train Loss: 0.7536455392837524\n",
            "Epoch [22], Batch [30], Train Loss: 0.8326135277748108\n",
            "Epoch [22], Batch [31], Train Loss: 0.7503697872161865\n",
            "Epoch [22], Batch [32], Train Loss: 0.7996922731399536\n",
            "Epoch [22], Batch [33], Train Loss: 0.737180769443512\n",
            "Epoch [22], Batch [34], Train Loss: 0.6285883188247681\n",
            "Epoch [22], Batch [35], Train Loss: 0.7452811598777771\n",
            "Epoch [22], Batch [36], Train Loss: 0.8542914986610413\n",
            "Epoch [22], Batch [37], Train Loss: 0.7553809285163879\n",
            "Epoch [22], Batch [38], Train Loss: 0.8137150406837463\n",
            "Epoch [22], Batch [39], Train Loss: 0.795994758605957\n",
            "Epoch [22], Batch [40], Train Loss: 0.7735625505447388\n",
            "Epoch [22], Batch [41], Train Loss: 0.7708477973937988\n",
            "Epoch [22], Batch [42], Train Loss: 0.8912376165390015\n",
            "Epoch [22], Batch [43], Train Loss: 0.795361340045929\n",
            "Epoch [22], Batch [44], Train Loss: 0.7421066761016846\n",
            "Epoch [22], Batch [45], Train Loss: 0.8862786293029785\n",
            "Epoch [22], Batch [46], Train Loss: 0.7100856900215149\n",
            "Epoch [22], Batch [47], Train Loss: 0.8392531871795654\n",
            "Epoch [22], Batch [48], Train Loss: 0.6571506261825562\n",
            "Epoch [22], Batch [49], Train Loss: 0.7535068988800049\n",
            "Epoch [22], Batch [50], Train Loss: 0.6658295392990112\n",
            "Epoch [22], Batch [51], Train Loss: 0.9281030893325806\n",
            "Epoch [22], Batch [52], Train Loss: 0.7447910308837891\n",
            "Epoch [22], Batch [53], Train Loss: 0.7573341727256775\n",
            "Epoch [22], Batch [54], Train Loss: 0.8461319804191589\n",
            "Epoch [22], Batch [55], Train Loss: 0.8425419926643372\n",
            "Epoch [22], Batch [56], Train Loss: 0.8266059756278992\n",
            "Epoch [22], Batch [57], Train Loss: 0.7082718014717102\n",
            "Epoch [22], Batch [58], Train Loss: 0.75617915391922\n",
            "Epoch [22], Batch [59], Train Loss: 0.8224471211433411\n",
            "Epoch [22], Batch [60], Train Loss: 0.8471195697784424\n",
            "Epoch [22], Batch [61], Train Loss: 0.6780034303665161\n",
            "Epoch [22], Batch [62], Train Loss: 0.6803590059280396\n",
            "Epoch [22], Batch [63], Train Loss: 0.4379589855670929\n",
            "Epoch [22], Batch [64], Train Loss: 0.8377644419670105\n",
            "Epoch [22], Batch [65], Train Loss: 0.8929240703582764\n",
            "Epoch [22], Batch [66], Train Loss: 0.766703188419342\n",
            "Epoch [22], Batch [67], Train Loss: 0.8004565834999084\n",
            "Epoch [22], Batch [68], Train Loss: 0.8210735321044922\n",
            "Epoch [22], Batch [69], Train Loss: 0.9040510654449463\n",
            "Epoch [22], Batch [70], Train Loss: 0.6634702086448669\n",
            "Epoch [22], Batch [71], Train Loss: 0.7825644612312317\n",
            "Epoch [22], Batch [72], Train Loss: 0.7761847972869873\n",
            "Epoch [22], Batch [73], Train Loss: 0.7458088397979736\n",
            "Epoch [22], Batch [74], Train Loss: 0.8138113617897034\n",
            "Epoch [22], Batch [75], Train Loss: 0.732017993927002\n",
            "Epoch [22], Batch [76], Train Loss: 0.729157567024231\n",
            "Epoch [22], Batch [77], Train Loss: 0.7523812055587769\n",
            "Epoch [22], Batch [78], Train Loss: 0.9568718671798706\n",
            "Epoch [22], Batch [79], Train Loss: 0.851077675819397\n",
            "Epoch [22], Batch [80], Train Loss: 0.8334176540374756\n",
            "Epoch [22], Batch [81], Train Loss: 0.7324060201644897\n",
            "Epoch [22], Batch [82], Train Loss: 0.7224747538566589\n",
            "Epoch [22], Batch [83], Train Loss: 1.0517586469650269\n",
            "Epoch [22], Batch [84], Train Loss: 0.7771105766296387\n",
            "Epoch [22], Batch [85], Train Loss: 0.7537488341331482\n",
            "Epoch [22], Batch [86], Train Loss: 0.6964379549026489\n",
            "Epoch [22], Batch [87], Train Loss: 0.684974193572998\n",
            "Epoch [22], Batch [88], Train Loss: 0.8345211148262024\n",
            "Epoch [22], Batch [89], Train Loss: 0.8320629000663757\n",
            "Epoch [22], Batch [90], Train Loss: 0.8658064603805542\n",
            "Epoch [22], Batch [91], Train Loss: 1.0034434795379639\n",
            "Epoch [22], Batch [92], Train Loss: 0.8754282593727112\n",
            "Epoch [22], Batch [93], Train Loss: 0.7835838198661804\n",
            "Epoch [22], Batch [94], Train Loss: 0.9209063053131104\n",
            "Epoch [22], Batch [95], Train Loss: 0.7297285795211792\n",
            "Epoch [22], Batch [96], Train Loss: 1.0786293745040894\n",
            "Epoch [22], Batch [97], Train Loss: 0.86910480260849\n",
            "Epoch [22], Batch [98], Train Loss: 0.9562661647796631\n",
            "Epoch [22], Batch [99], Train Loss: 0.7614795565605164\n",
            "Epoch [22], Batch [100], Train Loss: 0.7745511531829834\n",
            "Epoch [22], Batch [101], Train Loss: 0.7610896229743958\n",
            "Epoch [22], Batch [102], Train Loss: 0.7760958075523376\n",
            "Epoch [22], Batch [103], Train Loss: 0.6906246542930603\n",
            "Epoch [22], Batch [104], Train Loss: 0.5991456508636475\n",
            "Epoch [22], Batch [105], Train Loss: 0.855009913444519\n",
            "Epoch [22], Batch [106], Train Loss: 0.6843621134757996\n",
            "Epoch [22], Batch [107], Train Loss: 0.8220949769020081\n",
            "Epoch [22], Batch [108], Train Loss: 0.6838915944099426\n",
            "Epoch [22], Batch [109], Train Loss: 0.8079323768615723\n",
            "Epoch [22], Batch [110], Train Loss: 0.7898492217063904\n",
            "Epoch [22], Batch [111], Train Loss: 0.8597370386123657\n",
            "Epoch [22], Batch [112], Train Loss: 0.7003205418586731\n",
            "Epoch [22], Batch [113], Train Loss: 1.004956841468811\n",
            "Epoch [22], Batch [114], Train Loss: 0.5958490371704102\n",
            "Epoch [22], Batch [115], Train Loss: 0.6555228233337402\n",
            "Epoch [22], Batch [116], Train Loss: 0.7601034641265869\n",
            "Epoch [22], Batch [117], Train Loss: 0.8126136064529419\n",
            "Epoch [22], Batch [118], Train Loss: 0.7700672149658203\n",
            "Epoch [22], Batch [119], Train Loss: 0.724291205406189\n",
            "Epoch [22], Batch [120], Train Loss: 0.7894320487976074\n",
            "Epoch [22], Batch [121], Train Loss: 0.8966105580329895\n",
            "Epoch [22], Batch [122], Train Loss: 0.8141431212425232\n",
            "Epoch [22], Batch [123], Train Loss: 1.4578721523284912\n",
            "Epoch [22], Batch [124], Train Loss: 0.951309323310852\n",
            "Epoch [22], Batch [125], Train Loss: 1.0394920110702515\n",
            "Epoch [22], Batch [126], Train Loss: 0.7671793699264526\n",
            "Epoch [22], Batch [127], Train Loss: 0.7507649660110474\n",
            "Epoch [22], Batch [128], Train Loss: 0.644381046295166\n",
            "Epoch [22], Batch [129], Train Loss: 0.828770637512207\n",
            "Epoch [22], Batch [130], Train Loss: 0.642835259437561\n",
            "Epoch [22], Batch [131], Train Loss: 0.7503853440284729\n",
            "Epoch [22], Batch [132], Train Loss: 0.89783775806427\n",
            "Epoch [22], Batch [133], Train Loss: 0.9076704978942871\n",
            "Epoch [22], Batch [134], Train Loss: 1.017619013786316\n",
            "Epoch [22], Batch [135], Train Loss: 0.9002282619476318\n",
            "Epoch [22], Batch [136], Train Loss: 0.6924867630004883\n",
            "Epoch [22], Batch [137], Train Loss: 0.8662026524543762\n",
            "Epoch [22], Batch [138], Train Loss: 0.6626465916633606\n",
            "Epoch [22], Batch [139], Train Loss: 0.8551276326179504\n",
            "Epoch [22], Batch [140], Train Loss: 0.7531506419181824\n",
            "Epoch [22], Batch [141], Train Loss: 0.9613398313522339\n",
            "Epoch [22], Batch [142], Train Loss: 0.5990110635757446\n",
            "Epoch [22], Batch [143], Train Loss: 0.9310322999954224\n",
            "Epoch [22], Batch [144], Train Loss: 0.9382961392402649\n",
            "Epoch [22], Batch [145], Train Loss: 0.7607710361480713\n",
            "Epoch [22], Batch [146], Train Loss: 0.9466248154640198\n",
            "Epoch [22], Batch [147], Train Loss: 0.5969799757003784\n",
            "Epoch [22], Batch [148], Train Loss: 0.9221107959747314\n",
            "Epoch [22], Batch [149], Train Loss: 0.9840327501296997\n",
            "Epoch [22], Batch [150], Train Loss: 0.897016704082489\n",
            "Epoch [22], Batch [151], Train Loss: 0.9397464394569397\n",
            "Epoch [22], Batch [152], Train Loss: 0.9456765055656433\n",
            "Epoch [22], Batch [153], Train Loss: 0.7345014810562134\n",
            "Epoch [22], Batch [154], Train Loss: 0.6726505160331726\n",
            "Epoch [22], Batch [155], Train Loss: 0.6726118326187134\n",
            "Epoch [22], Batch [156], Train Loss: 0.7159821391105652\n",
            "Epoch [22], Batch [157], Train Loss: 0.7732985615730286\n",
            "Epoch [22], Batch [158], Train Loss: 0.8162497878074646\n",
            "Epoch [22], Batch [159], Train Loss: 0.7891891598701477\n",
            "Epoch [22], Batch [160], Train Loss: 0.756714403629303\n",
            "Epoch [22], Batch [161], Train Loss: 0.8452821373939514\n",
            "Epoch [22], Batch [162], Train Loss: 0.8351180553436279\n",
            "Epoch [22], Batch [163], Train Loss: 0.8581265211105347\n",
            "Epoch [22], Batch [164], Train Loss: 0.7031113505363464\n",
            "Epoch [22], Batch [165], Train Loss: 0.8211496472358704\n",
            "Epoch [22], Batch [166], Train Loss: 0.6751641631126404\n",
            "Epoch [22], Batch [167], Train Loss: 0.6153878569602966\n",
            "Epoch [22], Batch [168], Train Loss: 1.0493017435073853\n",
            "Epoch [22], Batch [169], Train Loss: 0.6937305331230164\n",
            "Epoch [22], Batch [170], Train Loss: 0.8272649645805359\n",
            "Epoch [22], Batch [171], Train Loss: 0.8604133129119873\n",
            "Epoch [22], Batch [172], Train Loss: 0.8076503872871399\n",
            "Epoch [22], Batch [173], Train Loss: 0.8568858504295349\n",
            "Epoch [22], Batch [174], Train Loss: 0.7863556146621704\n",
            "Epoch [22], Batch [175], Train Loss: 0.6430044174194336\n",
            "Epoch [22], Batch [176], Train Loss: 0.7215980887413025\n",
            "Epoch [22], Batch [177], Train Loss: 0.8891387581825256\n",
            "Epoch [22], Batch [178], Train Loss: 0.8021313548088074\n",
            "Epoch [22], Batch [179], Train Loss: 0.7268295288085938\n",
            "Epoch [22], Batch [180], Train Loss: 0.747798502445221\n",
            "Epoch [22], Batch [181], Train Loss: 0.7678730487823486\n",
            "Epoch [22], Batch [182], Train Loss: 0.9822020530700684\n",
            "Epoch [22], Batch [183], Train Loss: 1.2126086950302124\n",
            "Epoch [22], Batch [184], Train Loss: 0.8354345560073853\n",
            "Epoch [22], Batch [185], Train Loss: 0.875654935836792\n",
            "Epoch [22], Batch [186], Train Loss: 0.8692394495010376\n",
            "Epoch [22], Batch [187], Train Loss: 0.9328645467758179\n",
            "Epoch [22], Batch [188], Train Loss: 0.7228309512138367\n",
            "Epoch [22], Batch [189], Train Loss: 0.8013156652450562\n",
            "Epoch [22], Batch [190], Train Loss: 0.7983965277671814\n",
            "Epoch [22], Batch [191], Train Loss: 0.68684321641922\n",
            "Epoch [22], Batch [192], Train Loss: 0.6154977679252625\n",
            "Epoch [22], Batch [193], Train Loss: 0.7812114953994751\n",
            "Epoch [22], Batch [194], Train Loss: 0.8457363843917847\n",
            "Epoch [22], Batch [195], Train Loss: 0.724409282207489\n",
            "Epoch [22], Batch [196], Train Loss: 0.701331377029419\n",
            "Epoch [22], Batch [197], Train Loss: 0.7896926403045654\n",
            "Epoch [22], Batch [198], Train Loss: 0.9671393036842346\n",
            "Epoch [22], Batch [199], Train Loss: 0.9331658482551575\n",
            "Epoch [22], Batch [200], Train Loss: 0.9333133101463318\n",
            "Epoch [22], Batch [201], Train Loss: 0.8993018865585327\n",
            "Epoch [22], Batch [202], Train Loss: 0.9506059885025024\n",
            "Epoch [22], Batch [203], Train Loss: 0.8459718227386475\n",
            "Epoch [22], Batch [204], Train Loss: 0.7775987982749939\n",
            "Epoch [22], Batch [205], Train Loss: 0.6974498629570007\n",
            "Epoch [22], Batch [206], Train Loss: 1.0139148235321045\n",
            "Epoch [22], Batch [207], Train Loss: 0.7191740870475769\n",
            "Epoch [22], Batch [208], Train Loss: 0.8723936080932617\n",
            "Epoch [22], Batch [209], Train Loss: 0.8608057498931885\n",
            "Epoch [22], Batch [210], Train Loss: 0.6414217352867126\n",
            "Epoch [22], Batch [211], Train Loss: 0.8579578399658203\n",
            "Epoch [22], Batch [212], Train Loss: 0.8924977779388428\n",
            "Epoch [22], Batch [213], Train Loss: 0.8939036130905151\n",
            "Epoch [22], Batch [214], Train Loss: 0.7660276293754578\n",
            "Epoch [22], Batch [215], Train Loss: 0.6445505023002625\n",
            "Epoch [22], Batch [216], Train Loss: 0.8965468406677246\n",
            "Epoch [22], Batch [217], Train Loss: 0.7731455564498901\n",
            "Epoch [22], Batch [218], Train Loss: 0.7291396856307983\n",
            "Epoch [22], Batch [219], Train Loss: 0.7230111360549927\n",
            "Epoch [22], Batch [220], Train Loss: 0.7352954745292664\n",
            "Epoch [22], Batch [221], Train Loss: 0.8609844446182251\n",
            "Epoch [22], Batch [222], Train Loss: 0.9030078649520874\n",
            "Epoch [22], Batch [223], Train Loss: 0.7535348534584045\n",
            "Epoch [22], Batch [224], Train Loss: 0.8011102676391602\n",
            "Epoch [22], Batch [225], Train Loss: 0.6302433609962463\n",
            "Epoch [22], Batch [226], Train Loss: 0.8936851024627686\n",
            "Epoch [22], Batch [227], Train Loss: 0.7161292433738708\n",
            "Epoch [22], Batch [228], Train Loss: 0.965490996837616\n",
            "Epoch [22], Batch [229], Train Loss: 0.7744297385215759\n",
            "Epoch [22], Batch [230], Train Loss: 0.7110379338264465\n",
            "Epoch [22], Batch [231], Train Loss: 0.7461004257202148\n",
            "Epoch [22], Batch [232], Train Loss: 1.0048576593399048\n",
            "Epoch [22], Batch [233], Train Loss: 0.703292727470398\n",
            "Epoch [22], Batch [234], Train Loss: 0.7086678743362427\n",
            "Epoch [22], Batch [235], Train Loss: 1.0040907859802246\n",
            "Epoch [22], Batch [236], Train Loss: 0.9264049530029297\n",
            "Epoch [22], Batch [237], Train Loss: 0.7425049543380737\n",
            "Epoch [22], Batch [238], Train Loss: 0.7311885356903076\n",
            "Epoch [22], Batch [239], Train Loss: 0.7311999797821045\n",
            "Epoch [22], Batch [240], Train Loss: 0.9084557294845581\n",
            "Epoch [22], Batch [241], Train Loss: 0.8228074908256531\n",
            "Epoch [22], Batch [242], Train Loss: 0.8795636892318726\n",
            "Epoch [22], Batch [243], Train Loss: 0.6923702359199524\n",
            "Epoch [22], Batch [244], Train Loss: 0.8662127256393433\n",
            "Epoch [22], Batch [245], Train Loss: 0.8879081010818481\n",
            "Epoch [22], Batch [246], Train Loss: 0.7422112822532654\n",
            "Epoch [22], Batch [247], Train Loss: 0.7680861949920654\n",
            "Epoch [22], Batch [248], Train Loss: 0.9946085214614868\n",
            "Epoch [22], Batch [249], Train Loss: 0.7650742530822754\n",
            "Epoch [22], Batch [250], Train Loss: 0.8202527761459351\n",
            "Epoch [22], Batch [251], Train Loss: 0.7261309623718262\n",
            "Epoch [22], Batch [252], Train Loss: 0.8708146810531616\n",
            "Epoch [22], Batch [253], Train Loss: 0.7051982879638672\n",
            "Epoch [22], Batch [254], Train Loss: 0.7314042448997498\n",
            "Epoch [22], Batch [255], Train Loss: 0.780547022819519\n",
            "Epoch [22], Batch [256], Train Loss: 0.9403738379478455\n",
            "Epoch [22], Batch [257], Train Loss: 0.683651328086853\n",
            "Epoch [22], Batch [258], Train Loss: 0.9090055227279663\n",
            "Epoch [22], Batch [259], Train Loss: 0.6021663546562195\n",
            "Epoch [22], Batch [260], Train Loss: 0.6758784055709839\n",
            "Epoch [22], Batch [261], Train Loss: 0.6402522921562195\n",
            "Epoch [22], Batch [262], Train Loss: 0.8917319178581238\n",
            "Epoch [22], Batch [263], Train Loss: 1.1582915782928467\n",
            "Epoch [22], Batch [264], Train Loss: 0.8426595330238342\n",
            "Epoch [22], Batch [265], Train Loss: 0.7368582487106323\n",
            "Epoch [22], Batch [266], Train Loss: 0.7723771929740906\n",
            "Epoch [22], Batch [267], Train Loss: 0.7323775887489319\n",
            "Epoch [22], Batch [268], Train Loss: 0.7075851559638977\n",
            "Epoch [22], Batch [269], Train Loss: 0.7874419093132019\n",
            "Epoch [22], Batch [270], Train Loss: 0.688941478729248\n",
            "Epoch [22], Batch [271], Train Loss: 0.7674821615219116\n",
            "Epoch [22], Batch [272], Train Loss: 0.8507484793663025\n",
            "Epoch [22], Batch [273], Train Loss: 0.6859561800956726\n",
            "Epoch [22], Batch [274], Train Loss: 0.945522665977478\n",
            "Epoch [22], Batch [275], Train Loss: 0.9188441038131714\n",
            "Epoch [22], Batch [276], Train Loss: 0.9282016754150391\n",
            "Epoch [22], Batch [277], Train Loss: 0.7631515860557556\n",
            "Epoch [22], Batch [278], Train Loss: 0.8562459945678711\n",
            "Epoch [22], Batch [279], Train Loss: 0.6241488456726074\n",
            "Epoch [22], Batch [280], Train Loss: 0.6449787616729736\n",
            "Epoch [22], Batch [281], Train Loss: 0.777451753616333\n",
            "Epoch [22], Batch [282], Train Loss: 0.7354593873023987\n",
            "Epoch [22], Batch [283], Train Loss: 0.9706709384918213\n",
            "Epoch [22], Batch [284], Train Loss: 1.0228995084762573\n",
            "Epoch [22], Batch [285], Train Loss: 0.7813259959220886\n",
            "Epoch [22], Batch [286], Train Loss: 0.8734638690948486\n",
            "Epoch [22], Batch [287], Train Loss: 0.5253747701644897\n",
            "Epoch [22], Batch [288], Train Loss: 0.7863616347312927\n",
            "Epoch [22], Batch [289], Train Loss: 0.9886761903762817\n",
            "Epoch [22], Batch [290], Train Loss: 0.6764586567878723\n",
            "Epoch [22], Batch [291], Train Loss: 0.6773332357406616\n",
            "Epoch [22], Batch [292], Train Loss: 0.9651535749435425\n",
            "Epoch [22], Batch [293], Train Loss: 0.7153753042221069\n",
            "Epoch [22], Batch [294], Train Loss: 0.9674252867698669\n",
            "Epoch [22], Batch [295], Train Loss: 0.9051969051361084\n",
            "Epoch [22], Batch [296], Train Loss: 0.8799524903297424\n",
            "Epoch [22], Batch [297], Train Loss: 0.9327804446220398\n",
            "Epoch [22], Batch [298], Train Loss: 0.7928256392478943\n",
            "Epoch [22], Batch [299], Train Loss: 0.7351968884468079\n",
            "Epoch [22], Batch [300], Train Loss: 0.7470819354057312\n",
            "Epoch [22], Batch [301], Train Loss: 0.7967077493667603\n",
            "Epoch [22], Batch [302], Train Loss: 0.8113090991973877\n",
            "Epoch [22], Batch [303], Train Loss: 0.9766499400138855\n",
            "Epoch [22], Batch [304], Train Loss: 0.791488528251648\n",
            "Epoch [22], Batch [305], Train Loss: 0.7906827330589294\n",
            "Epoch [22], Batch [306], Train Loss: 0.779813289642334\n",
            "Epoch [22], Batch [307], Train Loss: 0.986483097076416\n",
            "Epoch [22], Batch [308], Train Loss: 0.6686112284660339\n",
            "Epoch [22], Batch [309], Train Loss: 0.7249430418014526\n",
            "Epoch [22], Batch [310], Train Loss: 0.7917131185531616\n",
            "Epoch [22], Batch [311], Train Loss: 0.7514758706092834\n",
            "Epoch [22], Batch [312], Train Loss: 0.7084060311317444\n",
            "Epoch [22], Batch [313], Train Loss: 0.8205969333648682\n",
            "Epoch [22], Batch [314], Train Loss: 0.7007122039794922\n",
            "Epoch [22], Batch [315], Train Loss: 0.7743239998817444\n",
            "Epoch [22], Batch [316], Train Loss: 0.777586817741394\n",
            "Epoch [22], Batch [317], Train Loss: 0.7301872372627258\n",
            "Epoch [22], Batch [318], Train Loss: 0.7858349084854126\n",
            "Epoch [22], Batch [319], Train Loss: 0.7697532176971436\n",
            "Epoch [22], Batch [320], Train Loss: 0.7899395823478699\n",
            "Epoch [22], Batch [321], Train Loss: 0.6516846418380737\n",
            "Epoch [22], Batch [322], Train Loss: 0.8594861030578613\n",
            "Epoch [22], Batch [323], Train Loss: 0.7296624779701233\n",
            "Epoch [22], Batch [324], Train Loss: 0.890061616897583\n",
            "Epoch [22], Batch [325], Train Loss: 0.9136785864830017\n",
            "Epoch [22], Batch [326], Train Loss: 0.8411396741867065\n",
            "Epoch [22], Batch [327], Train Loss: 0.7919424176216125\n",
            "Epoch [22], Batch [328], Train Loss: 0.7786388397216797\n",
            "Epoch [22], Batch [329], Train Loss: 1.0019829273223877\n",
            "Epoch [22], Batch [330], Train Loss: 0.7109180688858032\n",
            "Epoch [22], Batch [331], Train Loss: 0.8951132893562317\n",
            "Epoch [22], Batch [332], Train Loss: 0.8099086880683899\n",
            "Epoch [22], Batch [333], Train Loss: 0.7524116635322571\n",
            "Epoch [22], Batch [334], Train Loss: 0.7383702397346497\n",
            "Epoch [22], Batch [335], Train Loss: 0.7152932286262512\n",
            "Epoch [22], Batch [336], Train Loss: 0.7877520322799683\n",
            "Epoch [22], Batch [337], Train Loss: 0.712635338306427\n",
            "Epoch [22], Batch [338], Train Loss: 0.7742908596992493\n",
            "Epoch [22], Batch [339], Train Loss: 0.8833113312721252\n",
            "Epoch [22], Batch [340], Train Loss: 0.8485098481178284\n",
            "Epoch [22], Batch [341], Train Loss: 0.7788501977920532\n",
            "Epoch [22], Batch [342], Train Loss: 0.7061562538146973\n",
            "Epoch [22], Batch [343], Train Loss: 0.6777918338775635\n",
            "Epoch [22], Batch [344], Train Loss: 0.9863459467887878\n",
            "Epoch [22], Batch [345], Train Loss: 0.8289097547531128\n",
            "Epoch [22], Batch [346], Train Loss: 0.8108747005462646\n",
            "Epoch [22], Batch [347], Train Loss: 1.0124250650405884\n",
            "Epoch [22], Batch [348], Train Loss: 0.6202305555343628\n",
            "Epoch [22], Batch [349], Train Loss: 1.0760070085525513\n",
            "Epoch [22], Batch [350], Train Loss: 0.7571914196014404\n",
            "Epoch [22], Batch [351], Train Loss: 0.9453418850898743\n",
            "Epoch [22], Batch [352], Train Loss: 0.7711400985717773\n",
            "Epoch [22], Batch [353], Train Loss: 0.754342794418335\n",
            "Epoch [22], Batch [354], Train Loss: 0.7702333927154541\n",
            "Epoch [22], Batch [355], Train Loss: 0.7941692471504211\n",
            "Epoch [22], Batch [356], Train Loss: 1.0386528968811035\n",
            "Epoch [22], Batch [357], Train Loss: 0.8148403763771057\n",
            "Epoch [22], Batch [358], Train Loss: 0.8711569905281067\n",
            "Epoch [22], Batch [359], Train Loss: 1.082742691040039\n",
            "Epoch [22], Batch [360], Train Loss: 0.6774148344993591\n",
            "Epoch [22], Batch [361], Train Loss: 0.828291654586792\n",
            "Epoch [22], Batch [362], Train Loss: 0.7334948778152466\n",
            "Epoch [22], Batch [363], Train Loss: 0.7691476345062256\n",
            "Epoch [22], Batch [364], Train Loss: 0.8253666162490845\n",
            "Epoch [22], Batch [365], Train Loss: 0.6446840763092041\n",
            "Epoch [22], Batch [366], Train Loss: 0.834274411201477\n",
            "Epoch [22], Batch [367], Train Loss: 0.8388344049453735\n",
            "Epoch [22], Batch [368], Train Loss: 0.831368088722229\n",
            "Epoch [22], Batch [369], Train Loss: 0.9405127167701721\n",
            "Epoch [22], Batch [370], Train Loss: 0.8369274139404297\n",
            "Epoch [22], Batch [371], Train Loss: 0.7542579174041748\n",
            "Epoch [22], Batch [372], Train Loss: 0.7051658034324646\n",
            "Epoch [22], Batch [373], Train Loss: 0.8951468467712402\n",
            "Epoch [22], Batch [374], Train Loss: 0.7485010027885437\n",
            "Epoch [22], Batch [375], Train Loss: 0.9880082607269287\n",
            "Epoch [22], Batch [376], Train Loss: 1.0149365663528442\n",
            "Epoch [22], Batch [377], Train Loss: 0.6614000201225281\n",
            "Epoch [22], Batch [378], Train Loss: 0.9443577527999878\n",
            "Epoch [22], Batch [379], Train Loss: 0.787087082862854\n",
            "Epoch [22], Batch [380], Train Loss: 0.9525332450866699\n",
            "Epoch [22], Batch [381], Train Loss: 0.747195839881897\n",
            "Epoch [22], Batch [382], Train Loss: 0.7492600679397583\n",
            "Epoch [22], Batch [383], Train Loss: 0.8368457555770874\n",
            "Epoch [22], Batch [384], Train Loss: 0.8746639490127563\n",
            "Epoch [22], Batch [385], Train Loss: 0.9288290143013\n",
            "Epoch [22], Batch [386], Train Loss: 0.9215760827064514\n",
            "Epoch [22], Batch [387], Train Loss: 0.8720989227294922\n",
            "Epoch [22], Batch [388], Train Loss: 1.0332564115524292\n",
            "Epoch [22], Batch [389], Train Loss: 0.8211520314216614\n",
            "Epoch [22], Batch [390], Train Loss: 0.7107453942298889\n",
            "Epoch [22], Batch [391], Train Loss: 0.889313817024231\n",
            "Epoch [22], Batch [392], Train Loss: 0.8015430569648743\n",
            "Epoch [22], Batch [393], Train Loss: 0.6676987409591675\n",
            "Epoch [22], Batch [394], Train Loss: 0.8924587965011597\n",
            "Epoch [22], Batch [395], Train Loss: 0.6560771465301514\n",
            "Epoch [22], Batch [396], Train Loss: 0.8298647999763489\n",
            "Epoch [22], Batch [397], Train Loss: 0.7347242832183838\n",
            "Epoch [22], Batch [398], Train Loss: 0.8112378716468811\n",
            "Epoch [22], Batch [399], Train Loss: 0.686090350151062\n",
            "Epoch [22], Batch [400], Train Loss: 0.8877478241920471\n",
            "Epoch [22], Batch [401], Train Loss: 1.0225837230682373\n",
            "Epoch [22], Batch [402], Train Loss: 0.9010578989982605\n",
            "Epoch [22], Batch [403], Train Loss: 0.6651049256324768\n",
            "Epoch [22], Batch [404], Train Loss: 0.8064969182014465\n",
            "Epoch [22], Batch [405], Train Loss: 0.8931586146354675\n",
            "Epoch [22], Batch [406], Train Loss: 0.8405892848968506\n",
            "Epoch [22], Batch [407], Train Loss: 0.9132798910140991\n",
            "Epoch [22], Batch [408], Train Loss: 0.9328101277351379\n",
            "Epoch [22], Batch [409], Train Loss: 0.7784863710403442\n",
            "Epoch [22], Batch [410], Train Loss: 0.7280438542366028\n",
            "Epoch [22], Batch [411], Train Loss: 0.7549970746040344\n",
            "Epoch [22], Batch [412], Train Loss: 0.6170241832733154\n",
            "Epoch [22], Batch [413], Train Loss: 0.8072924017906189\n",
            "Epoch [22], Batch [414], Train Loss: 0.7279320359230042\n",
            "Epoch [22], Batch [415], Train Loss: 0.814245343208313\n",
            "Epoch [22], Batch [416], Train Loss: 0.786777913570404\n",
            "Epoch [22], Batch [417], Train Loss: 0.836333692073822\n",
            "Epoch [22], Batch [418], Train Loss: 1.0328532457351685\n",
            "Epoch [22], Batch [419], Train Loss: 0.7730634212493896\n",
            "Epoch [22], Batch [420], Train Loss: 0.7764708399772644\n",
            "Epoch [22], Batch [421], Train Loss: 0.9059766530990601\n",
            "Epoch [22], Batch [422], Train Loss: 0.8107683658599854\n",
            "Epoch [22], Batch [423], Train Loss: 0.8031558990478516\n",
            "Epoch [22], Batch [424], Train Loss: 0.8782873153686523\n",
            "Epoch [22], Batch [425], Train Loss: 0.8814080357551575\n",
            "Epoch [22], Batch [426], Train Loss: 0.9576976299285889\n",
            "Epoch [22], Batch [427], Train Loss: 0.7233017683029175\n",
            "Epoch [22], Batch [428], Train Loss: 0.741619884967804\n",
            "Epoch [22], Batch [429], Train Loss: 0.7280007600784302\n",
            "Epoch [22], Batch [430], Train Loss: 0.6777085661888123\n",
            "Epoch [22], Batch [431], Train Loss: 0.6670376062393188\n",
            "Epoch [22], Batch [432], Train Loss: 0.7293426990509033\n",
            "Epoch [22], Batch [433], Train Loss: 1.021730661392212\n",
            "Epoch [22], Batch [434], Train Loss: 0.8034699559211731\n",
            "Epoch [22], Batch [435], Train Loss: 0.8886082768440247\n",
            "Epoch [22], Batch [436], Train Loss: 0.8795536756515503\n",
            "Epoch [22], Batch [437], Train Loss: 1.0835856199264526\n",
            "Epoch [22], Batch [438], Train Loss: 0.860630214214325\n",
            "Epoch [22], Batch [439], Train Loss: 0.7088433504104614\n",
            "Epoch [22], Batch [440], Train Loss: 0.7493897676467896\n",
            "Epoch [22], Batch [441], Train Loss: 0.7547475695610046\n",
            "Epoch [22], Batch [442], Train Loss: 0.7548338174819946\n",
            "Epoch [22], Batch [443], Train Loss: 0.6957798004150391\n",
            "Epoch [22], Batch [444], Train Loss: 0.8433513641357422\n",
            "Epoch [22], Batch [445], Train Loss: 0.8645885586738586\n",
            "Epoch [22], Batch [446], Train Loss: 0.8629827499389648\n",
            "Epoch [22], Batch [447], Train Loss: 0.7406017780303955\n",
            "Epoch [22], Batch [448], Train Loss: 0.7292091250419617\n",
            "Epoch [22], Batch [449], Train Loss: 0.9241801500320435\n",
            "Epoch [22], Batch [450], Train Loss: 0.9358845949172974\n",
            "Epoch [22], Batch [451], Train Loss: 1.0081690549850464\n",
            "Epoch [22], Batch [452], Train Loss: 0.6898728013038635\n",
            "Epoch [22], Batch [453], Train Loss: 1.0073027610778809\n",
            "Epoch [22], Batch [454], Train Loss: 0.737623393535614\n",
            "Epoch [22], Batch [455], Train Loss: 0.8076136112213135\n",
            "Epoch [22], Batch [456], Train Loss: 0.7020479440689087\n",
            "Epoch [22], Batch [457], Train Loss: 0.8053411841392517\n",
            "Epoch [22], Batch [458], Train Loss: 0.6554747819900513\n",
            "Epoch [22], Batch [459], Train Loss: 0.7892782688140869\n",
            "Epoch [22], Batch [460], Train Loss: 0.788024365901947\n",
            "Epoch [22], Batch [461], Train Loss: 0.7254700660705566\n",
            "Epoch [22], Batch [462], Train Loss: 0.8328649401664734\n",
            "Epoch [22], Batch [463], Train Loss: 0.9807843565940857\n",
            "Epoch [22], Batch [464], Train Loss: 0.6975060701370239\n",
            "Epoch [22], Batch [465], Train Loss: 0.7793149948120117\n",
            "Epoch [22], Batch [466], Train Loss: 0.879727840423584\n",
            "Epoch [22], Batch [467], Train Loss: 0.6955386400222778\n",
            "Epoch [22], Batch [468], Train Loss: 0.7137171030044556\n",
            "Epoch [22], Batch [469], Train Loss: 0.8661152124404907\n",
            "Epoch [22], Batch [470], Train Loss: 0.673599898815155\n",
            "Epoch [22], Batch [471], Train Loss: 0.8050263524055481\n",
            "Epoch [22], Batch [472], Train Loss: 0.7712511420249939\n",
            "Epoch [22], Batch [473], Train Loss: 0.9621481895446777\n",
            "Epoch [22], Batch [474], Train Loss: 0.8552055954933167\n",
            "Epoch [22], Batch [475], Train Loss: 0.8393867015838623\n",
            "Epoch [22], Batch [476], Train Loss: 0.8188556432723999\n",
            "Epoch [22], Batch [477], Train Loss: 0.8760824203491211\n",
            "Epoch [22], Batch [478], Train Loss: 0.6034040451049805\n",
            "Epoch [22], Batch [479], Train Loss: 0.9130638837814331\n",
            "Epoch [22], Batch [480], Train Loss: 0.8596436977386475\n",
            "Epoch [22], Batch [481], Train Loss: 0.9389426708221436\n",
            "Epoch [22], Batch [482], Train Loss: 0.8584291934967041\n",
            "Epoch [22], Batch [483], Train Loss: 0.9009643793106079\n",
            "Epoch [22], Batch [484], Train Loss: 0.8315956592559814\n",
            "Epoch [22], Batch [485], Train Loss: 0.7198117971420288\n",
            "Epoch [22], Batch [486], Train Loss: 1.0988332033157349\n",
            "Epoch [22], Batch [487], Train Loss: 0.8836913108825684\n",
            "Epoch [22], Batch [488], Train Loss: 0.7373235821723938\n",
            "Epoch [22], Batch [489], Train Loss: 0.7593668699264526\n",
            "Epoch [22], Batch [490], Train Loss: 0.8010686635971069\n",
            "Epoch [22], Batch [491], Train Loss: 0.9599640965461731\n",
            "Epoch [22], Batch [492], Train Loss: 0.8531190156936646\n",
            "Epoch [22], Batch [493], Train Loss: 1.0291376113891602\n",
            "Epoch [22], Batch [494], Train Loss: 0.7855803370475769\n",
            "Epoch [22], Batch [495], Train Loss: 0.8050018548965454\n",
            "Epoch [22], Batch [496], Train Loss: 0.7530347108840942\n",
            "Epoch [22], Batch [497], Train Loss: 0.8020610809326172\n",
            "Epoch [22], Batch [498], Train Loss: 0.6858034729957581\n",
            "Epoch [22], Batch [499], Train Loss: 0.8302564024925232\n",
            "Epoch [22], Batch [500], Train Loss: 0.7991514205932617\n",
            "Epoch [22], Batch [501], Train Loss: 0.9244885444641113\n",
            "Epoch [22], Batch [502], Train Loss: 0.5525158643722534\n",
            "Epoch [22], Batch [503], Train Loss: 1.1339848041534424\n",
            "Epoch [22], Batch [504], Train Loss: 0.9904500246047974\n",
            "Epoch [22], Batch [505], Train Loss: 0.8367433547973633\n",
            "Epoch [22], Batch [506], Train Loss: 0.7505277395248413\n",
            "Epoch [22], Batch [507], Train Loss: 0.8205664157867432\n",
            "Epoch [22], Batch [508], Train Loss: 0.8722090125083923\n",
            "Epoch [22], Batch [509], Train Loss: 0.7545909881591797\n",
            "Epoch [22], Batch [510], Train Loss: 0.8335801362991333\n",
            "Epoch [22], Batch [511], Train Loss: 0.8772687315940857\n",
            "Epoch [22], Batch [512], Train Loss: 0.8218128681182861\n",
            "Epoch [22], Batch [513], Train Loss: 0.7925896644592285\n",
            "Epoch [22], Batch [514], Train Loss: 0.7614308595657349\n",
            "Epoch [22], Batch [515], Train Loss: 0.6307342052459717\n",
            "Epoch [22], Batch [516], Train Loss: 0.7369889616966248\n",
            "Epoch [22], Batch [517], Train Loss: 0.8147402405738831\n",
            "Epoch [22], Batch [518], Train Loss: 0.885380744934082\n",
            "Epoch [22], Batch [519], Train Loss: 0.6955875158309937\n",
            "Epoch [22], Batch [520], Train Loss: 0.8573817014694214\n",
            "Epoch [22], Batch [521], Train Loss: 0.7987573146820068\n",
            "Epoch [22], Batch [522], Train Loss: 0.5684558153152466\n",
            "Epoch [22], Batch [523], Train Loss: 0.7536564469337463\n",
            "Epoch [22], Batch [524], Train Loss: 0.9256124496459961\n",
            "Epoch [22], Batch [525], Train Loss: 0.6195751428604126\n",
            "Epoch [22], Batch [526], Train Loss: 0.7795523405075073\n",
            "Epoch [22], Batch [527], Train Loss: 0.9830154180526733\n",
            "Epoch [22], Batch [528], Train Loss: 0.8585867881774902\n",
            "Epoch [22], Batch [529], Train Loss: 0.826675295829773\n",
            "Epoch [22], Batch [530], Train Loss: 0.5549024343490601\n",
            "Epoch [22], Batch [531], Train Loss: 0.9162483215332031\n",
            "Epoch [22], Batch [532], Train Loss: 0.7434519529342651\n",
            "Epoch [22], Batch [533], Train Loss: 0.776618480682373\n",
            "Epoch [22], Batch [534], Train Loss: 1.0019726753234863\n",
            "Epoch [22], Batch [535], Train Loss: 0.8848792910575867\n",
            "Epoch [22], Batch [536], Train Loss: 0.7498088479042053\n",
            "Epoch [22], Batch [537], Train Loss: 0.8444370031356812\n",
            "Epoch [22], Batch [538], Train Loss: 0.6842826008796692\n",
            "Epoch [22], Batch [539], Train Loss: 0.5866670608520508\n",
            "Epoch [22], Batch [540], Train Loss: 1.003542184829712\n",
            "Epoch [22], Batch [541], Train Loss: 0.6631230711936951\n",
            "Epoch [22], Batch [542], Train Loss: 0.6641736626625061\n",
            "Epoch [22], Batch [543], Train Loss: 0.838904082775116\n",
            "Epoch [22], Batch [544], Train Loss: 0.8507500886917114\n",
            "Epoch [22], Batch [545], Train Loss: 0.7263056039810181\n",
            "Epoch [22], Batch [546], Train Loss: 0.7832097411155701\n",
            "Epoch [22], Batch [547], Train Loss: 0.8666691184043884\n",
            "Epoch [22], Batch [548], Train Loss: 1.0619144439697266\n",
            "Epoch [22], Batch [549], Train Loss: 0.8609017133712769\n",
            "Epoch [22], Batch [550], Train Loss: 0.7297282814979553\n",
            "Epoch [22], Batch [551], Train Loss: 1.0506095886230469\n",
            "Epoch [22], Batch [552], Train Loss: 0.771460771560669\n",
            "Epoch [22], Batch [553], Train Loss: 0.8344568014144897\n",
            "Epoch [22], Batch [554], Train Loss: 0.7210830450057983\n",
            "Epoch [22], Batch [555], Train Loss: 0.9487168788909912\n",
            "Epoch [22], Batch [556], Train Loss: 0.8870983123779297\n",
            "Epoch [22], Batch [557], Train Loss: 0.8968562483787537\n",
            "Epoch [22], Batch [558], Train Loss: 0.7657550573348999\n",
            "Epoch [22], Batch [559], Train Loss: 0.8154552578926086\n",
            "Epoch [22], Batch [560], Train Loss: 0.903329610824585\n",
            "Epoch [22], Batch [561], Train Loss: 0.7665122151374817\n",
            "Epoch [22], Batch [562], Train Loss: 0.7107371687889099\n",
            "Epoch [22], Batch [563], Train Loss: 0.7268021106719971\n",
            "Epoch [22], Batch [564], Train Loss: 0.7423447370529175\n",
            "Epoch [22], Batch [565], Train Loss: 0.7287249565124512\n",
            "Epoch [22], Batch [566], Train Loss: 0.9138327240943909\n",
            "Epoch [22], Batch [567], Train Loss: 1.2285507917404175\n",
            "Epoch [22], Batch [568], Train Loss: 0.7433999180793762\n",
            "Epoch [22], Batch [569], Train Loss: 0.7744947075843811\n",
            "Epoch [22], Batch [570], Train Loss: 0.8408278226852417\n",
            "Epoch [22], Batch [571], Train Loss: 0.6398763656616211\n",
            "Epoch [22], Batch [572], Train Loss: 0.6159586906433105\n",
            "Epoch [22], Batch [573], Train Loss: 0.8251907825469971\n",
            "Epoch [22], Batch [574], Train Loss: 0.7035415768623352\n",
            "Epoch [22], Batch [575], Train Loss: 0.8053799271583557\n",
            "Epoch [22], Batch [576], Train Loss: 0.8389452695846558\n",
            "Epoch [22], Batch [577], Train Loss: 0.9022507071495056\n",
            "Epoch [22], Batch [578], Train Loss: 0.8891381025314331\n",
            "Epoch [22], Batch [579], Train Loss: 0.9061180353164673\n",
            "Epoch [22], Batch [580], Train Loss: 0.8597760796546936\n",
            "Epoch [22], Batch [581], Train Loss: 0.7051171064376831\n",
            "Epoch [22], Batch [582], Train Loss: 0.9077153205871582\n",
            "Epoch [22], Batch [583], Train Loss: 0.7639003992080688\n",
            "Epoch [22], Batch [584], Train Loss: 0.9017690420150757\n",
            "Epoch [22], Batch [585], Train Loss: 0.7553579211235046\n",
            "Epoch [22], Batch [586], Train Loss: 0.8232431411743164\n",
            "Epoch [22], Batch [587], Train Loss: 0.8999000787734985\n",
            "Epoch [22], Batch [588], Train Loss: 0.9039114117622375\n",
            "Epoch [22], Batch [589], Train Loss: 0.6500558257102966\n",
            "Epoch [22], Batch [590], Train Loss: 0.9238227605819702\n",
            "Epoch [22], Batch [591], Train Loss: 0.6257262825965881\n",
            "Epoch [22], Batch [592], Train Loss: 0.622704029083252\n",
            "Epoch [22], Batch [593], Train Loss: 0.8750582933425903\n",
            "Epoch [22], Batch [594], Train Loss: 0.7838996052742004\n",
            "Epoch [22], Batch [595], Train Loss: 0.6842712163925171\n",
            "Epoch [22], Batch [596], Train Loss: 0.8590491414070129\n",
            "Epoch [22], Batch [597], Train Loss: 0.5695340037345886\n",
            "Epoch [22], Batch [598], Train Loss: 0.6866021752357483\n",
            "Epoch [22], Batch [599], Train Loss: 1.0109647512435913\n",
            "Epoch [22], Batch [600], Train Loss: 0.8294212222099304\n",
            "Epoch [22], Batch [601], Train Loss: 0.8746492266654968\n",
            "Epoch [22], Batch [602], Train Loss: 0.8508581519126892\n",
            "Epoch [22], Batch [603], Train Loss: 0.7425447106361389\n",
            "Epoch [22], Batch [604], Train Loss: 0.804149866104126\n",
            "Epoch [22], Batch [605], Train Loss: 0.8068873286247253\n",
            "Epoch [22], Batch [606], Train Loss: 1.0688642263412476\n",
            "Epoch [22], Batch [607], Train Loss: 0.7811897993087769\n",
            "Epoch [22], Batch [608], Train Loss: 0.7215676307678223\n",
            "Epoch [22], Batch [609], Train Loss: 0.6502381563186646\n",
            "Epoch [22], Batch [610], Train Loss: 0.933749794960022\n",
            "Epoch [22], Batch [611], Train Loss: 0.8455536961555481\n",
            "Epoch [22], Batch [612], Train Loss: 0.6960932612419128\n",
            "Epoch [22], Batch [613], Train Loss: 0.7482191324234009\n",
            "Epoch [22], Batch [614], Train Loss: 0.8281514048576355\n",
            "Epoch [22], Batch [615], Train Loss: 0.5706251263618469\n",
            "Epoch [22], Batch [616], Train Loss: 0.869902491569519\n",
            "Epoch [22], Batch [617], Train Loss: 0.5357671976089478\n",
            "Epoch [22], Batch [618], Train Loss: 0.7588791251182556\n",
            "Epoch [22], Batch [619], Train Loss: 0.6872227787971497\n",
            "Epoch [22], Batch [620], Train Loss: 0.8508186340332031\n",
            "Epoch [22], Batch [621], Train Loss: 0.5621073246002197\n",
            "Epoch [22], Batch [622], Train Loss: 0.8729369640350342\n",
            "Epoch [22], Batch [623], Train Loss: 0.9413954615592957\n",
            "Epoch [22], Batch [624], Train Loss: 1.067969560623169\n",
            "Epoch [22], Batch [625], Train Loss: 0.8535998463630676\n",
            "Epoch [22], Batch [626], Train Loss: 0.8589764833450317\n",
            "Epoch [22], Batch [627], Train Loss: 0.7056670188903809\n",
            "Epoch [22], Batch [628], Train Loss: 0.6386855244636536\n",
            "Epoch [22], Batch [629], Train Loss: 0.6396847367286682\n",
            "Epoch [22], Batch [630], Train Loss: 0.6610183715820312\n",
            "Epoch [22], Batch [631], Train Loss: 0.6779909729957581\n",
            "Epoch [22], Batch [632], Train Loss: 1.1849756240844727\n",
            "Epoch [22], Batch [633], Train Loss: 0.6186280846595764\n",
            "Epoch [22], Batch [634], Train Loss: 0.7981093525886536\n",
            "Epoch [22], Batch [635], Train Loss: 0.8371233940124512\n",
            "Epoch [22], Batch [636], Train Loss: 0.6920256018638611\n",
            "Epoch [22], Batch [637], Train Loss: 0.7524889707565308\n",
            "Epoch [22], Batch [638], Train Loss: 0.7937875390052795\n",
            "Epoch [22], Batch [639], Train Loss: 0.6522316336631775\n",
            "Epoch [22], Batch [640], Train Loss: 0.734974205493927\n",
            "Epoch [22], Batch [641], Train Loss: 0.7385076880455017\n",
            "Epoch [22], Batch [642], Train Loss: 0.8372327089309692\n",
            "Epoch [22], Batch [643], Train Loss: 0.6771233677864075\n",
            "Epoch [22], Batch [644], Train Loss: 0.7332918047904968\n",
            "Epoch [22], Batch [645], Train Loss: 0.775797963142395\n",
            "Epoch [22], Batch [646], Train Loss: 0.708851158618927\n",
            "Epoch [22], Batch [647], Train Loss: 0.8012975454330444\n",
            "Epoch [22], Batch [648], Train Loss: 0.7643419504165649\n",
            "Epoch [22], Batch [649], Train Loss: 0.9739111661911011\n",
            "Epoch [22], Batch [650], Train Loss: 0.7438217401504517\n",
            "Epoch [22], Batch [651], Train Loss: 0.7262020111083984\n",
            "Epoch [22], Batch [652], Train Loss: 0.7293376326560974\n",
            "Epoch [22], Batch [653], Train Loss: 0.9381165504455566\n",
            "Epoch [22], Batch [654], Train Loss: 0.8211115598678589\n",
            "Epoch [22], Batch [655], Train Loss: 0.6892074346542358\n",
            "Epoch [22], Batch [656], Train Loss: 0.8918068408966064\n",
            "Epoch [22], Batch [657], Train Loss: 0.6566709876060486\n",
            "Epoch [22], Batch [658], Train Loss: 0.6703653335571289\n",
            "Epoch [22], Batch [659], Train Loss: 0.9372957944869995\n",
            "Epoch [22], Batch [660], Train Loss: 0.7333748936653137\n",
            "Epoch [22], Batch [661], Train Loss: 0.8585789799690247\n",
            "Epoch [22], Batch [662], Train Loss: 0.7745267152786255\n",
            "Epoch [22], Batch [663], Train Loss: 0.7904319763183594\n",
            "Epoch [22], Batch [664], Train Loss: 0.6726741194725037\n",
            "Epoch [22], Batch [665], Train Loss: 0.6758559942245483\n",
            "Epoch [22], Batch [666], Train Loss: 0.9999054074287415\n",
            "Epoch [22], Batch [667], Train Loss: 0.7758892178535461\n",
            "Epoch [22], Batch [668], Train Loss: 0.7583160400390625\n",
            "Epoch [22], Batch [669], Train Loss: 0.8517606854438782\n",
            "Epoch [22], Batch [670], Train Loss: 0.7553838491439819\n",
            "Epoch [22], Batch [671], Train Loss: 0.8366609215736389\n",
            "Epoch [22], Batch [672], Train Loss: 0.9335106611251831\n",
            "Epoch [22], Batch [673], Train Loss: 0.8373461961746216\n",
            "Epoch [22], Batch [674], Train Loss: 0.7065063714981079\n",
            "Epoch [22], Batch [675], Train Loss: 0.8158398866653442\n",
            "Epoch [22], Batch [676], Train Loss: 0.8797090649604797\n",
            "Epoch [22], Batch [677], Train Loss: 0.7388468980789185\n",
            "Epoch [22], Batch [678], Train Loss: 0.927760124206543\n",
            "Epoch [22], Batch [679], Train Loss: 0.833023726940155\n",
            "Epoch [22], Batch [680], Train Loss: 0.9802846312522888\n",
            "Epoch [22], Batch [681], Train Loss: 0.8443058729171753\n",
            "Epoch [22], Batch [682], Train Loss: 0.9142023921012878\n",
            "Epoch [22], Batch [683], Train Loss: 0.7492249011993408\n",
            "Epoch [22], Batch [684], Train Loss: 0.6958866119384766\n",
            "Epoch [22], Batch [685], Train Loss: 0.665885329246521\n",
            "Epoch [22], Batch [686], Train Loss: 0.6193812489509583\n",
            "Epoch [22], Batch [687], Train Loss: 0.9649430513381958\n",
            "Epoch [22], Batch [688], Train Loss: 0.8482459187507629\n",
            "Epoch [22], Batch [689], Train Loss: 0.8502700924873352\n",
            "Epoch [22], Batch [690], Train Loss: 0.8265280723571777\n",
            "Epoch [22], Batch [691], Train Loss: 0.8811550736427307\n",
            "Epoch [22], Batch [692], Train Loss: 0.7708724141120911\n",
            "Epoch [22], Batch [693], Train Loss: 0.8919980525970459\n",
            "Epoch [22], Batch [694], Train Loss: 0.790686309337616\n",
            "Epoch [22], Batch [695], Train Loss: 0.842901349067688\n",
            "Epoch [22], Batch [696], Train Loss: 0.898464024066925\n",
            "Epoch [22], Batch [697], Train Loss: 0.8165324926376343\n",
            "Epoch [22], Batch [698], Train Loss: 0.8746064901351929\n",
            "Epoch [22], Batch [699], Train Loss: 0.7172924280166626\n",
            "Epoch [22], Batch [700], Train Loss: 0.9558634757995605\n",
            "Epoch [22], Batch [701], Train Loss: 0.8282151222229004\n",
            "Epoch [22], Batch [702], Train Loss: 0.8591080904006958\n",
            "Epoch [22], Batch [703], Train Loss: 0.8320529460906982\n",
            "Epoch [22], Batch [704], Train Loss: 0.8105583786964417\n",
            "Epoch [22], Batch [705], Train Loss: 0.8774044513702393\n",
            "Epoch [22], Batch [706], Train Loss: 0.6812558770179749\n",
            "Epoch [22], Batch [707], Train Loss: 0.8203052282333374\n",
            "Epoch [22], Batch [708], Train Loss: 1.15944242477417\n",
            "Epoch [22], Batch [709], Train Loss: 0.909088134765625\n",
            "Epoch [22], Batch [710], Train Loss: 0.8406869173049927\n",
            "Epoch [22], Batch [711], Train Loss: 0.8927568197250366\n",
            "Epoch [22], Batch [712], Train Loss: 0.7442343831062317\n",
            "Epoch [22], Batch [713], Train Loss: 0.6154966354370117\n",
            "Epoch [22], Batch [714], Train Loss: 0.8055304288864136\n",
            "Epoch [22], Batch [715], Train Loss: 0.9104589223861694\n",
            "Epoch [22], Batch [716], Train Loss: 0.8640305399894714\n",
            "Epoch [22], Batch [717], Train Loss: 0.8172569274902344\n",
            "Epoch [22], Batch [718], Train Loss: 0.9114180207252502\n",
            "Epoch [22], Batch [719], Train Loss: 0.7405110597610474\n",
            "Epoch [22], Batch [720], Train Loss: 0.6969355344772339\n",
            "Epoch [22], Batch [721], Train Loss: 0.9102506041526794\n",
            "Epoch [22], Batch [722], Train Loss: 0.7548113465309143\n",
            "Epoch [22], Batch [723], Train Loss: 0.8426541090011597\n",
            "Epoch [22], Batch [724], Train Loss: 0.7893949747085571\n",
            "Epoch [22], Batch [725], Train Loss: 0.7307625412940979\n",
            "Epoch [22], Batch [726], Train Loss: 0.8794476985931396\n",
            "Epoch [22], Batch [727], Train Loss: 1.054120421409607\n",
            "Epoch [22], Batch [728], Train Loss: 0.8065310716629028\n",
            "Epoch [22], Batch [729], Train Loss: 0.7097399830818176\n",
            "Epoch [22], Batch [730], Train Loss: 0.8219093680381775\n",
            "Epoch [22], Batch [731], Train Loss: 0.7955507040023804\n",
            "Epoch [22], Batch [732], Train Loss: 0.8914468288421631\n",
            "Epoch [22], Batch [733], Train Loss: 0.7115645408630371\n",
            "Epoch [22], Batch [734], Train Loss: 0.6739544868469238\n",
            "Epoch [22], Batch [735], Train Loss: 0.7081851959228516\n",
            "Epoch [22], Batch [736], Train Loss: 0.689009964466095\n",
            "Epoch [22], Batch [737], Train Loss: 0.8999765515327454\n",
            "Epoch [22], Batch [738], Train Loss: 0.7336690425872803\n",
            "Epoch [22], Batch [739], Train Loss: 0.7793744206428528\n",
            "Epoch [22], Batch [740], Train Loss: 0.6665926575660706\n",
            "Epoch [22], Batch [741], Train Loss: 0.8399875164031982\n",
            "Epoch [22], Batch [742], Train Loss: 0.7723710536956787\n",
            "Epoch [22], Batch [743], Train Loss: 0.8894086480140686\n",
            "Epoch [22], Batch [744], Train Loss: 0.5976279973983765\n",
            "Epoch [22], Batch [745], Train Loss: 0.9048269987106323\n",
            "Epoch [22], Batch [746], Train Loss: 0.8179295063018799\n",
            "Epoch [22], Batch [747], Train Loss: 0.798812985420227\n",
            "Epoch [22], Batch [748], Train Loss: 0.6449375748634338\n",
            "Epoch [22], Batch [749], Train Loss: 0.6887726783752441\n",
            "Epoch [22], Batch [750], Train Loss: 0.6303848624229431\n",
            "Epoch [22], Batch [751], Train Loss: 0.7323991656303406\n",
            "Epoch [22], Batch [752], Train Loss: 0.8501616716384888\n",
            "Epoch [22], Batch [753], Train Loss: 0.8544047474861145\n",
            "Epoch [22], Batch [754], Train Loss: 0.6828281879425049\n",
            "Epoch [22], Batch [755], Train Loss: 0.6203508377075195\n",
            "Epoch [22], Batch [756], Train Loss: 0.8310127258300781\n",
            "Epoch [22], Batch [757], Train Loss: 0.6925590634346008\n",
            "Epoch [22], Batch [758], Train Loss: 0.8027795553207397\n",
            "Epoch [22], Batch [759], Train Loss: 0.802135705947876\n",
            "Epoch [22], Batch [760], Train Loss: 0.7216098308563232\n",
            "Epoch [22], Batch [761], Train Loss: 0.8831652402877808\n",
            "Epoch [22], Batch [762], Train Loss: 0.8152658939361572\n",
            "Epoch [22], Batch [763], Train Loss: 1.0046024322509766\n",
            "Epoch [22], Batch [764], Train Loss: 0.8576998114585876\n",
            "Epoch [22], Batch [765], Train Loss: 0.9057102799415588\n",
            "Epoch [22], Batch [766], Train Loss: 0.7082073092460632\n",
            "Epoch [22], Batch [767], Train Loss: 0.8246934413909912\n",
            "Epoch [22], Batch [768], Train Loss: 1.047046422958374\n",
            "Epoch [22], Batch [769], Train Loss: 0.7814784049987793\n",
            "Epoch [22], Batch [770], Train Loss: 0.6281777620315552\n",
            "Epoch [22], Batch [771], Train Loss: 0.745446503162384\n",
            "Epoch [22], Batch [772], Train Loss: 0.7415844798088074\n",
            "Epoch [22], Batch [773], Train Loss: 0.7136430740356445\n",
            "Epoch [22], Batch [774], Train Loss: 0.9662980437278748\n",
            "Epoch [22], Batch [775], Train Loss: 0.7102946043014526\n",
            "Epoch [22], Batch [776], Train Loss: 0.6407439708709717\n",
            "Epoch [22], Batch [777], Train Loss: 0.7290474772453308\n",
            "Epoch [22], Batch [778], Train Loss: 0.7573334574699402\n",
            "Epoch [22], Batch [779], Train Loss: 0.7976566553115845\n",
            "Epoch [22], Batch [780], Train Loss: 0.7516113519668579\n",
            "Epoch [22], Batch [781], Train Loss: 0.6290543079376221\n",
            "Epoch [22], Batch [782], Train Loss: 0.6328153014183044\n",
            "Epoch [22], Batch [783], Train Loss: 0.8418008089065552\n",
            "Epoch [22], Batch [784], Train Loss: 0.6228482723236084\n",
            "Epoch [22], Batch [785], Train Loss: 0.7101073265075684\n",
            "Epoch [22], Batch [786], Train Loss: 0.9277451038360596\n",
            "Epoch [22], Batch [787], Train Loss: 0.9220288991928101\n",
            "Epoch [22], Batch [788], Train Loss: 0.8249773979187012\n",
            "Epoch [22], Batch [789], Train Loss: 0.9693965315818787\n",
            "Epoch [22], Batch [790], Train Loss: 0.7713600993156433\n",
            "Epoch [22], Batch [791], Train Loss: 0.7687942385673523\n",
            "Epoch [22], Batch [792], Train Loss: 0.751187264919281\n",
            "Epoch [22], Batch [793], Train Loss: 0.6199200749397278\n",
            "Epoch [22], Batch [794], Train Loss: 0.803625226020813\n",
            "Epoch [22], Batch [795], Train Loss: 0.8288977742195129\n",
            "Epoch [22], Batch [796], Train Loss: 0.778978168964386\n",
            "Epoch [22], Batch [797], Train Loss: 0.9908912181854248\n",
            "Epoch [22], Batch [798], Train Loss: 0.6889033317565918\n",
            "Epoch [22], Batch [799], Train Loss: 0.6935185790061951\n",
            "Epoch [22], Batch [800], Train Loss: 0.8104478716850281\n",
            "Epoch [22], Batch [801], Train Loss: 0.9236778020858765\n",
            "Epoch [22], Batch [802], Train Loss: 0.9315338134765625\n",
            "Epoch [22], Batch [803], Train Loss: 0.9341646432876587\n",
            "Epoch [22], Batch [804], Train Loss: 0.9566185474395752\n",
            "Epoch [22], Batch [805], Train Loss: 1.1490055322647095\n",
            "Epoch [22], Batch [806], Train Loss: 0.9479227066040039\n",
            "Epoch [22], Batch [807], Train Loss: 1.1515913009643555\n",
            "Epoch [22], Batch [808], Train Loss: 0.7053123712539673\n",
            "Epoch [22], Batch [809], Train Loss: 0.7788252830505371\n",
            "Epoch [22], Batch [810], Train Loss: 0.8145407438278198\n",
            "Epoch [22], Batch [811], Train Loss: 0.812755823135376\n",
            "Epoch [22], Batch [812], Train Loss: 0.6845853924751282\n",
            "Epoch [22], Batch [813], Train Loss: 0.7694344520568848\n",
            "Epoch [22], Batch [814], Train Loss: 1.0018057823181152\n",
            "Epoch [22], Batch [815], Train Loss: 0.9151281714439392\n",
            "Epoch [22], Batch [816], Train Loss: 0.5994315147399902\n",
            "Epoch [22], Batch [817], Train Loss: 0.539379894733429\n",
            "Epoch [22], Batch [818], Train Loss: 0.7612214684486389\n",
            "Epoch [22], Batch [819], Train Loss: 0.7839435338973999\n",
            "Epoch [22], Batch [820], Train Loss: 0.9077143669128418\n",
            "Epoch [22], Batch [821], Train Loss: 0.7318211793899536\n",
            "Epoch [22], Batch [822], Train Loss: 0.6915024518966675\n",
            "Epoch [22], Batch [823], Train Loss: 0.8815670609474182\n",
            "Epoch [22], Batch [824], Train Loss: 0.9306756258010864\n",
            "Epoch [22], Batch [825], Train Loss: 0.7310358285903931\n",
            "Epoch [22], Batch [826], Train Loss: 0.8441875576972961\n",
            "Epoch [22], Batch [827], Train Loss: 0.6752370595932007\n",
            "Epoch [22], Batch [828], Train Loss: 0.7069928646087646\n",
            "Epoch [22], Batch [829], Train Loss: 0.9112012386322021\n",
            "Epoch [22], Batch [830], Train Loss: 0.9216697216033936\n",
            "Epoch [22], Batch [831], Train Loss: 0.9376462697982788\n",
            "Epoch [22], Batch [832], Train Loss: 0.8991057872772217\n",
            "Epoch [22], Batch [833], Train Loss: 0.8572546243667603\n",
            "Epoch [22], Batch [834], Train Loss: 0.6633748412132263\n",
            "Epoch [22], Batch [835], Train Loss: 0.8359212875366211\n",
            "Epoch [22], Batch [836], Train Loss: 0.9980106353759766\n",
            "Epoch [22], Batch [837], Train Loss: 0.9215954542160034\n",
            "Epoch [22], Batch [838], Train Loss: 0.9456897974014282\n",
            "Epoch [22], Batch [839], Train Loss: 0.7596264481544495\n",
            "Epoch [22], Batch [840], Train Loss: 1.0976316928863525\n",
            "Epoch [22], Batch [841], Train Loss: 0.8863529562950134\n",
            "Epoch [22], Batch [842], Train Loss: 0.8784291744232178\n",
            "Epoch [22], Batch [843], Train Loss: 0.720614492893219\n",
            "Epoch [22], Batch [844], Train Loss: 0.910438060760498\n",
            "Epoch [22], Batch [845], Train Loss: 0.7735362648963928\n",
            "Epoch [22], Batch [846], Train Loss: 0.7552136778831482\n",
            "Epoch [22], Batch [847], Train Loss: 0.719363272190094\n",
            "Epoch [22], Batch [848], Train Loss: 0.7114832401275635\n",
            "Epoch [22], Batch [849], Train Loss: 1.0245821475982666\n",
            "Epoch [22], Batch [850], Train Loss: 0.5439975261688232\n",
            "Epoch [22], Batch [851], Train Loss: 0.8901499509811401\n",
            "Epoch [22], Batch [852], Train Loss: 1.0031251907348633\n",
            "Epoch [22], Batch [853], Train Loss: 0.7141855359077454\n",
            "Epoch [22], Batch [854], Train Loss: 0.9209359288215637\n",
            "Epoch [22], Batch [855], Train Loss: 0.7523702383041382\n",
            "Epoch [22], Batch [856], Train Loss: 1.1660315990447998\n",
            "Epoch [22], Batch [857], Train Loss: 0.9225079417228699\n",
            "Epoch [22], Batch [858], Train Loss: 0.7579331398010254\n",
            "Epoch [22], Batch [859], Train Loss: 0.9087263345718384\n",
            "Epoch [22], Batch [860], Train Loss: 0.7774012684822083\n",
            "Epoch [22], Batch [861], Train Loss: 0.8333863019943237\n",
            "Epoch [22], Batch [862], Train Loss: 0.8491582274436951\n",
            "Epoch [22], Batch [863], Train Loss: 0.8792474865913391\n",
            "Epoch [22], Batch [864], Train Loss: 0.7600811719894409\n",
            "Epoch [22], Batch [865], Train Loss: 0.7357628345489502\n",
            "Epoch [22], Batch [866], Train Loss: 1.1909377574920654\n",
            "Epoch [22], Batch [867], Train Loss: 0.6896830201148987\n",
            "Epoch [22], Batch [868], Train Loss: 0.7632522583007812\n",
            "Epoch [22], Batch [869], Train Loss: 0.9689940214157104\n",
            "Epoch [22], Batch [870], Train Loss: 0.7901836037635803\n",
            "Epoch [22], Batch [871], Train Loss: 0.9598121643066406\n",
            "Epoch [22], Batch [872], Train Loss: 0.6967051029205322\n",
            "Epoch [22], Batch [873], Train Loss: 0.6201539039611816\n",
            "Epoch [22], Batch [874], Train Loss: 0.8753061294555664\n",
            "Epoch [22], Batch [875], Train Loss: 0.6983535289764404\n",
            "Epoch [22], Batch [876], Train Loss: 0.8364410996437073\n",
            "Epoch [22], Batch [877], Train Loss: 0.8578797578811646\n",
            "Epoch [22], Batch [878], Train Loss: 0.7043810486793518\n",
            "Epoch [22], Batch [879], Train Loss: 0.7465242147445679\n",
            "Epoch [22], Batch [880], Train Loss: 0.9184389710426331\n",
            "Epoch [22], Batch [881], Train Loss: 0.5472788214683533\n",
            "Epoch [22], Batch [882], Train Loss: 0.9451764822006226\n",
            "Epoch [22], Batch [883], Train Loss: 1.1089625358581543\n",
            "Epoch [22], Batch [884], Train Loss: 0.6620577573776245\n",
            "Epoch [22], Batch [885], Train Loss: 0.9398963451385498\n",
            "Epoch [22], Batch [886], Train Loss: 0.9666361808776855\n",
            "Epoch [22], Batch [887], Train Loss: 0.815353274345398\n",
            "Epoch [22], Batch [888], Train Loss: 0.7089242935180664\n",
            "Epoch [22], Batch [889], Train Loss: 0.7725411653518677\n",
            "Epoch [22], Batch [890], Train Loss: 0.8028900623321533\n",
            "Epoch [22], Batch [891], Train Loss: 0.7112122774124146\n",
            "Epoch [22], Batch [892], Train Loss: 0.8045095205307007\n",
            "Epoch [22], Batch [893], Train Loss: 0.7882181406021118\n",
            "Epoch [22], Batch [894], Train Loss: 0.840831458568573\n",
            "Epoch [22], Batch [895], Train Loss: 0.9380958080291748\n",
            "Epoch [22], Batch [896], Train Loss: 0.7733728885650635\n",
            "Epoch [22], Batch [897], Train Loss: 0.9190261363983154\n",
            "Epoch [22], Batch [898], Train Loss: 0.7040248513221741\n",
            "Epoch [22], Batch [899], Train Loss: 0.5439046621322632\n",
            "Epoch [22], Batch [900], Train Loss: 0.7585956454277039\n",
            "Epoch [22], Batch [901], Train Loss: 0.8350244760513306\n",
            "Epoch [22], Batch [902], Train Loss: 0.6983159184455872\n",
            "Epoch [22], Batch [903], Train Loss: 0.7755987644195557\n",
            "Epoch [22], Batch [904], Train Loss: 0.8768759965896606\n",
            "Epoch [22], Batch [905], Train Loss: 0.8374969363212585\n",
            "Epoch [22], Batch [906], Train Loss: 0.6213970184326172\n",
            "Epoch [22], Batch [907], Train Loss: 0.945005476474762\n",
            "Epoch [22], Batch [908], Train Loss: 0.7557193636894226\n",
            "Epoch [22], Batch [909], Train Loss: 0.5873728394508362\n",
            "Epoch [22], Batch [910], Train Loss: 0.7044306993484497\n",
            "Epoch [22], Batch [911], Train Loss: 0.8435807228088379\n",
            "Epoch [22], Batch [912], Train Loss: 0.9190554022789001\n",
            "Epoch [22], Batch [913], Train Loss: 0.8223356604576111\n",
            "Epoch [22], Batch [914], Train Loss: 0.754996657371521\n",
            "Epoch [22], Batch [915], Train Loss: 0.811843752861023\n",
            "Epoch [22], Batch [916], Train Loss: 0.7600449323654175\n",
            "Epoch [22], Batch [917], Train Loss: 0.9518775939941406\n",
            "Epoch [22], Batch [918], Train Loss: 0.729698657989502\n",
            "Epoch [22], Batch [919], Train Loss: 0.9310407042503357\n",
            "Epoch [22], Batch [920], Train Loss: 0.7691671848297119\n",
            "Epoch [22], Batch [921], Train Loss: 0.9158978462219238\n",
            "Epoch [22], Batch [922], Train Loss: 0.8749465942382812\n",
            "Epoch [22], Batch [923], Train Loss: 0.8239351511001587\n",
            "Epoch [22], Batch [924], Train Loss: 0.956445574760437\n",
            "Epoch [22], Batch [925], Train Loss: 0.7092081904411316\n",
            "Epoch [22], Batch [926], Train Loss: 0.7188806533813477\n",
            "Epoch [22], Batch [927], Train Loss: 0.742389976978302\n",
            "Epoch [22], Batch [928], Train Loss: 1.0355185270309448\n",
            "Epoch [22], Batch [929], Train Loss: 0.8402910828590393\n",
            "Epoch [22], Batch [930], Train Loss: 0.6370571851730347\n",
            "Epoch [22], Batch [931], Train Loss: 0.7331123352050781\n",
            "Epoch [22], Batch [932], Train Loss: 0.9382725358009338\n",
            "Epoch [22], Batch [933], Train Loss: 0.8505666255950928\n",
            "Epoch [22], Batch [934], Train Loss: 0.9175757765769958\n",
            "Epoch [22], Batch [935], Train Loss: 1.024748682975769\n",
            "Epoch [22], Batch [936], Train Loss: 0.8202189803123474\n",
            "Epoch [22], Batch [937], Train Loss: 0.8413553833961487\n",
            "Epoch [22], Batch [938], Train Loss: 0.7862180471420288\n",
            "Accuracy of train set: 0.6960333333333333\n",
            "Epoch [23], Batch [1], Loss: 0.7946963310241699\n",
            "Epoch [23], Batch [2], Loss: 0.6251797080039978\n",
            "Epoch [23], Batch [3], Loss: 0.703243613243103\n",
            "Epoch [23], Batch [4], Loss: 0.715956449508667\n",
            "Epoch [23], Batch [5], Loss: 0.7479446530342102\n",
            "Epoch [23], Batch [6], Loss: 0.7343370914459229\n",
            "Epoch [23], Batch [7], Loss: 0.7967621088027954\n",
            "Epoch [23], Batch [8], Loss: 0.8387936949729919\n",
            "Epoch [23], Batch [9], Loss: 0.8200587630271912\n",
            "Epoch [23], Batch [10], Loss: 0.9544060826301575\n",
            "Epoch [23], Batch [11], Loss: 0.9559322595596313\n",
            "Epoch [23], Batch [12], Loss: 0.7960547208786011\n",
            "Epoch [23], Batch [13], Loss: 0.6368017196655273\n",
            "Epoch [23], Batch [14], Loss: 0.6734514832496643\n",
            "Epoch [23], Batch [15], Loss: 0.7203950881958008\n",
            "Epoch [23], Batch [16], Loss: 0.9203739166259766\n",
            "Epoch [23], Batch [17], Loss: 0.7483599781990051\n",
            "Epoch [23], Batch [18], Loss: 0.9679975509643555\n",
            "Epoch [23], Batch [19], Loss: 0.8503239154815674\n",
            "Epoch [23], Batch [20], Loss: 0.9327651262283325\n",
            "Epoch [23], Batch [21], Loss: 0.9459367990493774\n",
            "Epoch [23], Batch [22], Loss: 0.7966008186340332\n",
            "Epoch [23], Batch [23], Loss: 0.7083341479301453\n",
            "Epoch [23], Batch [24], Loss: 0.7153424620628357\n",
            "Epoch [23], Batch [25], Loss: 0.7421567440032959\n",
            "Epoch [23], Batch [26], Loss: 0.8333991765975952\n",
            "Epoch [23], Batch [27], Loss: 0.7898988723754883\n",
            "Epoch [23], Batch [28], Loss: 0.858809769153595\n",
            "Epoch [23], Batch [29], Loss: 0.8789461255073547\n",
            "Epoch [23], Batch [30], Loss: 0.4704819321632385\n",
            "Epoch [23], Batch [31], Loss: 0.9054323434829712\n",
            "Epoch [23], Batch [32], Loss: 0.6789050102233887\n",
            "Epoch [23], Batch [33], Loss: 0.8947387933731079\n",
            "Epoch [23], Batch [34], Loss: 0.7040705680847168\n",
            "Epoch [23], Batch [35], Loss: 0.7117130756378174\n",
            "Epoch [23], Batch [36], Loss: 0.7330083847045898\n",
            "Epoch [23], Batch [37], Loss: 0.6788844466209412\n",
            "Epoch [23], Batch [38], Loss: 0.7537555694580078\n",
            "Epoch [23], Batch [39], Loss: 0.8337556719779968\n",
            "Epoch [23], Batch [40], Loss: 0.7736952304840088\n",
            "Epoch [23], Batch [41], Loss: 0.8480430841445923\n",
            "Epoch [23], Batch [42], Loss: 0.9320796132087708\n",
            "Epoch [23], Batch [43], Loss: 0.9483685493469238\n",
            "Epoch [23], Batch [44], Loss: 0.6959791779518127\n",
            "Epoch [23], Batch [45], Loss: 0.9007267951965332\n",
            "Epoch [23], Batch [46], Loss: 1.0674806833267212\n",
            "Epoch [23], Batch [47], Loss: 0.7870420217514038\n",
            "Epoch [23], Batch [48], Loss: 1.0250002145767212\n",
            "Epoch [23], Batch [49], Loss: 0.8868061304092407\n",
            "Epoch [23], Batch [50], Loss: 0.7187334299087524\n",
            "Epoch [23], Batch [51], Loss: 1.1312862634658813\n",
            "Epoch [23], Batch [52], Loss: 0.9065425395965576\n",
            "Epoch [23], Batch [53], Loss: 0.725298285484314\n",
            "Epoch [23], Batch [54], Loss: 0.904610276222229\n",
            "Epoch [23], Batch [55], Loss: 1.0819003582000732\n",
            "Epoch [23], Batch [56], Loss: 0.771918773651123\n",
            "Epoch [23], Batch [57], Loss: 0.9113977551460266\n",
            "Epoch [23], Batch [58], Loss: 0.9673225283622742\n",
            "Epoch [23], Batch [59], Loss: 0.8082879185676575\n",
            "Epoch [23], Batch [60], Loss: 0.9955472350120544\n",
            "Epoch [23], Batch [61], Loss: 0.7925620675086975\n",
            "Epoch [23], Batch [62], Loss: 1.1639803647994995\n",
            "Epoch [23], Batch [63], Loss: 0.7977818846702576\n",
            "Epoch [23], Batch [64], Loss: 0.9901003837585449\n",
            "Epoch [23], Batch [65], Loss: 0.9724636077880859\n",
            "Epoch [23], Batch [66], Loss: 0.8093550205230713\n",
            "Epoch [23], Batch [67], Loss: 0.8697865605354309\n",
            "Epoch [23], Batch [68], Loss: 0.6536995768547058\n",
            "Epoch [23], Batch [69], Loss: 0.9094546437263489\n",
            "Epoch [23], Batch [70], Loss: 0.5805529952049255\n",
            "Epoch [23], Batch [71], Loss: 0.5892261266708374\n",
            "Epoch [23], Batch [72], Loss: 0.5768104195594788\n",
            "Epoch [23], Batch [73], Loss: 0.826768696308136\n",
            "Epoch [23], Batch [74], Loss: 0.8976264595985413\n",
            "Epoch [23], Batch [75], Loss: 0.7437335252761841\n",
            "Epoch [23], Batch [76], Loss: 0.9127498269081116\n",
            "Epoch [23], Batch [77], Loss: 0.8612809181213379\n",
            "Epoch [23], Batch [78], Loss: 0.7248740196228027\n",
            "Epoch [23], Batch [79], Loss: 1.0818992853164673\n",
            "Epoch [23], Batch [80], Loss: 0.8855928778648376\n",
            "Epoch [23], Batch [81], Loss: 1.0258228778839111\n",
            "Epoch [23], Batch [82], Loss: 0.7607901692390442\n",
            "Epoch [23], Batch [83], Loss: 0.7356908321380615\n",
            "Epoch [23], Batch [84], Loss: 0.7508148550987244\n",
            "Epoch [23], Batch [85], Loss: 0.7674745321273804\n",
            "Epoch [23], Batch [86], Loss: 0.9012163877487183\n",
            "Epoch [23], Batch [87], Loss: 0.955557644367218\n",
            "Epoch [23], Batch [88], Loss: 0.9879217147827148\n",
            "Epoch [23], Batch [89], Loss: 0.907239556312561\n",
            "Epoch [23], Batch [90], Loss: 0.7671587467193604\n",
            "Epoch [23], Batch [91], Loss: 0.8191366791725159\n",
            "Epoch [23], Batch [92], Loss: 0.6680036783218384\n",
            "Epoch [23], Batch [93], Loss: 0.5868357419967651\n",
            "Epoch [23], Batch [94], Loss: 0.9237786531448364\n",
            "Epoch [23], Batch [95], Loss: 0.7004201412200928\n",
            "Epoch [23], Batch [96], Loss: 0.8801604509353638\n",
            "Epoch [23], Batch [97], Loss: 0.7034317255020142\n",
            "Epoch [23], Batch [98], Loss: 0.8457980751991272\n",
            "Epoch [23], Batch [99], Loss: 1.0213714838027954\n",
            "Epoch [23], Batch [100], Loss: 0.6709694862365723\n",
            "Epoch [23], Batch [101], Loss: 0.832570493221283\n",
            "Epoch [23], Batch [102], Loss: 0.8292816877365112\n",
            "Epoch [23], Batch [103], Loss: 0.7248916625976562\n",
            "Epoch [23], Batch [104], Loss: 0.8054212331771851\n",
            "Epoch [23], Batch [105], Loss: 1.0543357133865356\n",
            "Epoch [23], Batch [106], Loss: 0.6419180035591125\n",
            "Epoch [23], Batch [107], Loss: 0.8606733083724976\n",
            "Epoch [23], Batch [108], Loss: 0.7700714468955994\n",
            "Epoch [23], Batch [109], Loss: 0.8781843185424805\n",
            "Epoch [23], Batch [110], Loss: 0.8004193902015686\n",
            "Epoch [23], Batch [111], Loss: 0.7399588823318481\n",
            "Epoch [23], Batch [112], Loss: 0.5894538760185242\n",
            "Epoch [23], Batch [113], Loss: 0.8957445621490479\n",
            "Epoch [23], Batch [114], Loss: 0.8051729202270508\n",
            "Epoch [23], Batch [115], Loss: 0.7588220834732056\n",
            "Epoch [23], Batch [116], Loss: 0.8196530938148499\n",
            "Epoch [23], Batch [117], Loss: 0.7833750247955322\n",
            "Epoch [23], Batch [118], Loss: 0.7807954549789429\n",
            "Epoch [23], Batch [119], Loss: 0.8112572431564331\n",
            "Epoch [23], Batch [120], Loss: 0.7261672019958496\n",
            "Epoch [23], Batch [121], Loss: 0.79619300365448\n",
            "Epoch [23], Batch [122], Loss: 0.6643887162208557\n",
            "Epoch [23], Batch [123], Loss: 0.8167774081230164\n",
            "Epoch [23], Batch [124], Loss: 1.1349437236785889\n",
            "Epoch [23], Batch [125], Loss: 0.8502918481826782\n",
            "Epoch [23], Batch [126], Loss: 0.838374137878418\n",
            "Epoch [23], Batch [127], Loss: 0.6647151112556458\n",
            "Epoch [23], Batch [128], Loss: 0.8477704524993896\n",
            "Epoch [23], Batch [129], Loss: 0.8852347731590271\n",
            "Epoch [23], Batch [130], Loss: 0.8683342337608337\n",
            "Epoch [23], Batch [131], Loss: 0.6270463466644287\n",
            "Epoch [23], Batch [132], Loss: 0.6313313245773315\n",
            "Epoch [23], Batch [133], Loss: 0.7026662230491638\n",
            "Epoch [23], Batch [134], Loss: 0.7929235100746155\n",
            "Epoch [23], Batch [135], Loss: 0.7942506074905396\n",
            "Epoch [23], Batch [136], Loss: 0.7147889733314514\n",
            "Epoch [23], Batch [137], Loss: 0.974066436290741\n",
            "Epoch [23], Batch [138], Loss: 0.7337908148765564\n",
            "Epoch [23], Batch [139], Loss: 0.8331704139709473\n",
            "Epoch [23], Batch [140], Loss: 0.8632326722145081\n",
            "Epoch [23], Batch [141], Loss: 0.6850227117538452\n",
            "Epoch [23], Batch [142], Loss: 0.8700664043426514\n",
            "Epoch [23], Batch [143], Loss: 0.9036712646484375\n",
            "Epoch [23], Batch [144], Loss: 0.8935384750366211\n",
            "Epoch [23], Batch [145], Loss: 0.830899178981781\n",
            "Epoch [23], Batch [146], Loss: 0.8229990005493164\n",
            "Epoch [23], Batch [147], Loss: 0.8142207264900208\n",
            "Epoch [23], Batch [148], Loss: 0.6490058898925781\n",
            "Epoch [23], Batch [149], Loss: 0.9787154793739319\n",
            "Epoch [23], Batch [150], Loss: 1.0337096452713013\n",
            "Epoch [23], Batch [151], Loss: 1.0833592414855957\n",
            "Epoch [23], Batch [152], Loss: 0.6336288452148438\n",
            "Epoch [23], Batch [153], Loss: 0.7005294561386108\n",
            "Epoch [23], Batch [154], Loss: 0.7185037732124329\n",
            "Epoch [23], Batch [155], Loss: 1.1409122943878174\n",
            "Epoch [23], Batch [156], Loss: 0.7775791883468628\n",
            "Epoch [23], Batch [157], Loss: 0.715063214302063\n",
            "Accuracy of None set: 0.687\n",
            "Epoch [23], Batch [1], Train Loss: 0.9064539670944214\n",
            "Epoch [23], Batch [2], Train Loss: 0.7863500714302063\n",
            "Epoch [23], Batch [3], Train Loss: 0.969666600227356\n",
            "Epoch [23], Batch [4], Train Loss: 0.973008394241333\n",
            "Epoch [23], Batch [5], Train Loss: 0.8085987567901611\n",
            "Epoch [23], Batch [6], Train Loss: 0.8299165964126587\n",
            "Epoch [23], Batch [7], Train Loss: 0.7362521290779114\n",
            "Epoch [23], Batch [8], Train Loss: 0.7138089537620544\n",
            "Epoch [23], Batch [9], Train Loss: 0.8216127157211304\n",
            "Epoch [23], Batch [10], Train Loss: 0.7427837252616882\n",
            "Epoch [23], Batch [11], Train Loss: 0.6158801913261414\n",
            "Epoch [23], Batch [12], Train Loss: 0.878747820854187\n",
            "Epoch [23], Batch [13], Train Loss: 0.9286437034606934\n",
            "Epoch [23], Batch [14], Train Loss: 0.7784984707832336\n",
            "Epoch [23], Batch [15], Train Loss: 0.7887765169143677\n",
            "Epoch [23], Batch [16], Train Loss: 0.694589376449585\n",
            "Epoch [23], Batch [17], Train Loss: 0.6226166486740112\n",
            "Epoch [23], Batch [18], Train Loss: 0.966262698173523\n",
            "Epoch [23], Batch [19], Train Loss: 0.804994523525238\n",
            "Epoch [23], Batch [20], Train Loss: 0.7059059739112854\n",
            "Epoch [23], Batch [21], Train Loss: 0.7145629525184631\n",
            "Epoch [23], Batch [22], Train Loss: 0.8467487096786499\n",
            "Epoch [23], Batch [23], Train Loss: 0.7631931900978088\n",
            "Epoch [23], Batch [24], Train Loss: 0.9030333161354065\n",
            "Epoch [23], Batch [25], Train Loss: 1.0036888122558594\n",
            "Epoch [23], Batch [26], Train Loss: 0.7302514314651489\n",
            "Epoch [23], Batch [27], Train Loss: 0.6488780975341797\n",
            "Epoch [23], Batch [28], Train Loss: 0.74183189868927\n",
            "Epoch [23], Batch [29], Train Loss: 0.7867887616157532\n",
            "Epoch [23], Batch [30], Train Loss: 0.7021396160125732\n",
            "Epoch [23], Batch [31], Train Loss: 0.6681880354881287\n",
            "Epoch [23], Batch [32], Train Loss: 0.619640588760376\n",
            "Epoch [23], Batch [33], Train Loss: 0.7233538627624512\n",
            "Epoch [23], Batch [34], Train Loss: 0.7127809524536133\n",
            "Epoch [23], Batch [35], Train Loss: 0.8442996740341187\n",
            "Epoch [23], Batch [36], Train Loss: 0.61292564868927\n",
            "Epoch [23], Batch [37], Train Loss: 0.7909061312675476\n",
            "Epoch [23], Batch [38], Train Loss: 0.9226893782615662\n",
            "Epoch [23], Batch [39], Train Loss: 0.9719707369804382\n",
            "Epoch [23], Batch [40], Train Loss: 0.8400051593780518\n",
            "Epoch [23], Batch [41], Train Loss: 0.8206067681312561\n",
            "Epoch [23], Batch [42], Train Loss: 0.714280366897583\n",
            "Epoch [23], Batch [43], Train Loss: 0.6860122680664062\n",
            "Epoch [23], Batch [44], Train Loss: 0.968433141708374\n",
            "Epoch [23], Batch [45], Train Loss: 0.69007408618927\n",
            "Epoch [23], Batch [46], Train Loss: 0.8779639601707458\n",
            "Epoch [23], Batch [47], Train Loss: 0.7215895652770996\n",
            "Epoch [23], Batch [48], Train Loss: 0.6476292610168457\n",
            "Epoch [23], Batch [49], Train Loss: 0.6873417496681213\n",
            "Epoch [23], Batch [50], Train Loss: 0.7073193192481995\n",
            "Epoch [23], Batch [51], Train Loss: 0.8517871499061584\n",
            "Epoch [23], Batch [52], Train Loss: 0.7343783378601074\n",
            "Epoch [23], Batch [53], Train Loss: 0.7947171330451965\n",
            "Epoch [23], Batch [54], Train Loss: 0.6527931094169617\n",
            "Epoch [23], Batch [55], Train Loss: 0.947838306427002\n",
            "Epoch [23], Batch [56], Train Loss: 0.7629353404045105\n",
            "Epoch [23], Batch [57], Train Loss: 0.7155358195304871\n",
            "Epoch [23], Batch [58], Train Loss: 0.8926805853843689\n",
            "Epoch [23], Batch [59], Train Loss: 0.9325981140136719\n",
            "Epoch [23], Batch [60], Train Loss: 0.9083861112594604\n",
            "Epoch [23], Batch [61], Train Loss: 0.5919181704521179\n",
            "Epoch [23], Batch [62], Train Loss: 0.7966586947441101\n",
            "Epoch [23], Batch [63], Train Loss: 0.8268218636512756\n",
            "Epoch [23], Batch [64], Train Loss: 0.6681324243545532\n",
            "Epoch [23], Batch [65], Train Loss: 0.7424572706222534\n",
            "Epoch [23], Batch [66], Train Loss: 0.8564256429672241\n",
            "Epoch [23], Batch [67], Train Loss: 0.8040285110473633\n",
            "Epoch [23], Batch [68], Train Loss: 0.9570960402488708\n",
            "Epoch [23], Batch [69], Train Loss: 0.8713751435279846\n",
            "Epoch [23], Batch [70], Train Loss: 0.825379490852356\n",
            "Epoch [23], Batch [71], Train Loss: 0.7796168327331543\n",
            "Epoch [23], Batch [72], Train Loss: 0.9398459792137146\n",
            "Epoch [23], Batch [73], Train Loss: 0.7933188676834106\n",
            "Epoch [23], Batch [74], Train Loss: 0.7209243178367615\n",
            "Epoch [23], Batch [75], Train Loss: 0.7044786810874939\n",
            "Epoch [23], Batch [76], Train Loss: 0.9515524506568909\n",
            "Epoch [23], Batch [77], Train Loss: 0.7904609441757202\n",
            "Epoch [23], Batch [78], Train Loss: 0.7957176566123962\n",
            "Epoch [23], Batch [79], Train Loss: 0.7836462259292603\n",
            "Epoch [23], Batch [80], Train Loss: 0.6070560812950134\n",
            "Epoch [23], Batch [81], Train Loss: 0.6696146726608276\n",
            "Epoch [23], Batch [82], Train Loss: 0.6682304739952087\n",
            "Epoch [23], Batch [83], Train Loss: 0.7454527020454407\n",
            "Epoch [23], Batch [84], Train Loss: 0.765876054763794\n",
            "Epoch [23], Batch [85], Train Loss: 0.7873342633247375\n",
            "Epoch [23], Batch [86], Train Loss: 0.865172266960144\n",
            "Epoch [23], Batch [87], Train Loss: 0.6818027496337891\n",
            "Epoch [23], Batch [88], Train Loss: 0.7481069564819336\n",
            "Epoch [23], Batch [89], Train Loss: 0.6405565142631531\n",
            "Epoch [23], Batch [90], Train Loss: 0.8741321563720703\n",
            "Epoch [23], Batch [91], Train Loss: 0.8445988297462463\n",
            "Epoch [23], Batch [92], Train Loss: 0.7469495534896851\n",
            "Epoch [23], Batch [93], Train Loss: 0.696805477142334\n",
            "Epoch [23], Batch [94], Train Loss: 1.1516515016555786\n",
            "Epoch [23], Batch [95], Train Loss: 0.9700332283973694\n",
            "Epoch [23], Batch [96], Train Loss: 0.8669383525848389\n",
            "Epoch [23], Batch [97], Train Loss: 0.6116933822631836\n",
            "Epoch [23], Batch [98], Train Loss: 0.8415864706039429\n",
            "Epoch [23], Batch [99], Train Loss: 0.7183544635772705\n",
            "Epoch [23], Batch [100], Train Loss: 0.8998706340789795\n",
            "Epoch [23], Batch [101], Train Loss: 0.9105640649795532\n",
            "Epoch [23], Batch [102], Train Loss: 0.92467200756073\n",
            "Epoch [23], Batch [103], Train Loss: 0.7574211359024048\n",
            "Epoch [23], Batch [104], Train Loss: 1.0767219066619873\n",
            "Epoch [23], Batch [105], Train Loss: 0.8235933780670166\n",
            "Epoch [23], Batch [106], Train Loss: 0.6759050488471985\n",
            "Epoch [23], Batch [107], Train Loss: 0.8500567674636841\n",
            "Epoch [23], Batch [108], Train Loss: 0.8601912260055542\n",
            "Epoch [23], Batch [109], Train Loss: 0.8684688210487366\n",
            "Epoch [23], Batch [110], Train Loss: 0.8781585097312927\n",
            "Epoch [23], Batch [111], Train Loss: 0.9316490888595581\n",
            "Epoch [23], Batch [112], Train Loss: 0.7687628269195557\n",
            "Epoch [23], Batch [113], Train Loss: 0.7763946652412415\n",
            "Epoch [23], Batch [114], Train Loss: 0.7968857884407043\n",
            "Epoch [23], Batch [115], Train Loss: 0.8095961213111877\n",
            "Epoch [23], Batch [116], Train Loss: 0.8474376201629639\n",
            "Epoch [23], Batch [117], Train Loss: 0.8637503385543823\n",
            "Epoch [23], Batch [118], Train Loss: 0.785022497177124\n",
            "Epoch [23], Batch [119], Train Loss: 0.9622551202774048\n",
            "Epoch [23], Batch [120], Train Loss: 0.7879534363746643\n",
            "Epoch [23], Batch [121], Train Loss: 0.8959285616874695\n",
            "Epoch [23], Batch [122], Train Loss: 0.819529116153717\n",
            "Epoch [23], Batch [123], Train Loss: 0.8063743114471436\n",
            "Epoch [23], Batch [124], Train Loss: 0.7208773493766785\n",
            "Epoch [23], Batch [125], Train Loss: 0.6466035842895508\n",
            "Epoch [23], Batch [126], Train Loss: 0.7214239835739136\n",
            "Epoch [23], Batch [127], Train Loss: 0.8099446892738342\n",
            "Epoch [23], Batch [128], Train Loss: 0.9879717826843262\n",
            "Epoch [23], Batch [129], Train Loss: 0.8245344161987305\n",
            "Epoch [23], Batch [130], Train Loss: 0.8809449672698975\n",
            "Epoch [23], Batch [131], Train Loss: 0.800045907497406\n",
            "Epoch [23], Batch [132], Train Loss: 0.6936131715774536\n",
            "Epoch [23], Batch [133], Train Loss: 0.7332499623298645\n",
            "Epoch [23], Batch [134], Train Loss: 0.8551047444343567\n",
            "Epoch [23], Batch [135], Train Loss: 0.8429682850837708\n",
            "Epoch [23], Batch [136], Train Loss: 0.7736334204673767\n",
            "Epoch [23], Batch [137], Train Loss: 0.8975687026977539\n",
            "Epoch [23], Batch [138], Train Loss: 0.7717311978340149\n",
            "Epoch [23], Batch [139], Train Loss: 0.8284658789634705\n",
            "Epoch [23], Batch [140], Train Loss: 0.8624038696289062\n",
            "Epoch [23], Batch [141], Train Loss: 0.7036461234092712\n",
            "Epoch [23], Batch [142], Train Loss: 0.7322081327438354\n",
            "Epoch [23], Batch [143], Train Loss: 0.7206100225448608\n",
            "Epoch [23], Batch [144], Train Loss: 0.7757249474525452\n",
            "Epoch [23], Batch [145], Train Loss: 0.8984540700912476\n",
            "Epoch [23], Batch [146], Train Loss: 0.6019911766052246\n",
            "Epoch [23], Batch [147], Train Loss: 0.7338811755180359\n",
            "Epoch [23], Batch [148], Train Loss: 0.6266413927078247\n",
            "Epoch [23], Batch [149], Train Loss: 0.7176240086555481\n",
            "Epoch [23], Batch [150], Train Loss: 0.8439619541168213\n",
            "Epoch [23], Batch [151], Train Loss: 0.9122306704521179\n",
            "Epoch [23], Batch [152], Train Loss: 0.738158643245697\n",
            "Epoch [23], Batch [153], Train Loss: 0.6436731815338135\n",
            "Epoch [23], Batch [154], Train Loss: 0.8919270038604736\n",
            "Epoch [23], Batch [155], Train Loss: 0.8399676084518433\n",
            "Epoch [23], Batch [156], Train Loss: 0.8494156002998352\n",
            "Epoch [23], Batch [157], Train Loss: 0.640404999256134\n",
            "Epoch [23], Batch [158], Train Loss: 0.6223146319389343\n",
            "Epoch [23], Batch [159], Train Loss: 0.81877201795578\n",
            "Epoch [23], Batch [160], Train Loss: 1.0201765298843384\n",
            "Epoch [23], Batch [161], Train Loss: 0.8136196136474609\n",
            "Epoch [23], Batch [162], Train Loss: 0.7942166328430176\n",
            "Epoch [23], Batch [163], Train Loss: 0.755517303943634\n",
            "Epoch [23], Batch [164], Train Loss: 0.7609734535217285\n",
            "Epoch [23], Batch [165], Train Loss: 0.8269769549369812\n",
            "Epoch [23], Batch [166], Train Loss: 0.8232607245445251\n",
            "Epoch [23], Batch [167], Train Loss: 0.6129488348960876\n",
            "Epoch [23], Batch [168], Train Loss: 0.7829037308692932\n",
            "Epoch [23], Batch [169], Train Loss: 0.7199682593345642\n",
            "Epoch [23], Batch [170], Train Loss: 0.8026832938194275\n",
            "Epoch [23], Batch [171], Train Loss: 0.9222902059555054\n",
            "Epoch [23], Batch [172], Train Loss: 0.8344385027885437\n",
            "Epoch [23], Batch [173], Train Loss: 0.866037905216217\n",
            "Epoch [23], Batch [174], Train Loss: 0.8488454222679138\n",
            "Epoch [23], Batch [175], Train Loss: 0.7916115522384644\n",
            "Epoch [23], Batch [176], Train Loss: 0.7566763162612915\n",
            "Epoch [23], Batch [177], Train Loss: 0.7444758415222168\n",
            "Epoch [23], Batch [178], Train Loss: 1.0893710851669312\n",
            "Epoch [23], Batch [179], Train Loss: 0.7938286662101746\n",
            "Epoch [23], Batch [180], Train Loss: 1.3603010177612305\n",
            "Epoch [23], Batch [181], Train Loss: 0.8073346018791199\n",
            "Epoch [23], Batch [182], Train Loss: 0.8080192804336548\n",
            "Epoch [23], Batch [183], Train Loss: 0.7178828716278076\n",
            "Epoch [23], Batch [184], Train Loss: 0.71299147605896\n",
            "Epoch [23], Batch [185], Train Loss: 0.8273590207099915\n",
            "Epoch [23], Batch [186], Train Loss: 0.712092936038971\n",
            "Epoch [23], Batch [187], Train Loss: 0.8135957717895508\n",
            "Epoch [23], Batch [188], Train Loss: 0.5937023162841797\n",
            "Epoch [23], Batch [189], Train Loss: 0.8189780712127686\n",
            "Epoch [23], Batch [190], Train Loss: 0.6729564070701599\n",
            "Epoch [23], Batch [191], Train Loss: 0.9376846551895142\n",
            "Epoch [23], Batch [192], Train Loss: 0.702317476272583\n",
            "Epoch [23], Batch [193], Train Loss: 0.688665509223938\n",
            "Epoch [23], Batch [194], Train Loss: 0.8090405464172363\n",
            "Epoch [23], Batch [195], Train Loss: 0.9280252456665039\n",
            "Epoch [23], Batch [196], Train Loss: 0.7077302932739258\n",
            "Epoch [23], Batch [197], Train Loss: 0.8474017381668091\n",
            "Epoch [23], Batch [198], Train Loss: 0.8852244019508362\n",
            "Epoch [23], Batch [199], Train Loss: 0.7895625233650208\n",
            "Epoch [23], Batch [200], Train Loss: 0.7646825313568115\n",
            "Epoch [23], Batch [201], Train Loss: 0.7870516180992126\n",
            "Epoch [23], Batch [202], Train Loss: 0.6680067777633667\n",
            "Epoch [23], Batch [203], Train Loss: 0.6139671802520752\n",
            "Epoch [23], Batch [204], Train Loss: 0.8877646327018738\n",
            "Epoch [23], Batch [205], Train Loss: 0.7885271310806274\n",
            "Epoch [23], Batch [206], Train Loss: 0.852363109588623\n",
            "Epoch [23], Batch [207], Train Loss: 0.7013444900512695\n",
            "Epoch [23], Batch [208], Train Loss: 0.8349206447601318\n",
            "Epoch [23], Batch [209], Train Loss: 0.7522424459457397\n",
            "Epoch [23], Batch [210], Train Loss: 0.9083825349807739\n",
            "Epoch [23], Batch [211], Train Loss: 0.8032868504524231\n",
            "Epoch [23], Batch [212], Train Loss: 0.7295548319816589\n",
            "Epoch [23], Batch [213], Train Loss: 0.7742992043495178\n",
            "Epoch [23], Batch [214], Train Loss: 0.8646040558815002\n",
            "Epoch [23], Batch [215], Train Loss: 0.8332228660583496\n",
            "Epoch [23], Batch [216], Train Loss: 0.6746252179145813\n",
            "Epoch [23], Batch [217], Train Loss: 0.693833589553833\n",
            "Epoch [23], Batch [218], Train Loss: 0.7277679443359375\n",
            "Epoch [23], Batch [219], Train Loss: 1.039857029914856\n",
            "Epoch [23], Batch [220], Train Loss: 0.7848305106163025\n",
            "Epoch [23], Batch [221], Train Loss: 0.9067720174789429\n",
            "Epoch [23], Batch [222], Train Loss: 0.8657535314559937\n",
            "Epoch [23], Batch [223], Train Loss: 0.9247782826423645\n",
            "Epoch [23], Batch [224], Train Loss: 0.7094200253486633\n",
            "Epoch [23], Batch [225], Train Loss: 0.8722407221794128\n",
            "Epoch [23], Batch [226], Train Loss: 0.7247639298439026\n",
            "Epoch [23], Batch [227], Train Loss: 0.6553983688354492\n",
            "Epoch [23], Batch [228], Train Loss: 0.810707151889801\n",
            "Epoch [23], Batch [229], Train Loss: 0.9394112825393677\n",
            "Epoch [23], Batch [230], Train Loss: 0.8529993891716003\n",
            "Epoch [23], Batch [231], Train Loss: 0.774083137512207\n",
            "Epoch [23], Batch [232], Train Loss: 0.9205626845359802\n",
            "Epoch [23], Batch [233], Train Loss: 0.7495959401130676\n",
            "Epoch [23], Batch [234], Train Loss: 0.7814440727233887\n",
            "Epoch [23], Batch [235], Train Loss: 0.8134386539459229\n",
            "Epoch [23], Batch [236], Train Loss: 0.8290172219276428\n",
            "Epoch [23], Batch [237], Train Loss: 0.6238436102867126\n",
            "Epoch [23], Batch [238], Train Loss: 0.8545377254486084\n",
            "Epoch [23], Batch [239], Train Loss: 0.9390720129013062\n",
            "Epoch [23], Batch [240], Train Loss: 0.6949285268783569\n",
            "Epoch [23], Batch [241], Train Loss: 0.8110828399658203\n",
            "Epoch [23], Batch [242], Train Loss: 0.712437093257904\n",
            "Epoch [23], Batch [243], Train Loss: 0.7751730680465698\n",
            "Epoch [23], Batch [244], Train Loss: 0.8796802163124084\n",
            "Epoch [23], Batch [245], Train Loss: 0.674059271812439\n",
            "Epoch [23], Batch [246], Train Loss: 0.6914616227149963\n",
            "Epoch [23], Batch [247], Train Loss: 0.7709879875183105\n",
            "Epoch [23], Batch [248], Train Loss: 0.7188703417778015\n",
            "Epoch [23], Batch [249], Train Loss: 0.8092287182807922\n",
            "Epoch [23], Batch [250], Train Loss: 0.8472428917884827\n",
            "Epoch [23], Batch [251], Train Loss: 0.8380439877510071\n",
            "Epoch [23], Batch [252], Train Loss: 0.8060811758041382\n",
            "Epoch [23], Batch [253], Train Loss: 0.8933852314949036\n",
            "Epoch [23], Batch [254], Train Loss: 0.7688881158828735\n",
            "Epoch [23], Batch [255], Train Loss: 0.7969814538955688\n",
            "Epoch [23], Batch [256], Train Loss: 0.8081294894218445\n",
            "Epoch [23], Batch [257], Train Loss: 0.7801528573036194\n",
            "Epoch [23], Batch [258], Train Loss: 0.7642744779586792\n",
            "Epoch [23], Batch [259], Train Loss: 0.7717937231063843\n",
            "Epoch [23], Batch [260], Train Loss: 0.9010019898414612\n",
            "Epoch [23], Batch [261], Train Loss: 0.8648726940155029\n",
            "Epoch [23], Batch [262], Train Loss: 1.0048248767852783\n",
            "Epoch [23], Batch [263], Train Loss: 0.7383038997650146\n",
            "Epoch [23], Batch [264], Train Loss: 0.7978726625442505\n",
            "Epoch [23], Batch [265], Train Loss: 0.7287923693656921\n",
            "Epoch [23], Batch [266], Train Loss: 0.814636766910553\n",
            "Epoch [23], Batch [267], Train Loss: 0.8668259382247925\n",
            "Epoch [23], Batch [268], Train Loss: 0.5913297533988953\n",
            "Epoch [23], Batch [269], Train Loss: 0.7790079116821289\n",
            "Epoch [23], Batch [270], Train Loss: 0.7939571142196655\n",
            "Epoch [23], Batch [271], Train Loss: 0.6754544973373413\n",
            "Epoch [23], Batch [272], Train Loss: 0.9348099827766418\n",
            "Epoch [23], Batch [273], Train Loss: 0.8914292454719543\n",
            "Epoch [23], Batch [274], Train Loss: 0.7754788398742676\n",
            "Epoch [23], Batch [275], Train Loss: 0.8048504590988159\n",
            "Epoch [23], Batch [276], Train Loss: 0.8608409762382507\n",
            "Epoch [23], Batch [277], Train Loss: 0.9295375347137451\n",
            "Epoch [23], Batch [278], Train Loss: 0.8554530143737793\n",
            "Epoch [23], Batch [279], Train Loss: 0.7749798893928528\n",
            "Epoch [23], Batch [280], Train Loss: 0.9078670740127563\n",
            "Epoch [23], Batch [281], Train Loss: 0.7687784433364868\n",
            "Epoch [23], Batch [282], Train Loss: 0.7083864212036133\n",
            "Epoch [23], Batch [283], Train Loss: 0.7803248167037964\n",
            "Epoch [23], Batch [284], Train Loss: 0.6297115683555603\n",
            "Epoch [23], Batch [285], Train Loss: 0.6664218306541443\n",
            "Epoch [23], Batch [286], Train Loss: 0.7635387182235718\n",
            "Epoch [23], Batch [287], Train Loss: 0.6397786736488342\n",
            "Epoch [23], Batch [288], Train Loss: 1.1529285907745361\n",
            "Epoch [23], Batch [289], Train Loss: 0.8192113041877747\n",
            "Epoch [23], Batch [290], Train Loss: 0.883792519569397\n",
            "Epoch [23], Batch [291], Train Loss: 0.7417253255844116\n",
            "Epoch [23], Batch [292], Train Loss: 0.615416407585144\n",
            "Epoch [23], Batch [293], Train Loss: 0.7612790465354919\n",
            "Epoch [23], Batch [294], Train Loss: 0.7035248279571533\n",
            "Epoch [23], Batch [295], Train Loss: 1.372096300125122\n",
            "Epoch [23], Batch [296], Train Loss: 0.9170066118240356\n",
            "Epoch [23], Batch [297], Train Loss: 0.6617927551269531\n",
            "Epoch [23], Batch [298], Train Loss: 0.8083272576332092\n",
            "Epoch [23], Batch [299], Train Loss: 0.8481277227401733\n",
            "Epoch [23], Batch [300], Train Loss: 0.7090903520584106\n",
            "Epoch [23], Batch [301], Train Loss: 0.9267491102218628\n",
            "Epoch [23], Batch [302], Train Loss: 0.7651000022888184\n",
            "Epoch [23], Batch [303], Train Loss: 0.6157791614532471\n",
            "Epoch [23], Batch [304], Train Loss: 0.6897557973861694\n",
            "Epoch [23], Batch [305], Train Loss: 0.766574501991272\n",
            "Epoch [23], Batch [306], Train Loss: 0.7624369859695435\n",
            "Epoch [23], Batch [307], Train Loss: 0.7823507189750671\n",
            "Epoch [23], Batch [308], Train Loss: 0.8104710578918457\n",
            "Epoch [23], Batch [309], Train Loss: 0.8047129511833191\n",
            "Epoch [23], Batch [310], Train Loss: 0.6094915270805359\n",
            "Epoch [23], Batch [311], Train Loss: 0.6852558851242065\n",
            "Epoch [23], Batch [312], Train Loss: 0.7660989165306091\n",
            "Epoch [23], Batch [313], Train Loss: 0.7165515422821045\n",
            "Epoch [23], Batch [314], Train Loss: 0.6365299224853516\n",
            "Epoch [23], Batch [315], Train Loss: 0.8274661302566528\n",
            "Epoch [23], Batch [316], Train Loss: 0.819442868232727\n",
            "Epoch [23], Batch [317], Train Loss: 0.8636886477470398\n",
            "Epoch [23], Batch [318], Train Loss: 0.7595655918121338\n",
            "Epoch [23], Batch [319], Train Loss: 0.7104623317718506\n",
            "Epoch [23], Batch [320], Train Loss: 0.9729679822921753\n",
            "Epoch [23], Batch [321], Train Loss: 0.6645460724830627\n",
            "Epoch [23], Batch [322], Train Loss: 0.5921732187271118\n",
            "Epoch [23], Batch [323], Train Loss: 0.6402041912078857\n",
            "Epoch [23], Batch [324], Train Loss: 0.6537203788757324\n",
            "Epoch [23], Batch [325], Train Loss: 0.7225965857505798\n",
            "Epoch [23], Batch [326], Train Loss: 0.7614091634750366\n",
            "Epoch [23], Batch [327], Train Loss: 0.7734943628311157\n",
            "Epoch [23], Batch [328], Train Loss: 0.839434802532196\n",
            "Epoch [23], Batch [329], Train Loss: 0.6824386119842529\n",
            "Epoch [23], Batch [330], Train Loss: 0.7406763434410095\n",
            "Epoch [23], Batch [331], Train Loss: 0.7422906756401062\n",
            "Epoch [23], Batch [332], Train Loss: 0.8684147596359253\n",
            "Epoch [23], Batch [333], Train Loss: 0.6984484195709229\n",
            "Epoch [23], Batch [334], Train Loss: 0.6352857947349548\n",
            "Epoch [23], Batch [335], Train Loss: 0.8293024301528931\n",
            "Epoch [23], Batch [336], Train Loss: 1.0066978931427002\n",
            "Epoch [23], Batch [337], Train Loss: 0.8110677003860474\n",
            "Epoch [23], Batch [338], Train Loss: 0.9627999663352966\n",
            "Epoch [23], Batch [339], Train Loss: 0.7165894508361816\n",
            "Epoch [23], Batch [340], Train Loss: 0.7508068084716797\n",
            "Epoch [23], Batch [341], Train Loss: 0.7805860042572021\n",
            "Epoch [23], Batch [342], Train Loss: 0.6595650911331177\n",
            "Epoch [23], Batch [343], Train Loss: 0.8205864429473877\n",
            "Epoch [23], Batch [344], Train Loss: 0.9011844396591187\n",
            "Epoch [23], Batch [345], Train Loss: 0.8983980417251587\n",
            "Epoch [23], Batch [346], Train Loss: 0.7014091610908508\n",
            "Epoch [23], Batch [347], Train Loss: 0.9168095588684082\n",
            "Epoch [23], Batch [348], Train Loss: 0.8772928714752197\n",
            "Epoch [23], Batch [349], Train Loss: 0.9224943518638611\n",
            "Epoch [23], Batch [350], Train Loss: 0.7557599544525146\n",
            "Epoch [23], Batch [351], Train Loss: 0.8744456768035889\n",
            "Epoch [23], Batch [352], Train Loss: 0.9190869927406311\n",
            "Epoch [23], Batch [353], Train Loss: 0.8300716876983643\n",
            "Epoch [23], Batch [354], Train Loss: 0.5804694890975952\n",
            "Epoch [23], Batch [355], Train Loss: 0.7422953844070435\n",
            "Epoch [23], Batch [356], Train Loss: 0.7354655861854553\n",
            "Epoch [23], Batch [357], Train Loss: 0.8359686136245728\n",
            "Epoch [23], Batch [358], Train Loss: 0.7344887256622314\n",
            "Epoch [23], Batch [359], Train Loss: 0.9034786224365234\n",
            "Epoch [23], Batch [360], Train Loss: 0.7991868257522583\n",
            "Epoch [23], Batch [361], Train Loss: 0.7320770621299744\n",
            "Epoch [23], Batch [362], Train Loss: 1.0231982469558716\n",
            "Epoch [23], Batch [363], Train Loss: 0.8476930260658264\n",
            "Epoch [23], Batch [364], Train Loss: 0.7867974042892456\n",
            "Epoch [23], Batch [365], Train Loss: 0.7635618448257446\n",
            "Epoch [23], Batch [366], Train Loss: 0.9231284260749817\n",
            "Epoch [23], Batch [367], Train Loss: 0.8642104864120483\n",
            "Epoch [23], Batch [368], Train Loss: 1.0605026483535767\n",
            "Epoch [23], Batch [369], Train Loss: 0.7654789686203003\n",
            "Epoch [23], Batch [370], Train Loss: 0.6906190514564514\n",
            "Epoch [23], Batch [371], Train Loss: 0.6469051241874695\n",
            "Epoch [23], Batch [372], Train Loss: 0.6941763162612915\n",
            "Epoch [23], Batch [373], Train Loss: 0.8536429405212402\n",
            "Epoch [23], Batch [374], Train Loss: 0.7679507732391357\n",
            "Epoch [23], Batch [375], Train Loss: 0.838584303855896\n",
            "Epoch [23], Batch [376], Train Loss: 0.7660775780677795\n",
            "Epoch [23], Batch [377], Train Loss: 0.7360896468162537\n",
            "Epoch [23], Batch [378], Train Loss: 0.847437858581543\n",
            "Epoch [23], Batch [379], Train Loss: 0.7607340812683105\n",
            "Epoch [23], Batch [380], Train Loss: 0.7001906037330627\n",
            "Epoch [23], Batch [381], Train Loss: 0.670915424823761\n",
            "Epoch [23], Batch [382], Train Loss: 0.8347105383872986\n",
            "Epoch [23], Batch [383], Train Loss: 0.6760312914848328\n",
            "Epoch [23], Batch [384], Train Loss: 0.7909250855445862\n",
            "Epoch [23], Batch [385], Train Loss: 0.7681615352630615\n",
            "Epoch [23], Batch [386], Train Loss: 0.8512535095214844\n",
            "Epoch [23], Batch [387], Train Loss: 0.6418606638908386\n",
            "Epoch [23], Batch [388], Train Loss: 0.8025718927383423\n",
            "Epoch [23], Batch [389], Train Loss: 0.7507673501968384\n",
            "Epoch [23], Batch [390], Train Loss: 0.7590605020523071\n",
            "Epoch [23], Batch [391], Train Loss: 0.7211351990699768\n",
            "Epoch [23], Batch [392], Train Loss: 0.7375307083129883\n",
            "Epoch [23], Batch [393], Train Loss: 0.7971742749214172\n",
            "Epoch [23], Batch [394], Train Loss: 1.0111817121505737\n",
            "Epoch [23], Batch [395], Train Loss: 0.6633626818656921\n",
            "Epoch [23], Batch [396], Train Loss: 0.8458207845687866\n",
            "Epoch [23], Batch [397], Train Loss: 0.9476988315582275\n",
            "Epoch [23], Batch [398], Train Loss: 0.5838499665260315\n",
            "Epoch [23], Batch [399], Train Loss: 0.7429060339927673\n",
            "Epoch [23], Batch [400], Train Loss: 0.607070803642273\n",
            "Epoch [23], Batch [401], Train Loss: 0.5894417762756348\n",
            "Epoch [23], Batch [402], Train Loss: 0.7038962841033936\n",
            "Epoch [23], Batch [403], Train Loss: 0.6707767248153687\n",
            "Epoch [23], Batch [404], Train Loss: 0.8559274077415466\n",
            "Epoch [23], Batch [405], Train Loss: 0.6712125539779663\n",
            "Epoch [23], Batch [406], Train Loss: 1.0725371837615967\n",
            "Epoch [23], Batch [407], Train Loss: 0.6753049492835999\n",
            "Epoch [23], Batch [408], Train Loss: 0.8878626227378845\n",
            "Epoch [23], Batch [409], Train Loss: 0.7394543886184692\n",
            "Epoch [23], Batch [410], Train Loss: 0.7877994775772095\n",
            "Epoch [23], Batch [411], Train Loss: 0.7043890953063965\n",
            "Epoch [23], Batch [412], Train Loss: 0.688808798789978\n",
            "Epoch [23], Batch [413], Train Loss: 0.6904810070991516\n",
            "Epoch [23], Batch [414], Train Loss: 0.8642083406448364\n",
            "Epoch [23], Batch [415], Train Loss: 0.6125549077987671\n",
            "Epoch [23], Batch [416], Train Loss: 0.8803095817565918\n",
            "Epoch [23], Batch [417], Train Loss: 0.862220048904419\n",
            "Epoch [23], Batch [418], Train Loss: 0.8755350708961487\n",
            "Epoch [23], Batch [419], Train Loss: 0.5900629758834839\n",
            "Epoch [23], Batch [420], Train Loss: 0.7859995365142822\n",
            "Epoch [23], Batch [421], Train Loss: 0.8498582243919373\n",
            "Epoch [23], Batch [422], Train Loss: 0.753489077091217\n",
            "Epoch [23], Batch [423], Train Loss: 0.7312291860580444\n",
            "Epoch [23], Batch [424], Train Loss: 0.9001080989837646\n",
            "Epoch [23], Batch [425], Train Loss: 1.12169349193573\n",
            "Epoch [23], Batch [426], Train Loss: 0.8399073481559753\n",
            "Epoch [23], Batch [427], Train Loss: 0.7578381896018982\n",
            "Epoch [23], Batch [428], Train Loss: 0.7835322618484497\n",
            "Epoch [23], Batch [429], Train Loss: 0.7900518178939819\n",
            "Epoch [23], Batch [430], Train Loss: 1.050655722618103\n",
            "Epoch [23], Batch [431], Train Loss: 0.8596920371055603\n",
            "Epoch [23], Batch [432], Train Loss: 0.8750607371330261\n",
            "Epoch [23], Batch [433], Train Loss: 0.9447779059410095\n",
            "Epoch [23], Batch [434], Train Loss: 0.9072220325469971\n",
            "Epoch [23], Batch [435], Train Loss: 0.8413418531417847\n",
            "Epoch [23], Batch [436], Train Loss: 0.772386372089386\n",
            "Epoch [23], Batch [437], Train Loss: 0.6832272410392761\n",
            "Epoch [23], Batch [438], Train Loss: 1.08267343044281\n",
            "Epoch [23], Batch [439], Train Loss: 0.7938043475151062\n",
            "Epoch [23], Batch [440], Train Loss: 0.8974176645278931\n",
            "Epoch [23], Batch [441], Train Loss: 0.7428889274597168\n",
            "Epoch [23], Batch [442], Train Loss: 0.8060486316680908\n",
            "Epoch [23], Batch [443], Train Loss: 0.8187437057495117\n",
            "Epoch [23], Batch [444], Train Loss: 0.9704761505126953\n",
            "Epoch [23], Batch [445], Train Loss: 0.871870219707489\n",
            "Epoch [23], Batch [446], Train Loss: 0.8718069791793823\n",
            "Epoch [23], Batch [447], Train Loss: 0.8183629512786865\n",
            "Epoch [23], Batch [448], Train Loss: 0.681439995765686\n",
            "Epoch [23], Batch [449], Train Loss: 0.8564020991325378\n",
            "Epoch [23], Batch [450], Train Loss: 0.772034764289856\n",
            "Epoch [23], Batch [451], Train Loss: 0.7219104170799255\n",
            "Epoch [23], Batch [452], Train Loss: 0.9455280900001526\n",
            "Epoch [23], Batch [453], Train Loss: 0.698540449142456\n",
            "Epoch [23], Batch [454], Train Loss: 0.6654911041259766\n",
            "Epoch [23], Batch [455], Train Loss: 0.8662710785865784\n",
            "Epoch [23], Batch [456], Train Loss: 0.7133721113204956\n",
            "Epoch [23], Batch [457], Train Loss: 0.7440844178199768\n",
            "Epoch [23], Batch [458], Train Loss: 0.79063481092453\n",
            "Epoch [23], Batch [459], Train Loss: 0.7738217115402222\n",
            "Epoch [23], Batch [460], Train Loss: 0.9367369413375854\n",
            "Epoch [23], Batch [461], Train Loss: 0.7814383506774902\n",
            "Epoch [23], Batch [462], Train Loss: 0.7345691919326782\n",
            "Epoch [23], Batch [463], Train Loss: 0.7638142108917236\n",
            "Epoch [23], Batch [464], Train Loss: 0.9220727682113647\n",
            "Epoch [23], Batch [465], Train Loss: 0.8679331541061401\n",
            "Epoch [23], Batch [466], Train Loss: 0.7494463324546814\n",
            "Epoch [23], Batch [467], Train Loss: 0.8094750046730042\n",
            "Epoch [23], Batch [468], Train Loss: 0.8510872721672058\n",
            "Epoch [23], Batch [469], Train Loss: 0.9146494269371033\n",
            "Epoch [23], Batch [470], Train Loss: 0.542457640171051\n",
            "Epoch [23], Batch [471], Train Loss: 0.671237051486969\n",
            "Epoch [23], Batch [472], Train Loss: 0.9017311334609985\n",
            "Epoch [23], Batch [473], Train Loss: 0.9542292356491089\n",
            "Epoch [23], Batch [474], Train Loss: 0.8800796866416931\n",
            "Epoch [23], Batch [475], Train Loss: 0.8858047127723694\n",
            "Epoch [23], Batch [476], Train Loss: 0.9590011835098267\n",
            "Epoch [23], Batch [477], Train Loss: 0.7326514720916748\n",
            "Epoch [23], Batch [478], Train Loss: 0.7753888964653015\n",
            "Epoch [23], Batch [479], Train Loss: 0.6784349679946899\n",
            "Epoch [23], Batch [480], Train Loss: 0.7624716758728027\n",
            "Epoch [23], Batch [481], Train Loss: 0.7723188996315002\n",
            "Epoch [23], Batch [482], Train Loss: 0.9082695841789246\n",
            "Epoch [23], Batch [483], Train Loss: 0.8174496293067932\n",
            "Epoch [23], Batch [484], Train Loss: 0.8079859018325806\n",
            "Epoch [23], Batch [485], Train Loss: 0.9353489875793457\n",
            "Epoch [23], Batch [486], Train Loss: 0.6666265726089478\n",
            "Epoch [23], Batch [487], Train Loss: 0.5786418914794922\n",
            "Epoch [23], Batch [488], Train Loss: 1.0043699741363525\n",
            "Epoch [23], Batch [489], Train Loss: 0.7114143371582031\n",
            "Epoch [23], Batch [490], Train Loss: 0.6293994784355164\n",
            "Epoch [23], Batch [491], Train Loss: 0.6256378889083862\n",
            "Epoch [23], Batch [492], Train Loss: 1.0269464254379272\n",
            "Epoch [23], Batch [493], Train Loss: 0.8902136087417603\n",
            "Epoch [23], Batch [494], Train Loss: 0.7853211164474487\n",
            "Epoch [23], Batch [495], Train Loss: 0.9319298267364502\n",
            "Epoch [23], Batch [496], Train Loss: 0.7674190402030945\n",
            "Epoch [23], Batch [497], Train Loss: 0.6750319004058838\n",
            "Epoch [23], Batch [498], Train Loss: 0.8499957919120789\n",
            "Epoch [23], Batch [499], Train Loss: 0.9213125705718994\n",
            "Epoch [23], Batch [500], Train Loss: 1.0101970434188843\n",
            "Epoch [23], Batch [501], Train Loss: 0.6363540887832642\n",
            "Epoch [23], Batch [502], Train Loss: 0.9015088677406311\n",
            "Epoch [23], Batch [503], Train Loss: 0.8634675145149231\n",
            "Epoch [23], Batch [504], Train Loss: 0.7833385467529297\n",
            "Epoch [23], Batch [505], Train Loss: 0.7657525539398193\n",
            "Epoch [23], Batch [506], Train Loss: 0.8650959134101868\n",
            "Epoch [23], Batch [507], Train Loss: 0.9265310168266296\n",
            "Epoch [23], Batch [508], Train Loss: 0.8722975850105286\n",
            "Epoch [23], Batch [509], Train Loss: 0.9398108720779419\n",
            "Epoch [23], Batch [510], Train Loss: 0.6463401913642883\n",
            "Epoch [23], Batch [511], Train Loss: 0.760876476764679\n",
            "Epoch [23], Batch [512], Train Loss: 1.0803428888320923\n",
            "Epoch [23], Batch [513], Train Loss: 0.6871742010116577\n",
            "Epoch [23], Batch [514], Train Loss: 0.8304058909416199\n",
            "Epoch [23], Batch [515], Train Loss: 0.7821328043937683\n",
            "Epoch [23], Batch [516], Train Loss: 0.7590033411979675\n",
            "Epoch [23], Batch [517], Train Loss: 0.8953977823257446\n",
            "Epoch [23], Batch [518], Train Loss: 0.7095303535461426\n",
            "Epoch [23], Batch [519], Train Loss: 0.8181334733963013\n",
            "Epoch [23], Batch [520], Train Loss: 0.6017969846725464\n",
            "Epoch [23], Batch [521], Train Loss: 0.9753485918045044\n",
            "Epoch [23], Batch [522], Train Loss: 1.009226679801941\n",
            "Epoch [23], Batch [523], Train Loss: 0.7674669027328491\n",
            "Epoch [23], Batch [524], Train Loss: 0.7732252478599548\n",
            "Epoch [23], Batch [525], Train Loss: 0.8450131416320801\n",
            "Epoch [23], Batch [526], Train Loss: 0.8205831050872803\n",
            "Epoch [23], Batch [527], Train Loss: 0.8818832635879517\n",
            "Epoch [23], Batch [528], Train Loss: 0.8512900471687317\n",
            "Epoch [23], Batch [529], Train Loss: 0.801874577999115\n",
            "Epoch [23], Batch [530], Train Loss: 0.6691550016403198\n",
            "Epoch [23], Batch [531], Train Loss: 0.8014388084411621\n",
            "Epoch [23], Batch [532], Train Loss: 0.735273003578186\n",
            "Epoch [23], Batch [533], Train Loss: 0.7164288759231567\n",
            "Epoch [23], Batch [534], Train Loss: 0.5960593223571777\n",
            "Epoch [23], Batch [535], Train Loss: 0.864255428314209\n",
            "Epoch [23], Batch [536], Train Loss: 0.9372677206993103\n",
            "Epoch [23], Batch [537], Train Loss: 0.8854500651359558\n",
            "Epoch [23], Batch [538], Train Loss: 0.6710421442985535\n",
            "Epoch [23], Batch [539], Train Loss: 0.8508649468421936\n",
            "Epoch [23], Batch [540], Train Loss: 0.7097715735435486\n",
            "Epoch [23], Batch [541], Train Loss: 0.8229354619979858\n",
            "Epoch [23], Batch [542], Train Loss: 0.8664109706878662\n",
            "Epoch [23], Batch [543], Train Loss: 0.8103981018066406\n",
            "Epoch [23], Batch [544], Train Loss: 0.8729042410850525\n",
            "Epoch [23], Batch [545], Train Loss: 1.0573657751083374\n",
            "Epoch [23], Batch [546], Train Loss: 0.7618672251701355\n",
            "Epoch [23], Batch [547], Train Loss: 0.7662139534950256\n",
            "Epoch [23], Batch [548], Train Loss: 1.1257822513580322\n",
            "Epoch [23], Batch [549], Train Loss: 0.8097249269485474\n",
            "Epoch [23], Batch [550], Train Loss: 0.6959197521209717\n",
            "Epoch [23], Batch [551], Train Loss: 0.6011070609092712\n",
            "Epoch [23], Batch [552], Train Loss: 0.7766554355621338\n",
            "Epoch [23], Batch [553], Train Loss: 0.6319855451583862\n",
            "Epoch [23], Batch [554], Train Loss: 0.8206804990768433\n",
            "Epoch [23], Batch [555], Train Loss: 1.213321328163147\n",
            "Epoch [23], Batch [556], Train Loss: 0.9515416622161865\n",
            "Epoch [23], Batch [557], Train Loss: 0.9559708833694458\n",
            "Epoch [23], Batch [558], Train Loss: 0.6388477683067322\n",
            "Epoch [23], Batch [559], Train Loss: 1.0046008825302124\n",
            "Epoch [23], Batch [560], Train Loss: 0.8481865525245667\n",
            "Epoch [23], Batch [561], Train Loss: 0.887639582157135\n",
            "Epoch [23], Batch [562], Train Loss: 0.9595469832420349\n",
            "Epoch [23], Batch [563], Train Loss: 0.705141544342041\n",
            "Epoch [23], Batch [564], Train Loss: 0.6971946358680725\n",
            "Epoch [23], Batch [565], Train Loss: 0.6997631192207336\n",
            "Epoch [23], Batch [566], Train Loss: 0.9024271965026855\n",
            "Epoch [23], Batch [567], Train Loss: 0.6524055600166321\n",
            "Epoch [23], Batch [568], Train Loss: 0.7675089836120605\n",
            "Epoch [23], Batch [569], Train Loss: 0.9858225584030151\n",
            "Epoch [23], Batch [570], Train Loss: 0.7439044117927551\n",
            "Epoch [23], Batch [571], Train Loss: 0.714106559753418\n",
            "Epoch [23], Batch [572], Train Loss: 0.7838983535766602\n",
            "Epoch [23], Batch [573], Train Loss: 0.7974882125854492\n",
            "Epoch [23], Batch [574], Train Loss: 0.8185608386993408\n",
            "Epoch [23], Batch [575], Train Loss: 0.752714991569519\n",
            "Epoch [23], Batch [576], Train Loss: 0.7490189075469971\n",
            "Epoch [23], Batch [577], Train Loss: 0.6891831755638123\n",
            "Epoch [23], Batch [578], Train Loss: 0.6550169587135315\n",
            "Epoch [23], Batch [579], Train Loss: 0.6510778665542603\n",
            "Epoch [23], Batch [580], Train Loss: 0.6424989700317383\n",
            "Epoch [23], Batch [581], Train Loss: 0.7392936944961548\n",
            "Epoch [23], Batch [582], Train Loss: 0.6450591683387756\n",
            "Epoch [23], Batch [583], Train Loss: 0.8315696120262146\n",
            "Epoch [23], Batch [584], Train Loss: 0.729077935218811\n",
            "Epoch [23], Batch [585], Train Loss: 0.8818193674087524\n",
            "Epoch [23], Batch [586], Train Loss: 0.8395397663116455\n",
            "Epoch [23], Batch [587], Train Loss: 0.7056692242622375\n",
            "Epoch [23], Batch [588], Train Loss: 0.8786534667015076\n",
            "Epoch [23], Batch [589], Train Loss: 0.8152844905853271\n",
            "Epoch [23], Batch [590], Train Loss: 0.7625604271888733\n",
            "Epoch [23], Batch [591], Train Loss: 0.8571258783340454\n",
            "Epoch [23], Batch [592], Train Loss: 0.6688250303268433\n",
            "Epoch [23], Batch [593], Train Loss: 0.9937223792076111\n",
            "Epoch [23], Batch [594], Train Loss: 0.8824234008789062\n",
            "Epoch [23], Batch [595], Train Loss: 0.8369244337081909\n",
            "Epoch [23], Batch [596], Train Loss: 0.7167472839355469\n",
            "Epoch [23], Batch [597], Train Loss: 0.5675482153892517\n",
            "Epoch [23], Batch [598], Train Loss: 0.749779224395752\n",
            "Epoch [23], Batch [599], Train Loss: 0.9790334105491638\n",
            "Epoch [23], Batch [600], Train Loss: 0.9111552834510803\n",
            "Epoch [23], Batch [601], Train Loss: 0.6653122305870056\n",
            "Epoch [23], Batch [602], Train Loss: 0.5853450298309326\n",
            "Epoch [23], Batch [603], Train Loss: 0.7693198919296265\n",
            "Epoch [23], Batch [604], Train Loss: 0.7178622484207153\n",
            "Epoch [23], Batch [605], Train Loss: 0.9189770221710205\n",
            "Epoch [23], Batch [606], Train Loss: 0.7835751175880432\n",
            "Epoch [23], Batch [607], Train Loss: 0.8165239095687866\n",
            "Epoch [23], Batch [608], Train Loss: 1.2957926988601685\n",
            "Epoch [23], Batch [609], Train Loss: 0.8862050771713257\n",
            "Epoch [23], Batch [610], Train Loss: 0.8643693923950195\n",
            "Epoch [23], Batch [611], Train Loss: 0.5750749111175537\n",
            "Epoch [23], Batch [612], Train Loss: 0.5939126014709473\n",
            "Epoch [23], Batch [613], Train Loss: 0.7952249050140381\n",
            "Epoch [23], Batch [614], Train Loss: 0.7859179973602295\n",
            "Epoch [23], Batch [615], Train Loss: 0.578031063079834\n",
            "Epoch [23], Batch [616], Train Loss: 0.7670927047729492\n",
            "Epoch [23], Batch [617], Train Loss: 0.780347466468811\n",
            "Epoch [23], Batch [618], Train Loss: 0.7250333428382874\n",
            "Epoch [23], Batch [619], Train Loss: 0.7184999585151672\n",
            "Epoch [23], Batch [620], Train Loss: 0.7450653910636902\n",
            "Epoch [23], Batch [621], Train Loss: 0.7503310441970825\n",
            "Epoch [23], Batch [622], Train Loss: 0.8990097641944885\n",
            "Epoch [23], Batch [623], Train Loss: 0.7282019257545471\n",
            "Epoch [23], Batch [624], Train Loss: 0.562969982624054\n",
            "Epoch [23], Batch [625], Train Loss: 0.6001123785972595\n",
            "Epoch [23], Batch [626], Train Loss: 0.5664912462234497\n",
            "Epoch [23], Batch [627], Train Loss: 0.7804968357086182\n",
            "Epoch [23], Batch [628], Train Loss: 0.7635973691940308\n",
            "Epoch [23], Batch [629], Train Loss: 0.8196800947189331\n",
            "Epoch [23], Batch [630], Train Loss: 0.798280656337738\n",
            "Epoch [23], Batch [631], Train Loss: 0.7534071803092957\n",
            "Epoch [23], Batch [632], Train Loss: 0.8110038042068481\n",
            "Epoch [23], Batch [633], Train Loss: 0.6980347633361816\n",
            "Epoch [23], Batch [634], Train Loss: 0.68907630443573\n",
            "Epoch [23], Batch [635], Train Loss: 0.6505410671234131\n",
            "Epoch [23], Batch [636], Train Loss: 1.046201467514038\n",
            "Epoch [23], Batch [637], Train Loss: 0.6882376670837402\n",
            "Epoch [23], Batch [638], Train Loss: 0.805496335029602\n",
            "Epoch [23], Batch [639], Train Loss: 0.666156530380249\n",
            "Epoch [23], Batch [640], Train Loss: 1.0574408769607544\n",
            "Epoch [23], Batch [641], Train Loss: 0.7043794393539429\n",
            "Epoch [23], Batch [642], Train Loss: 0.6892595291137695\n",
            "Epoch [23], Batch [643], Train Loss: 0.9182637333869934\n",
            "Epoch [23], Batch [644], Train Loss: 0.6255656480789185\n",
            "Epoch [23], Batch [645], Train Loss: 0.7606245279312134\n",
            "Epoch [23], Batch [646], Train Loss: 0.6927013993263245\n",
            "Epoch [23], Batch [647], Train Loss: 0.6499269604682922\n",
            "Epoch [23], Batch [648], Train Loss: 1.1238956451416016\n",
            "Epoch [23], Batch [649], Train Loss: 0.9845839738845825\n",
            "Epoch [23], Batch [650], Train Loss: 0.7126358151435852\n",
            "Epoch [23], Batch [651], Train Loss: 0.906604528427124\n",
            "Epoch [23], Batch [652], Train Loss: 0.829601526260376\n",
            "Epoch [23], Batch [653], Train Loss: 0.6458301544189453\n",
            "Epoch [23], Batch [654], Train Loss: 0.798559308052063\n",
            "Epoch [23], Batch [655], Train Loss: 0.8658426403999329\n",
            "Epoch [23], Batch [656], Train Loss: 0.7678054571151733\n",
            "Epoch [23], Batch [657], Train Loss: 0.8001847863197327\n",
            "Epoch [23], Batch [658], Train Loss: 0.896747887134552\n",
            "Epoch [23], Batch [659], Train Loss: 0.7801029682159424\n",
            "Epoch [23], Batch [660], Train Loss: 0.7902088165283203\n",
            "Epoch [23], Batch [661], Train Loss: 0.6266539692878723\n",
            "Epoch [23], Batch [662], Train Loss: 0.8014439344406128\n",
            "Epoch [23], Batch [663], Train Loss: 0.7120487689971924\n",
            "Epoch [23], Batch [664], Train Loss: 0.5984634160995483\n",
            "Epoch [23], Batch [665], Train Loss: 0.7211529016494751\n",
            "Epoch [23], Batch [666], Train Loss: 0.9297648072242737\n",
            "Epoch [23], Batch [667], Train Loss: 0.8209431171417236\n",
            "Epoch [23], Batch [668], Train Loss: 0.7512794733047485\n",
            "Epoch [23], Batch [669], Train Loss: 0.8057432174682617\n",
            "Epoch [23], Batch [670], Train Loss: 0.8270924091339111\n",
            "Epoch [23], Batch [671], Train Loss: 1.0822863578796387\n",
            "Epoch [23], Batch [672], Train Loss: 0.7257305383682251\n",
            "Epoch [23], Batch [673], Train Loss: 0.8891850709915161\n",
            "Epoch [23], Batch [674], Train Loss: 1.048154354095459\n",
            "Epoch [23], Batch [675], Train Loss: 0.7996246218681335\n",
            "Epoch [23], Batch [676], Train Loss: 0.9324732422828674\n",
            "Epoch [23], Batch [677], Train Loss: 0.7693387269973755\n",
            "Epoch [23], Batch [678], Train Loss: 0.6906997561454773\n",
            "Epoch [23], Batch [679], Train Loss: 0.8475828170776367\n",
            "Epoch [23], Batch [680], Train Loss: 0.6950691938400269\n",
            "Epoch [23], Batch [681], Train Loss: 0.6949619054794312\n",
            "Epoch [23], Batch [682], Train Loss: 0.7622891664505005\n",
            "Epoch [23], Batch [683], Train Loss: 0.9097834825515747\n",
            "Epoch [23], Batch [684], Train Loss: 0.747018575668335\n",
            "Epoch [23], Batch [685], Train Loss: 0.6765944957733154\n",
            "Epoch [23], Batch [686], Train Loss: 0.9191783666610718\n",
            "Epoch [23], Batch [687], Train Loss: 0.7548962831497192\n",
            "Epoch [23], Batch [688], Train Loss: 0.7060187458992004\n",
            "Epoch [23], Batch [689], Train Loss: 0.664733350276947\n",
            "Epoch [23], Batch [690], Train Loss: 0.7782799005508423\n",
            "Epoch [23], Batch [691], Train Loss: 0.7752567529678345\n",
            "Epoch [23], Batch [692], Train Loss: 0.7635781764984131\n",
            "Epoch [23], Batch [693], Train Loss: 0.7974934577941895\n",
            "Epoch [23], Batch [694], Train Loss: 0.7789466977119446\n",
            "Epoch [23], Batch [695], Train Loss: 0.9059894680976868\n",
            "Epoch [23], Batch [696], Train Loss: 0.9238686561584473\n",
            "Epoch [23], Batch [697], Train Loss: 0.8155580759048462\n",
            "Epoch [23], Batch [698], Train Loss: 0.8669837713241577\n",
            "Epoch [23], Batch [699], Train Loss: 0.8493558168411255\n",
            "Epoch [23], Batch [700], Train Loss: 0.8716710209846497\n",
            "Epoch [23], Batch [701], Train Loss: 0.833797037601471\n",
            "Epoch [23], Batch [702], Train Loss: 0.7486281394958496\n",
            "Epoch [23], Batch [703], Train Loss: 0.7571830153465271\n",
            "Epoch [23], Batch [704], Train Loss: 1.09779691696167\n",
            "Epoch [23], Batch [705], Train Loss: 0.6610327959060669\n",
            "Epoch [23], Batch [706], Train Loss: 0.7178922295570374\n",
            "Epoch [23], Batch [707], Train Loss: 0.5935875177383423\n",
            "Epoch [23], Batch [708], Train Loss: 0.7486481666564941\n",
            "Epoch [23], Batch [709], Train Loss: 0.8532907366752625\n",
            "Epoch [23], Batch [710], Train Loss: 0.7863850593566895\n",
            "Epoch [23], Batch [711], Train Loss: 0.8742592334747314\n",
            "Epoch [23], Batch [712], Train Loss: 0.6464060544967651\n",
            "Epoch [23], Batch [713], Train Loss: 0.9726264476776123\n",
            "Epoch [23], Batch [714], Train Loss: 0.7221924066543579\n",
            "Epoch [23], Batch [715], Train Loss: 0.7320280075073242\n",
            "Epoch [23], Batch [716], Train Loss: 0.6569925546646118\n",
            "Epoch [23], Batch [717], Train Loss: 0.5661084651947021\n",
            "Epoch [23], Batch [718], Train Loss: 0.7172926664352417\n",
            "Epoch [23], Batch [719], Train Loss: 0.6680291295051575\n",
            "Epoch [23], Batch [720], Train Loss: 0.730445146560669\n",
            "Epoch [23], Batch [721], Train Loss: 0.6857468485832214\n",
            "Epoch [23], Batch [722], Train Loss: 0.8511041402816772\n",
            "Epoch [23], Batch [723], Train Loss: 0.759524941444397\n",
            "Epoch [23], Batch [724], Train Loss: 0.7469396591186523\n",
            "Epoch [23], Batch [725], Train Loss: 0.8977053165435791\n",
            "Epoch [23], Batch [726], Train Loss: 1.013503909111023\n",
            "Epoch [23], Batch [727], Train Loss: 0.9872702360153198\n",
            "Epoch [23], Batch [728], Train Loss: 0.7653350830078125\n",
            "Epoch [23], Batch [729], Train Loss: 0.8319711685180664\n",
            "Epoch [23], Batch [730], Train Loss: 0.736402690410614\n",
            "Epoch [23], Batch [731], Train Loss: 0.9053440093994141\n",
            "Epoch [23], Batch [732], Train Loss: 1.0206983089447021\n",
            "Epoch [23], Batch [733], Train Loss: 0.992143452167511\n",
            "Epoch [23], Batch [734], Train Loss: 0.8082230687141418\n",
            "Epoch [23], Batch [735], Train Loss: 0.8070205450057983\n",
            "Epoch [23], Batch [736], Train Loss: 0.8239492177963257\n",
            "Epoch [23], Batch [737], Train Loss: 0.697513222694397\n",
            "Epoch [23], Batch [738], Train Loss: 0.705223798751831\n",
            "Epoch [23], Batch [739], Train Loss: 0.817064642906189\n",
            "Epoch [23], Batch [740], Train Loss: 0.7429803609848022\n",
            "Epoch [23], Batch [741], Train Loss: 0.5922946929931641\n",
            "Epoch [23], Batch [742], Train Loss: 0.5974385142326355\n",
            "Epoch [23], Batch [743], Train Loss: 0.7046028971672058\n",
            "Epoch [23], Batch [744], Train Loss: 0.7623476386070251\n",
            "Epoch [23], Batch [745], Train Loss: 0.8613752126693726\n",
            "Epoch [23], Batch [746], Train Loss: 0.8863533735275269\n",
            "Epoch [23], Batch [747], Train Loss: 0.9632879495620728\n",
            "Epoch [23], Batch [748], Train Loss: 0.6548295021057129\n",
            "Epoch [23], Batch [749], Train Loss: 0.8602068424224854\n",
            "Epoch [23], Batch [750], Train Loss: 0.6785484552383423\n",
            "Epoch [23], Batch [751], Train Loss: 0.6997570991516113\n",
            "Epoch [23], Batch [752], Train Loss: 0.8092443346977234\n",
            "Epoch [23], Batch [753], Train Loss: 0.782362163066864\n",
            "Epoch [23], Batch [754], Train Loss: 0.752240002155304\n",
            "Epoch [23], Batch [755], Train Loss: 0.7190158367156982\n",
            "Epoch [23], Batch [756], Train Loss: 0.7120535373687744\n",
            "Epoch [23], Batch [757], Train Loss: 0.9278766512870789\n",
            "Epoch [23], Batch [758], Train Loss: 0.744617760181427\n",
            "Epoch [23], Batch [759], Train Loss: 0.8563156127929688\n",
            "Epoch [23], Batch [760], Train Loss: 0.8091501593589783\n",
            "Epoch [23], Batch [761], Train Loss: 0.6676764488220215\n",
            "Epoch [23], Batch [762], Train Loss: 1.063833236694336\n",
            "Epoch [23], Batch [763], Train Loss: 0.6931138038635254\n",
            "Epoch [23], Batch [764], Train Loss: 0.6865296363830566\n",
            "Epoch [23], Batch [765], Train Loss: 0.9390004277229309\n",
            "Epoch [23], Batch [766], Train Loss: 0.9179338216781616\n",
            "Epoch [23], Batch [767], Train Loss: 0.8708863854408264\n",
            "Epoch [23], Batch [768], Train Loss: 0.6828436851501465\n",
            "Epoch [23], Batch [769], Train Loss: 0.7065829634666443\n",
            "Epoch [23], Batch [770], Train Loss: 0.9042872786521912\n",
            "Epoch [23], Batch [771], Train Loss: 0.6981407403945923\n",
            "Epoch [23], Batch [772], Train Loss: 0.9342535138130188\n",
            "Epoch [23], Batch [773], Train Loss: 0.6769930124282837\n",
            "Epoch [23], Batch [774], Train Loss: 0.6678029298782349\n",
            "Epoch [23], Batch [775], Train Loss: 0.703154444694519\n",
            "Epoch [23], Batch [776], Train Loss: 0.9830029010772705\n",
            "Epoch [23], Batch [777], Train Loss: 0.6515437960624695\n",
            "Epoch [23], Batch [778], Train Loss: 0.8157474398612976\n",
            "Epoch [23], Batch [779], Train Loss: 0.9012134075164795\n",
            "Epoch [23], Batch [780], Train Loss: 0.8080292344093323\n",
            "Epoch [23], Batch [781], Train Loss: 1.1564148664474487\n",
            "Epoch [23], Batch [782], Train Loss: 0.8500478267669678\n",
            "Epoch [23], Batch [783], Train Loss: 0.736170768737793\n",
            "Epoch [23], Batch [784], Train Loss: 0.7258058190345764\n",
            "Epoch [23], Batch [785], Train Loss: 0.6767582893371582\n",
            "Epoch [23], Batch [786], Train Loss: 1.024296760559082\n",
            "Epoch [23], Batch [787], Train Loss: 1.055476188659668\n",
            "Epoch [23], Batch [788], Train Loss: 0.9459044337272644\n",
            "Epoch [23], Batch [789], Train Loss: 0.7334140539169312\n",
            "Epoch [23], Batch [790], Train Loss: 0.9429500102996826\n",
            "Epoch [23], Batch [791], Train Loss: 0.7917400598526001\n",
            "Epoch [23], Batch [792], Train Loss: 0.8356379866600037\n",
            "Epoch [23], Batch [793], Train Loss: 0.6398055553436279\n",
            "Epoch [23], Batch [794], Train Loss: 0.9269877672195435\n",
            "Epoch [23], Batch [795], Train Loss: 0.7033969163894653\n",
            "Epoch [23], Batch [796], Train Loss: 0.700134813785553\n",
            "Epoch [23], Batch [797], Train Loss: 0.9138329029083252\n",
            "Epoch [23], Batch [798], Train Loss: 0.8586143255233765\n",
            "Epoch [23], Batch [799], Train Loss: 0.7794592380523682\n",
            "Epoch [23], Batch [800], Train Loss: 0.9009768962860107\n",
            "Epoch [23], Batch [801], Train Loss: 0.8539953231811523\n",
            "Epoch [23], Batch [802], Train Loss: 0.7758566737174988\n",
            "Epoch [23], Batch [803], Train Loss: 0.8428563475608826\n",
            "Epoch [23], Batch [804], Train Loss: 0.8775224089622498\n",
            "Epoch [23], Batch [805], Train Loss: 0.6141183376312256\n",
            "Epoch [23], Batch [806], Train Loss: 0.9334613680839539\n",
            "Epoch [23], Batch [807], Train Loss: 0.7075148820877075\n",
            "Epoch [23], Batch [808], Train Loss: 0.718568742275238\n",
            "Epoch [23], Batch [809], Train Loss: 0.7705436944961548\n",
            "Epoch [23], Batch [810], Train Loss: 0.645582914352417\n",
            "Epoch [23], Batch [811], Train Loss: 0.6708763241767883\n",
            "Epoch [23], Batch [812], Train Loss: 0.7587112188339233\n",
            "Epoch [23], Batch [813], Train Loss: 0.7333602905273438\n",
            "Epoch [23], Batch [814], Train Loss: 0.6754345893859863\n",
            "Epoch [23], Batch [815], Train Loss: 0.837931752204895\n",
            "Epoch [23], Batch [816], Train Loss: 0.8013184070587158\n",
            "Epoch [23], Batch [817], Train Loss: 0.7176851630210876\n",
            "Epoch [23], Batch [818], Train Loss: 0.9404402375221252\n",
            "Epoch [23], Batch [819], Train Loss: 0.5546674132347107\n",
            "Epoch [23], Batch [820], Train Loss: 0.948162317276001\n",
            "Epoch [23], Batch [821], Train Loss: 0.8954740762710571\n",
            "Epoch [23], Batch [822], Train Loss: 0.8575137257575989\n",
            "Epoch [23], Batch [823], Train Loss: 0.9922681450843811\n",
            "Epoch [23], Batch [824], Train Loss: 0.7942994832992554\n",
            "Epoch [23], Batch [825], Train Loss: 1.073714256286621\n",
            "Epoch [23], Batch [826], Train Loss: 0.7978160381317139\n",
            "Epoch [23], Batch [827], Train Loss: 0.6434520483016968\n",
            "Epoch [23], Batch [828], Train Loss: 0.6774374842643738\n",
            "Epoch [23], Batch [829], Train Loss: 0.7065410017967224\n",
            "Epoch [23], Batch [830], Train Loss: 0.8234782814979553\n",
            "Epoch [23], Batch [831], Train Loss: 0.8657152652740479\n",
            "Epoch [23], Batch [832], Train Loss: 0.9851937294006348\n",
            "Epoch [23], Batch [833], Train Loss: 0.6614012122154236\n",
            "Epoch [23], Batch [834], Train Loss: 0.8343580961227417\n",
            "Epoch [23], Batch [835], Train Loss: 0.7010931372642517\n",
            "Epoch [23], Batch [836], Train Loss: 0.6834859848022461\n",
            "Epoch [23], Batch [837], Train Loss: 0.8860795497894287\n",
            "Epoch [23], Batch [838], Train Loss: 0.8318932056427002\n",
            "Epoch [23], Batch [839], Train Loss: 1.1217831373214722\n",
            "Epoch [23], Batch [840], Train Loss: 0.7161772847175598\n",
            "Epoch [23], Batch [841], Train Loss: 0.776614785194397\n",
            "Epoch [23], Batch [842], Train Loss: 0.651222288608551\n",
            "Epoch [23], Batch [843], Train Loss: 0.7431541085243225\n",
            "Epoch [23], Batch [844], Train Loss: 0.6337059140205383\n",
            "Epoch [23], Batch [845], Train Loss: 0.84138023853302\n",
            "Epoch [23], Batch [846], Train Loss: 0.7379043102264404\n",
            "Epoch [23], Batch [847], Train Loss: 0.9180268049240112\n",
            "Epoch [23], Batch [848], Train Loss: 0.668030858039856\n",
            "Epoch [23], Batch [849], Train Loss: 0.8253082036972046\n",
            "Epoch [23], Batch [850], Train Loss: 0.6229748725891113\n",
            "Epoch [23], Batch [851], Train Loss: 0.7905877232551575\n",
            "Epoch [23], Batch [852], Train Loss: 0.6743100881576538\n",
            "Epoch [23], Batch [853], Train Loss: 0.8393877148628235\n",
            "Epoch [23], Batch [854], Train Loss: 0.7102183699607849\n",
            "Epoch [23], Batch [855], Train Loss: 0.8485572338104248\n",
            "Epoch [23], Batch [856], Train Loss: 0.6736553907394409\n",
            "Epoch [23], Batch [857], Train Loss: 0.7651362419128418\n",
            "Epoch [23], Batch [858], Train Loss: 0.6839601993560791\n",
            "Epoch [23], Batch [859], Train Loss: 0.7221593260765076\n",
            "Epoch [23], Batch [860], Train Loss: 0.6814837455749512\n",
            "Epoch [23], Batch [861], Train Loss: 0.8656212091445923\n",
            "Epoch [23], Batch [862], Train Loss: 0.5475945472717285\n",
            "Epoch [23], Batch [863], Train Loss: 0.7811533808708191\n",
            "Epoch [23], Batch [864], Train Loss: 0.910544753074646\n",
            "Epoch [23], Batch [865], Train Loss: 0.6769539713859558\n",
            "Epoch [23], Batch [866], Train Loss: 0.6615927219390869\n",
            "Epoch [23], Batch [867], Train Loss: 0.8194640278816223\n",
            "Epoch [23], Batch [868], Train Loss: 0.7495201826095581\n",
            "Epoch [23], Batch [869], Train Loss: 0.6376354694366455\n",
            "Epoch [23], Batch [870], Train Loss: 0.7545722126960754\n",
            "Epoch [23], Batch [871], Train Loss: 0.7674506306648254\n",
            "Epoch [23], Batch [872], Train Loss: 0.7323710322380066\n",
            "Epoch [23], Batch [873], Train Loss: 0.6699990630149841\n",
            "Epoch [23], Batch [874], Train Loss: 0.7597476840019226\n",
            "Epoch [23], Batch [875], Train Loss: 0.7186324596405029\n",
            "Epoch [23], Batch [876], Train Loss: 0.9464442133903503\n",
            "Epoch [23], Batch [877], Train Loss: 0.6901739835739136\n",
            "Epoch [23], Batch [878], Train Loss: 0.8936452865600586\n",
            "Epoch [23], Batch [879], Train Loss: 1.1398835182189941\n",
            "Epoch [23], Batch [880], Train Loss: 0.8239318132400513\n",
            "Epoch [23], Batch [881], Train Loss: 0.6074148416519165\n",
            "Epoch [23], Batch [882], Train Loss: 0.9221239686012268\n",
            "Epoch [23], Batch [883], Train Loss: 1.0287792682647705\n",
            "Epoch [23], Batch [884], Train Loss: 1.0608651638031006\n",
            "Epoch [23], Batch [885], Train Loss: 0.9390076398849487\n",
            "Epoch [23], Batch [886], Train Loss: 0.7672378420829773\n",
            "Epoch [23], Batch [887], Train Loss: 0.9445846080780029\n",
            "Epoch [23], Batch [888], Train Loss: 0.7771227955818176\n",
            "Epoch [23], Batch [889], Train Loss: 0.7815151810646057\n",
            "Epoch [23], Batch [890], Train Loss: 0.8103018403053284\n",
            "Epoch [23], Batch [891], Train Loss: 0.6411850452423096\n",
            "Epoch [23], Batch [892], Train Loss: 0.6990014314651489\n",
            "Epoch [23], Batch [893], Train Loss: 0.6148393154144287\n",
            "Epoch [23], Batch [894], Train Loss: 0.6293904781341553\n",
            "Epoch [23], Batch [895], Train Loss: 0.7967536449432373\n",
            "Epoch [23], Batch [896], Train Loss: 0.7968720197677612\n",
            "Epoch [23], Batch [897], Train Loss: 0.6331893801689148\n",
            "Epoch [23], Batch [898], Train Loss: 0.9089969396591187\n",
            "Epoch [23], Batch [899], Train Loss: 0.6482670903205872\n",
            "Epoch [23], Batch [900], Train Loss: 0.8639214634895325\n",
            "Epoch [23], Batch [901], Train Loss: 0.8446168899536133\n",
            "Epoch [23], Batch [902], Train Loss: 0.6333719491958618\n",
            "Epoch [23], Batch [903], Train Loss: 0.615313708782196\n",
            "Epoch [23], Batch [904], Train Loss: 0.6811702847480774\n",
            "Epoch [23], Batch [905], Train Loss: 0.8768342137336731\n",
            "Epoch [23], Batch [906], Train Loss: 0.7650105953216553\n",
            "Epoch [23], Batch [907], Train Loss: 0.7513300180435181\n",
            "Epoch [23], Batch [908], Train Loss: 0.7935584187507629\n",
            "Epoch [23], Batch [909], Train Loss: 0.7189579010009766\n",
            "Epoch [23], Batch [910], Train Loss: 0.9330039024353027\n",
            "Epoch [23], Batch [911], Train Loss: 0.846354603767395\n",
            "Epoch [23], Batch [912], Train Loss: 0.5495036244392395\n",
            "Epoch [23], Batch [913], Train Loss: 0.8036129474639893\n",
            "Epoch [23], Batch [914], Train Loss: 0.7056741714477539\n",
            "Epoch [23], Batch [915], Train Loss: 0.7658519148826599\n",
            "Epoch [23], Batch [916], Train Loss: 0.6812016367912292\n",
            "Epoch [23], Batch [917], Train Loss: 0.8086626529693604\n",
            "Epoch [23], Batch [918], Train Loss: 0.9257603883743286\n",
            "Epoch [23], Batch [919], Train Loss: 0.8484439849853516\n",
            "Epoch [23], Batch [920], Train Loss: 0.7881529331207275\n",
            "Epoch [23], Batch [921], Train Loss: 1.1460130214691162\n",
            "Epoch [23], Batch [922], Train Loss: 0.8544767498970032\n",
            "Epoch [23], Batch [923], Train Loss: 0.8087581992149353\n",
            "Epoch [23], Batch [924], Train Loss: 0.9082796573638916\n",
            "Epoch [23], Batch [925], Train Loss: 0.6545141935348511\n",
            "Epoch [23], Batch [926], Train Loss: 0.9863417148590088\n",
            "Epoch [23], Batch [927], Train Loss: 0.7304725646972656\n",
            "Epoch [23], Batch [928], Train Loss: 0.753635048866272\n",
            "Epoch [23], Batch [929], Train Loss: 0.8027740716934204\n",
            "Epoch [23], Batch [930], Train Loss: 0.7861351370811462\n",
            "Epoch [23], Batch [931], Train Loss: 1.0863986015319824\n",
            "Epoch [23], Batch [932], Train Loss: 0.7887883186340332\n",
            "Epoch [23], Batch [933], Train Loss: 0.6934887170791626\n",
            "Epoch [23], Batch [934], Train Loss: 0.787065327167511\n",
            "Epoch [23], Batch [935], Train Loss: 0.7886043190956116\n",
            "Epoch [23], Batch [936], Train Loss: 0.7543215751647949\n",
            "Epoch [23], Batch [937], Train Loss: 0.6925895810127258\n",
            "Epoch [23], Batch [938], Train Loss: 0.640779435634613\n",
            "Accuracy of train set: 0.7016833333333333\n",
            "Epoch [24], Batch [1], Loss: 0.7824468612670898\n",
            "Epoch [24], Batch [2], Loss: 0.6038403511047363\n",
            "Epoch [24], Batch [3], Loss: 0.6816769242286682\n",
            "Epoch [24], Batch [4], Loss: 0.7113940715789795\n",
            "Epoch [24], Batch [5], Loss: 0.735336422920227\n",
            "Epoch [24], Batch [6], Loss: 0.7245897650718689\n",
            "Epoch [24], Batch [7], Loss: 0.7716670036315918\n",
            "Epoch [24], Batch [8], Loss: 0.825630784034729\n",
            "Epoch [24], Batch [9], Loss: 0.7838068008422852\n",
            "Epoch [24], Batch [10], Loss: 0.9053707122802734\n",
            "Epoch [24], Batch [11], Loss: 0.9432464838027954\n",
            "Epoch [24], Batch [12], Loss: 0.7933017611503601\n",
            "Epoch [24], Batch [13], Loss: 0.6345852613449097\n",
            "Epoch [24], Batch [14], Loss: 0.6550379991531372\n",
            "Epoch [24], Batch [15], Loss: 0.7211065292358398\n",
            "Epoch [24], Batch [16], Loss: 0.9230328798294067\n",
            "Epoch [24], Batch [17], Loss: 0.7209963798522949\n",
            "Epoch [24], Batch [18], Loss: 0.9692445993423462\n",
            "Epoch [24], Batch [19], Loss: 0.8336625099182129\n",
            "Epoch [24], Batch [20], Loss: 0.8889168500900269\n",
            "Epoch [24], Batch [21], Loss: 0.9440394043922424\n",
            "Epoch [24], Batch [22], Loss: 0.7749476432800293\n",
            "Epoch [24], Batch [23], Loss: 0.679280161857605\n",
            "Epoch [24], Batch [24], Loss: 0.7024644613265991\n",
            "Epoch [24], Batch [25], Loss: 0.7437378168106079\n",
            "Epoch [24], Batch [26], Loss: 0.8248816132545471\n",
            "Epoch [24], Batch [27], Loss: 0.7797995805740356\n",
            "Epoch [24], Batch [28], Loss: 0.8312509655952454\n",
            "Epoch [24], Batch [29], Loss: 0.8810579180717468\n",
            "Epoch [24], Batch [30], Loss: 0.4733196496963501\n",
            "Epoch [24], Batch [31], Loss: 0.8920060396194458\n",
            "Epoch [24], Batch [32], Loss: 0.6559098958969116\n",
            "Epoch [24], Batch [33], Loss: 0.8658186197280884\n",
            "Epoch [24], Batch [34], Loss: 0.7009097337722778\n",
            "Epoch [24], Batch [35], Loss: 0.6892767548561096\n",
            "Epoch [24], Batch [36], Loss: 0.707353949546814\n",
            "Epoch [24], Batch [37], Loss: 0.6758773922920227\n",
            "Epoch [24], Batch [38], Loss: 0.7380185127258301\n",
            "Epoch [24], Batch [39], Loss: 0.8155292868614197\n",
            "Epoch [24], Batch [40], Loss: 0.7522264719009399\n",
            "Epoch [24], Batch [41], Loss: 0.8242318034172058\n",
            "Epoch [24], Batch [42], Loss: 0.941153883934021\n",
            "Epoch [24], Batch [43], Loss: 0.9739947319030762\n",
            "Epoch [24], Batch [44], Loss: 0.6818857192993164\n",
            "Epoch [24], Batch [45], Loss: 0.913777232170105\n",
            "Epoch [24], Batch [46], Loss: 1.08370840549469\n",
            "Epoch [24], Batch [47], Loss: 0.8032028675079346\n",
            "Epoch [24], Batch [48], Loss: 1.0119765996932983\n",
            "Epoch [24], Batch [49], Loss: 0.885290265083313\n",
            "Epoch [24], Batch [50], Loss: 0.7178186178207397\n",
            "Epoch [24], Batch [51], Loss: 1.1089247465133667\n",
            "Epoch [24], Batch [52], Loss: 0.8911989331245422\n",
            "Epoch [24], Batch [53], Loss: 0.7200050354003906\n",
            "Epoch [24], Batch [54], Loss: 0.9108700752258301\n",
            "Epoch [24], Batch [55], Loss: 1.054585576057434\n",
            "Epoch [24], Batch [56], Loss: 0.7561566829681396\n",
            "Epoch [24], Batch [57], Loss: 0.8912988901138306\n",
            "Epoch [24], Batch [58], Loss: 0.9649325609207153\n",
            "Epoch [24], Batch [59], Loss: 0.8104478120803833\n",
            "Epoch [24], Batch [60], Loss: 0.9754646420478821\n",
            "Epoch [24], Batch [61], Loss: 0.7782533764839172\n",
            "Epoch [24], Batch [62], Loss: 1.2043991088867188\n",
            "Epoch [24], Batch [63], Loss: 0.7975098490715027\n",
            "Epoch [24], Batch [64], Loss: 0.9687676429748535\n",
            "Epoch [24], Batch [65], Loss: 0.9452795386314392\n",
            "Epoch [24], Batch [66], Loss: 0.7856398224830627\n",
            "Epoch [24], Batch [67], Loss: 0.8508888483047485\n",
            "Epoch [24], Batch [68], Loss: 0.6388856768608093\n",
            "Epoch [24], Batch [69], Loss: 0.8966680765151978\n",
            "Epoch [24], Batch [70], Loss: 0.5595836639404297\n",
            "Epoch [24], Batch [71], Loss: 0.5883090496063232\n",
            "Epoch [24], Batch [72], Loss: 0.5453117489814758\n",
            "Epoch [24], Batch [73], Loss: 0.7918379306793213\n",
            "Epoch [24], Batch [74], Loss: 0.8840134143829346\n",
            "Epoch [24], Batch [75], Loss: 0.7285214066505432\n",
            "Epoch [24], Batch [76], Loss: 0.8595659732818604\n",
            "Epoch [24], Batch [77], Loss: 0.8161363005638123\n",
            "Epoch [24], Batch [78], Loss: 0.728447437286377\n",
            "Epoch [24], Batch [79], Loss: 1.065690517425537\n",
            "Epoch [24], Batch [80], Loss: 0.8550755977630615\n",
            "Epoch [24], Batch [81], Loss: 1.004526972770691\n",
            "Epoch [24], Batch [82], Loss: 0.7498814463615417\n",
            "Epoch [24], Batch [83], Loss: 0.7273393869400024\n",
            "Epoch [24], Batch [84], Loss: 0.752912163734436\n",
            "Epoch [24], Batch [85], Loss: 0.7395588755607605\n",
            "Epoch [24], Batch [86], Loss: 0.8448892831802368\n",
            "Epoch [24], Batch [87], Loss: 0.9709057807922363\n",
            "Epoch [24], Batch [88], Loss: 0.9974313974380493\n",
            "Epoch [24], Batch [89], Loss: 0.9112206697463989\n",
            "Epoch [24], Batch [90], Loss: 0.7455433011054993\n",
            "Epoch [24], Batch [91], Loss: 0.7956129312515259\n",
            "Epoch [24], Batch [92], Loss: 0.6538474559783936\n",
            "Epoch [24], Batch [93], Loss: 0.5807378888130188\n",
            "Epoch [24], Batch [94], Loss: 0.8908880949020386\n",
            "Epoch [24], Batch [95], Loss: 0.677524745464325\n",
            "Epoch [24], Batch [96], Loss: 0.8895819187164307\n",
            "Epoch [24], Batch [97], Loss: 0.6944589614868164\n",
            "Epoch [24], Batch [98], Loss: 0.8023391962051392\n",
            "Epoch [24], Batch [99], Loss: 0.9804376363754272\n",
            "Epoch [24], Batch [100], Loss: 0.6610780954360962\n",
            "Epoch [24], Batch [101], Loss: 0.814562201499939\n",
            "Epoch [24], Batch [102], Loss: 0.8124829530715942\n",
            "Epoch [24], Batch [103], Loss: 0.7257932424545288\n",
            "Epoch [24], Batch [104], Loss: 0.7732523083686829\n",
            "Epoch [24], Batch [105], Loss: 1.1035211086273193\n",
            "Epoch [24], Batch [106], Loss: 0.6148378252983093\n",
            "Epoch [24], Batch [107], Loss: 0.867815375328064\n",
            "Epoch [24], Batch [108], Loss: 0.7731146812438965\n",
            "Epoch [24], Batch [109], Loss: 0.8721351623535156\n",
            "Epoch [24], Batch [110], Loss: 0.7976067662239075\n",
            "Epoch [24], Batch [111], Loss: 0.7260097861289978\n",
            "Epoch [24], Batch [112], Loss: 0.571296215057373\n",
            "Epoch [24], Batch [113], Loss: 0.8841009140014648\n",
            "Epoch [24], Batch [114], Loss: 0.7996803522109985\n",
            "Epoch [24], Batch [115], Loss: 0.7529381513595581\n",
            "Epoch [24], Batch [116], Loss: 0.7758764028549194\n",
            "Epoch [24], Batch [117], Loss: 0.7618176937103271\n",
            "Epoch [24], Batch [118], Loss: 0.7816031575202942\n",
            "Epoch [24], Batch [119], Loss: 0.8032744526863098\n",
            "Epoch [24], Batch [120], Loss: 0.7288498878479004\n",
            "Epoch [24], Batch [121], Loss: 0.7753379940986633\n",
            "Epoch [24], Batch [122], Loss: 0.6610856056213379\n",
            "Epoch [24], Batch [123], Loss: 0.831598699092865\n",
            "Epoch [24], Batch [124], Loss: 1.1357570886611938\n",
            "Epoch [24], Batch [125], Loss: 0.8295696377754211\n",
            "Epoch [24], Batch [126], Loss: 0.8491480946540833\n",
            "Epoch [24], Batch [127], Loss: 0.6670055985450745\n",
            "Epoch [24], Batch [128], Loss: 0.8446706533432007\n",
            "Epoch [24], Batch [129], Loss: 0.8898475766181946\n",
            "Epoch [24], Batch [130], Loss: 0.8573063611984253\n",
            "Epoch [24], Batch [131], Loss: 0.6621199250221252\n",
            "Epoch [24], Batch [132], Loss: 0.638237476348877\n",
            "Epoch [24], Batch [133], Loss: 0.7044657468795776\n",
            "Epoch [24], Batch [134], Loss: 0.7730011343955994\n",
            "Epoch [24], Batch [135], Loss: 0.7578033208847046\n",
            "Epoch [24], Batch [136], Loss: 0.7012453079223633\n",
            "Epoch [24], Batch [137], Loss: 0.9744799137115479\n",
            "Epoch [24], Batch [138], Loss: 0.7527206540107727\n",
            "Epoch [24], Batch [139], Loss: 0.8467292189598083\n",
            "Epoch [24], Batch [140], Loss: 0.8578734397888184\n",
            "Epoch [24], Batch [141], Loss: 0.6899222135543823\n",
            "Epoch [24], Batch [142], Loss: 0.8773520588874817\n",
            "Epoch [24], Batch [143], Loss: 0.8798397183418274\n",
            "Epoch [24], Batch [144], Loss: 0.884366512298584\n",
            "Epoch [24], Batch [145], Loss: 0.8269574642181396\n",
            "Epoch [24], Batch [146], Loss: 0.804743766784668\n",
            "Epoch [24], Batch [147], Loss: 0.8029239177703857\n",
            "Epoch [24], Batch [148], Loss: 0.6409333348274231\n",
            "Epoch [24], Batch [149], Loss: 0.9818833470344543\n",
            "Epoch [24], Batch [150], Loss: 1.0333805084228516\n",
            "Epoch [24], Batch [151], Loss: 1.0845117568969727\n",
            "Epoch [24], Batch [152], Loss: 0.628663182258606\n",
            "Epoch [24], Batch [153], Loss: 0.6835519671440125\n",
            "Epoch [24], Batch [154], Loss: 0.7099642753601074\n",
            "Epoch [24], Batch [155], Loss: 1.1297396421432495\n",
            "Epoch [24], Batch [156], Loss: 0.7709515690803528\n",
            "Epoch [24], Batch [157], Loss: 0.6851682662963867\n",
            "Accuracy of None set: 0.6966\n",
            "Epoch [24], Batch [1], Train Loss: 0.840232789516449\n",
            "Epoch [24], Batch [2], Train Loss: 0.6630367040634155\n",
            "Epoch [24], Batch [3], Train Loss: 0.9835517406463623\n",
            "Epoch [24], Batch [4], Train Loss: 0.8143560886383057\n",
            "Epoch [24], Batch [5], Train Loss: 0.7701882123947144\n",
            "Epoch [24], Batch [6], Train Loss: 0.5665134191513062\n",
            "Epoch [24], Batch [7], Train Loss: 0.8482925295829773\n",
            "Epoch [24], Batch [8], Train Loss: 0.9601606130599976\n",
            "Epoch [24], Batch [9], Train Loss: 1.190208911895752\n",
            "Epoch [24], Batch [10], Train Loss: 0.9784482717514038\n",
            "Epoch [24], Batch [11], Train Loss: 0.9443065524101257\n",
            "Epoch [24], Batch [12], Train Loss: 0.6964027285575867\n",
            "Epoch [24], Batch [13], Train Loss: 0.9349905252456665\n",
            "Epoch [24], Batch [14], Train Loss: 0.6615840196609497\n",
            "Epoch [24], Batch [15], Train Loss: 0.7157598733901978\n",
            "Epoch [24], Batch [16], Train Loss: 0.756473183631897\n",
            "Epoch [24], Batch [17], Train Loss: 0.732824981212616\n",
            "Epoch [24], Batch [18], Train Loss: 0.6676570177078247\n",
            "Epoch [24], Batch [19], Train Loss: 0.7084008455276489\n",
            "Epoch [24], Batch [20], Train Loss: 0.637594997882843\n",
            "Epoch [24], Batch [21], Train Loss: 0.7287552952766418\n",
            "Epoch [24], Batch [22], Train Loss: 0.6662348508834839\n",
            "Epoch [24], Batch [23], Train Loss: 0.8906480073928833\n",
            "Epoch [24], Batch [24], Train Loss: 0.7177141904830933\n",
            "Epoch [24], Batch [25], Train Loss: 0.8308333158493042\n",
            "Epoch [24], Batch [26], Train Loss: 0.7218345999717712\n",
            "Epoch [24], Batch [27], Train Loss: 0.7380241751670837\n",
            "Epoch [24], Batch [28], Train Loss: 0.6081347465515137\n",
            "Epoch [24], Batch [29], Train Loss: 1.0057669878005981\n",
            "Epoch [24], Batch [30], Train Loss: 0.8494088649749756\n",
            "Epoch [24], Batch [31], Train Loss: 1.0251545906066895\n",
            "Epoch [24], Batch [32], Train Loss: 0.6552094221115112\n",
            "Epoch [24], Batch [33], Train Loss: 0.6762954592704773\n",
            "Epoch [24], Batch [34], Train Loss: 0.7628591060638428\n",
            "Epoch [24], Batch [35], Train Loss: 0.637383222579956\n",
            "Epoch [24], Batch [36], Train Loss: 0.7259528040885925\n",
            "Epoch [24], Batch [37], Train Loss: 0.9495323896408081\n",
            "Epoch [24], Batch [38], Train Loss: 0.8463407754898071\n",
            "Epoch [24], Batch [39], Train Loss: 0.700130820274353\n",
            "Epoch [24], Batch [40], Train Loss: 0.7439298033714294\n",
            "Epoch [24], Batch [41], Train Loss: 0.8596663475036621\n",
            "Epoch [24], Batch [42], Train Loss: 0.8976881504058838\n",
            "Epoch [24], Batch [43], Train Loss: 0.8941383361816406\n",
            "Epoch [24], Batch [44], Train Loss: 0.7933783531188965\n",
            "Epoch [24], Batch [45], Train Loss: 0.9738276600837708\n",
            "Epoch [24], Batch [46], Train Loss: 0.7259681820869446\n",
            "Epoch [24], Batch [47], Train Loss: 0.8376345634460449\n",
            "Epoch [24], Batch [48], Train Loss: 0.8114087581634521\n",
            "Epoch [24], Batch [49], Train Loss: 0.8873173594474792\n",
            "Epoch [24], Batch [50], Train Loss: 0.8768066167831421\n",
            "Epoch [24], Batch [51], Train Loss: 0.8005486130714417\n",
            "Epoch [24], Batch [52], Train Loss: 0.7420293092727661\n",
            "Epoch [24], Batch [53], Train Loss: 0.7971752285957336\n",
            "Epoch [24], Batch [54], Train Loss: 0.8086793422698975\n",
            "Epoch [24], Batch [55], Train Loss: 0.7478446364402771\n",
            "Epoch [24], Batch [56], Train Loss: 0.6010156273841858\n",
            "Epoch [24], Batch [57], Train Loss: 0.6803845167160034\n",
            "Epoch [24], Batch [58], Train Loss: 0.6816623210906982\n",
            "Epoch [24], Batch [59], Train Loss: 0.7295151948928833\n",
            "Epoch [24], Batch [60], Train Loss: 0.6620298624038696\n",
            "Epoch [24], Batch [61], Train Loss: 0.8760873079299927\n",
            "Epoch [24], Batch [62], Train Loss: 0.8542843461036682\n",
            "Epoch [24], Batch [63], Train Loss: 0.913152813911438\n",
            "Epoch [24], Batch [64], Train Loss: 0.5808539390563965\n",
            "Epoch [24], Batch [65], Train Loss: 0.7738670706748962\n",
            "Epoch [24], Batch [66], Train Loss: 0.7311577796936035\n",
            "Epoch [24], Batch [67], Train Loss: 0.8768932223320007\n",
            "Epoch [24], Batch [68], Train Loss: 0.6977046132087708\n",
            "Epoch [24], Batch [69], Train Loss: 0.6568900346755981\n",
            "Epoch [24], Batch [70], Train Loss: 0.7341398596763611\n",
            "Epoch [24], Batch [71], Train Loss: 0.9014533758163452\n",
            "Epoch [24], Batch [72], Train Loss: 0.9154373407363892\n",
            "Epoch [24], Batch [73], Train Loss: 0.7059791088104248\n",
            "Epoch [24], Batch [74], Train Loss: 0.863619863986969\n",
            "Epoch [24], Batch [75], Train Loss: 0.772363007068634\n",
            "Epoch [24], Batch [76], Train Loss: 0.6415212154388428\n",
            "Epoch [24], Batch [77], Train Loss: 0.6549220085144043\n",
            "Epoch [24], Batch [78], Train Loss: 0.8195813894271851\n",
            "Epoch [24], Batch [79], Train Loss: 0.7139039635658264\n",
            "Epoch [24], Batch [80], Train Loss: 0.8328877091407776\n",
            "Epoch [24], Batch [81], Train Loss: 0.632752537727356\n",
            "Epoch [24], Batch [82], Train Loss: 0.6389369964599609\n",
            "Epoch [24], Batch [83], Train Loss: 0.6112337112426758\n",
            "Epoch [24], Batch [84], Train Loss: 0.6909079551696777\n",
            "Epoch [24], Batch [85], Train Loss: 0.712876558303833\n",
            "Epoch [24], Batch [86], Train Loss: 0.8660314083099365\n",
            "Epoch [24], Batch [87], Train Loss: 0.650863528251648\n",
            "Epoch [24], Batch [88], Train Loss: 0.955802857875824\n",
            "Epoch [24], Batch [89], Train Loss: 0.9901874661445618\n",
            "Epoch [24], Batch [90], Train Loss: 0.6813758611679077\n",
            "Epoch [24], Batch [91], Train Loss: 0.8165648579597473\n",
            "Epoch [24], Batch [92], Train Loss: 0.9389253854751587\n",
            "Epoch [24], Batch [93], Train Loss: 0.6094545125961304\n",
            "Epoch [24], Batch [94], Train Loss: 0.7069819569587708\n",
            "Epoch [24], Batch [95], Train Loss: 0.7791424989700317\n",
            "Epoch [24], Batch [96], Train Loss: 0.9134268164634705\n",
            "Epoch [24], Batch [97], Train Loss: 0.869930624961853\n",
            "Epoch [24], Batch [98], Train Loss: 0.6549674272537231\n",
            "Epoch [24], Batch [99], Train Loss: 0.7570930123329163\n",
            "Epoch [24], Batch [100], Train Loss: 0.6667131781578064\n",
            "Epoch [24], Batch [101], Train Loss: 0.7560700178146362\n",
            "Epoch [24], Batch [102], Train Loss: 0.829184889793396\n",
            "Epoch [24], Batch [103], Train Loss: 0.800019383430481\n",
            "Epoch [24], Batch [104], Train Loss: 0.8103125691413879\n",
            "Epoch [24], Batch [105], Train Loss: 0.8460347652435303\n",
            "Epoch [24], Batch [106], Train Loss: 0.7568849921226501\n",
            "Epoch [24], Batch [107], Train Loss: 1.0458662509918213\n",
            "Epoch [24], Batch [108], Train Loss: 0.8155084252357483\n",
            "Epoch [24], Batch [109], Train Loss: 0.9053638577461243\n",
            "Epoch [24], Batch [110], Train Loss: 0.8500357270240784\n",
            "Epoch [24], Batch [111], Train Loss: 0.6929813623428345\n",
            "Epoch [24], Batch [112], Train Loss: 0.8046340346336365\n",
            "Epoch [24], Batch [113], Train Loss: 0.7345212697982788\n",
            "Epoch [24], Batch [114], Train Loss: 0.9724915623664856\n",
            "Epoch [24], Batch [115], Train Loss: 0.7276453971862793\n",
            "Epoch [24], Batch [116], Train Loss: 0.8068616390228271\n",
            "Epoch [24], Batch [117], Train Loss: 0.8684748411178589\n",
            "Epoch [24], Batch [118], Train Loss: 0.725805938243866\n",
            "Epoch [24], Batch [119], Train Loss: 0.7686502933502197\n",
            "Epoch [24], Batch [120], Train Loss: 0.6823575496673584\n",
            "Epoch [24], Batch [121], Train Loss: 0.8834975361824036\n",
            "Epoch [24], Batch [122], Train Loss: 0.8757114410400391\n",
            "Epoch [24], Batch [123], Train Loss: 0.911084771156311\n",
            "Epoch [24], Batch [124], Train Loss: 0.7074774503707886\n",
            "Epoch [24], Batch [125], Train Loss: 0.7011674046516418\n",
            "Epoch [24], Batch [126], Train Loss: 0.6142575144767761\n",
            "Epoch [24], Batch [127], Train Loss: 0.8224304914474487\n",
            "Epoch [24], Batch [128], Train Loss: 0.8193871974945068\n",
            "Epoch [24], Batch [129], Train Loss: 0.6140642166137695\n",
            "Epoch [24], Batch [130], Train Loss: 0.7306299805641174\n",
            "Epoch [24], Batch [131], Train Loss: 0.7759057879447937\n",
            "Epoch [24], Batch [132], Train Loss: 0.9384217262268066\n",
            "Epoch [24], Batch [133], Train Loss: 1.040602445602417\n",
            "Epoch [24], Batch [134], Train Loss: 0.7012226581573486\n",
            "Epoch [24], Batch [135], Train Loss: 0.7137389183044434\n",
            "Epoch [24], Batch [136], Train Loss: 0.9540604948997498\n",
            "Epoch [24], Batch [137], Train Loss: 1.0601930618286133\n",
            "Epoch [24], Batch [138], Train Loss: 1.0107433795928955\n",
            "Epoch [24], Batch [139], Train Loss: 0.868747353553772\n",
            "Epoch [24], Batch [140], Train Loss: 0.9059338569641113\n",
            "Epoch [24], Batch [141], Train Loss: 0.7026264667510986\n",
            "Epoch [24], Batch [142], Train Loss: 0.8178845643997192\n",
            "Epoch [24], Batch [143], Train Loss: 0.702812910079956\n",
            "Epoch [24], Batch [144], Train Loss: 0.5854560732841492\n",
            "Epoch [24], Batch [145], Train Loss: 0.7526068687438965\n",
            "Epoch [24], Batch [146], Train Loss: 0.6151606440544128\n",
            "Epoch [24], Batch [147], Train Loss: 0.8741903901100159\n",
            "Epoch [24], Batch [148], Train Loss: 0.7501888871192932\n",
            "Epoch [24], Batch [149], Train Loss: 0.7631922364234924\n",
            "Epoch [24], Batch [150], Train Loss: 0.9026190042495728\n",
            "Epoch [24], Batch [151], Train Loss: 0.7322214841842651\n",
            "Epoch [24], Batch [152], Train Loss: 0.8174124956130981\n",
            "Epoch [24], Batch [153], Train Loss: 0.6681815385818481\n",
            "Epoch [24], Batch [154], Train Loss: 0.7886281609535217\n",
            "Epoch [24], Batch [155], Train Loss: 0.66310054063797\n",
            "Epoch [24], Batch [156], Train Loss: 0.6704748868942261\n",
            "Epoch [24], Batch [157], Train Loss: 0.7913661003112793\n",
            "Epoch [24], Batch [158], Train Loss: 0.6753535270690918\n",
            "Epoch [24], Batch [159], Train Loss: 0.6725212335586548\n",
            "Epoch [24], Batch [160], Train Loss: 0.7260372638702393\n",
            "Epoch [24], Batch [161], Train Loss: 0.7487930059432983\n",
            "Epoch [24], Batch [162], Train Loss: 0.5830351114273071\n",
            "Epoch [24], Batch [163], Train Loss: 0.710848331451416\n",
            "Epoch [24], Batch [164], Train Loss: 0.8617299795150757\n",
            "Epoch [24], Batch [165], Train Loss: 0.8371602296829224\n",
            "Epoch [24], Batch [166], Train Loss: 0.7218606472015381\n",
            "Epoch [24], Batch [167], Train Loss: 0.6692947149276733\n",
            "Epoch [24], Batch [168], Train Loss: 0.8943006992340088\n",
            "Epoch [24], Batch [169], Train Loss: 0.8507696390151978\n",
            "Epoch [24], Batch [170], Train Loss: 0.587984561920166\n",
            "Epoch [24], Batch [171], Train Loss: 0.8397630453109741\n",
            "Epoch [24], Batch [172], Train Loss: 0.8726451396942139\n",
            "Epoch [24], Batch [173], Train Loss: 0.7049580812454224\n",
            "Epoch [24], Batch [174], Train Loss: 0.8734784722328186\n",
            "Epoch [24], Batch [175], Train Loss: 0.7999548316001892\n",
            "Epoch [24], Batch [176], Train Loss: 0.7109087705612183\n",
            "Epoch [24], Batch [177], Train Loss: 0.7912956476211548\n",
            "Epoch [24], Batch [178], Train Loss: 0.7770088911056519\n",
            "Epoch [24], Batch [179], Train Loss: 0.8619253635406494\n",
            "Epoch [24], Batch [180], Train Loss: 0.861724853515625\n",
            "Epoch [24], Batch [181], Train Loss: 0.8545074462890625\n",
            "Epoch [24], Batch [182], Train Loss: 0.758987545967102\n",
            "Epoch [24], Batch [183], Train Loss: 0.7653544545173645\n",
            "Epoch [24], Batch [184], Train Loss: 0.7411584854125977\n",
            "Epoch [24], Batch [185], Train Loss: 1.0160515308380127\n",
            "Epoch [24], Batch [186], Train Loss: 0.9227518439292908\n",
            "Epoch [24], Batch [187], Train Loss: 0.6522150039672852\n",
            "Epoch [24], Batch [188], Train Loss: 0.7406070828437805\n",
            "Epoch [24], Batch [189], Train Loss: 0.9761852025985718\n",
            "Epoch [24], Batch [190], Train Loss: 0.8111958503723145\n",
            "Epoch [24], Batch [191], Train Loss: 0.8383203744888306\n",
            "Epoch [24], Batch [192], Train Loss: 0.782899796962738\n",
            "Epoch [24], Batch [193], Train Loss: 0.9098954796791077\n",
            "Epoch [24], Batch [194], Train Loss: 0.8009587526321411\n",
            "Epoch [24], Batch [195], Train Loss: 0.732086181640625\n",
            "Epoch [24], Batch [196], Train Loss: 0.9266965389251709\n",
            "Epoch [24], Batch [197], Train Loss: 0.7296217083930969\n",
            "Epoch [24], Batch [198], Train Loss: 0.6943161487579346\n",
            "Epoch [24], Batch [199], Train Loss: 1.0154292583465576\n",
            "Epoch [24], Batch [200], Train Loss: 0.7951694130897522\n",
            "Epoch [24], Batch [201], Train Loss: 0.7428231239318848\n",
            "Epoch [24], Batch [202], Train Loss: 0.7364015579223633\n",
            "Epoch [24], Batch [203], Train Loss: 0.6481161713600159\n",
            "Epoch [24], Batch [204], Train Loss: 0.7036161422729492\n",
            "Epoch [24], Batch [205], Train Loss: 0.8238083124160767\n",
            "Epoch [24], Batch [206], Train Loss: 0.6159305572509766\n",
            "Epoch [24], Batch [207], Train Loss: 0.8526220917701721\n",
            "Epoch [24], Batch [208], Train Loss: 0.6996845006942749\n",
            "Epoch [24], Batch [209], Train Loss: 0.847709059715271\n",
            "Epoch [24], Batch [210], Train Loss: 0.9226332306861877\n",
            "Epoch [24], Batch [211], Train Loss: 0.7121700048446655\n",
            "Epoch [24], Batch [212], Train Loss: 1.0356546640396118\n",
            "Epoch [24], Batch [213], Train Loss: 0.8869375586509705\n",
            "Epoch [24], Batch [214], Train Loss: 0.7831524610519409\n",
            "Epoch [24], Batch [215], Train Loss: 0.9525843858718872\n",
            "Epoch [24], Batch [216], Train Loss: 0.949381411075592\n",
            "Epoch [24], Batch [217], Train Loss: 0.9669144749641418\n",
            "Epoch [24], Batch [218], Train Loss: 0.7669776082038879\n",
            "Epoch [24], Batch [219], Train Loss: 0.6809693574905396\n",
            "Epoch [24], Batch [220], Train Loss: 0.8412352204322815\n",
            "Epoch [24], Batch [221], Train Loss: 0.853885293006897\n",
            "Epoch [24], Batch [222], Train Loss: 0.7460349202156067\n",
            "Epoch [24], Batch [223], Train Loss: 0.8117300271987915\n",
            "Epoch [24], Batch [224], Train Loss: 0.7444666028022766\n",
            "Epoch [24], Batch [225], Train Loss: 0.805770993232727\n",
            "Epoch [24], Batch [226], Train Loss: 1.0748014450073242\n",
            "Epoch [24], Batch [227], Train Loss: 0.7774269580841064\n",
            "Epoch [24], Batch [228], Train Loss: 0.6246388554573059\n",
            "Epoch [24], Batch [229], Train Loss: 0.6583278179168701\n",
            "Epoch [24], Batch [230], Train Loss: 0.7552358508110046\n",
            "Epoch [24], Batch [231], Train Loss: 0.7365236878395081\n",
            "Epoch [24], Batch [232], Train Loss: 0.7611953020095825\n",
            "Epoch [24], Batch [233], Train Loss: 0.6872930526733398\n",
            "Epoch [24], Batch [234], Train Loss: 0.7020666003227234\n",
            "Epoch [24], Batch [235], Train Loss: 0.8478703498840332\n",
            "Epoch [24], Batch [236], Train Loss: 0.8034857511520386\n",
            "Epoch [24], Batch [237], Train Loss: 0.7672571539878845\n",
            "Epoch [24], Batch [238], Train Loss: 0.7157880663871765\n",
            "Epoch [24], Batch [239], Train Loss: 0.6230805516242981\n",
            "Epoch [24], Batch [240], Train Loss: 0.8846836090087891\n",
            "Epoch [24], Batch [241], Train Loss: 0.6364017128944397\n",
            "Epoch [24], Batch [242], Train Loss: 0.7670111656188965\n",
            "Epoch [24], Batch [243], Train Loss: 0.7166164517402649\n",
            "Epoch [24], Batch [244], Train Loss: 0.9741532802581787\n",
            "Epoch [24], Batch [245], Train Loss: 0.706013560295105\n",
            "Epoch [24], Batch [246], Train Loss: 0.868669331073761\n",
            "Epoch [24], Batch [247], Train Loss: 0.7275134921073914\n",
            "Epoch [24], Batch [248], Train Loss: 0.7340771555900574\n",
            "Epoch [24], Batch [249], Train Loss: 0.6249231696128845\n",
            "Epoch [24], Batch [250], Train Loss: 0.8647632598876953\n",
            "Epoch [24], Batch [251], Train Loss: 0.8176301717758179\n",
            "Epoch [24], Batch [252], Train Loss: 0.7438622713088989\n",
            "Epoch [24], Batch [253], Train Loss: 0.8911816477775574\n",
            "Epoch [24], Batch [254], Train Loss: 0.8211174607276917\n",
            "Epoch [24], Batch [255], Train Loss: 0.8659120798110962\n",
            "Epoch [24], Batch [256], Train Loss: 0.6962389945983887\n",
            "Epoch [24], Batch [257], Train Loss: 0.8975698947906494\n",
            "Epoch [24], Batch [258], Train Loss: 1.146252155303955\n",
            "Epoch [24], Batch [259], Train Loss: 0.7840636372566223\n",
            "Epoch [24], Batch [260], Train Loss: 0.8576532006263733\n",
            "Epoch [24], Batch [261], Train Loss: 0.9121432304382324\n",
            "Epoch [24], Batch [262], Train Loss: 0.5981071591377258\n",
            "Epoch [24], Batch [263], Train Loss: 0.7193200588226318\n",
            "Epoch [24], Batch [264], Train Loss: 0.6411023736000061\n",
            "Epoch [24], Batch [265], Train Loss: 0.6637961864471436\n",
            "Epoch [24], Batch [266], Train Loss: 0.828950822353363\n",
            "Epoch [24], Batch [267], Train Loss: 0.7488825917243958\n",
            "Epoch [24], Batch [268], Train Loss: 0.977979302406311\n",
            "Epoch [24], Batch [269], Train Loss: 0.7807396054267883\n",
            "Epoch [24], Batch [270], Train Loss: 0.6564946174621582\n",
            "Epoch [24], Batch [271], Train Loss: 0.8796913623809814\n",
            "Epoch [24], Batch [272], Train Loss: 0.8609391450881958\n",
            "Epoch [24], Batch [273], Train Loss: 0.9272188544273376\n",
            "Epoch [24], Batch [274], Train Loss: 0.7133849859237671\n",
            "Epoch [24], Batch [275], Train Loss: 0.7292230129241943\n",
            "Epoch [24], Batch [276], Train Loss: 0.7066611647605896\n",
            "Epoch [24], Batch [277], Train Loss: 0.633838415145874\n",
            "Epoch [24], Batch [278], Train Loss: 1.0839579105377197\n",
            "Epoch [24], Batch [279], Train Loss: 0.7495670914649963\n",
            "Epoch [24], Batch [280], Train Loss: 0.6876393556594849\n",
            "Epoch [24], Batch [281], Train Loss: 0.8684581518173218\n",
            "Epoch [24], Batch [282], Train Loss: 0.7546966671943665\n",
            "Epoch [24], Batch [283], Train Loss: 0.8138291835784912\n",
            "Epoch [24], Batch [284], Train Loss: 0.9058240056037903\n",
            "Epoch [24], Batch [285], Train Loss: 0.6289287805557251\n",
            "Epoch [24], Batch [286], Train Loss: 0.6379560828208923\n",
            "Epoch [24], Batch [287], Train Loss: 0.9398243427276611\n",
            "Epoch [24], Batch [288], Train Loss: 0.9303337931632996\n",
            "Epoch [24], Batch [289], Train Loss: 0.6014990210533142\n",
            "Epoch [24], Batch [290], Train Loss: 0.7668609619140625\n",
            "Epoch [24], Batch [291], Train Loss: 0.616809606552124\n",
            "Epoch [24], Batch [292], Train Loss: 0.8250433206558228\n",
            "Epoch [24], Batch [293], Train Loss: 0.82320237159729\n",
            "Epoch [24], Batch [294], Train Loss: 0.6821290850639343\n",
            "Epoch [24], Batch [295], Train Loss: 0.7771338224411011\n",
            "Epoch [24], Batch [296], Train Loss: 0.5703179240226746\n",
            "Epoch [24], Batch [297], Train Loss: 0.7801090478897095\n",
            "Epoch [24], Batch [298], Train Loss: 0.6033970713615417\n",
            "Epoch [24], Batch [299], Train Loss: 0.9778332710266113\n",
            "Epoch [24], Batch [300], Train Loss: 0.8324604034423828\n",
            "Epoch [24], Batch [301], Train Loss: 0.728973388671875\n",
            "Epoch [24], Batch [302], Train Loss: 0.801632285118103\n",
            "Epoch [24], Batch [303], Train Loss: 0.6849676370620728\n",
            "Epoch [24], Batch [304], Train Loss: 0.9383718371391296\n",
            "Epoch [24], Batch [305], Train Loss: 1.0460790395736694\n",
            "Epoch [24], Batch [306], Train Loss: 0.8502211570739746\n",
            "Epoch [24], Batch [307], Train Loss: 0.7636757493019104\n",
            "Epoch [24], Batch [308], Train Loss: 0.7948557138442993\n",
            "Epoch [24], Batch [309], Train Loss: 0.775062620639801\n",
            "Epoch [24], Batch [310], Train Loss: 0.7699713110923767\n",
            "Epoch [24], Batch [311], Train Loss: 0.8958857655525208\n",
            "Epoch [24], Batch [312], Train Loss: 0.7461643815040588\n",
            "Epoch [24], Batch [313], Train Loss: 0.7956465482711792\n",
            "Epoch [24], Batch [314], Train Loss: 0.8867411613464355\n",
            "Epoch [24], Batch [315], Train Loss: 0.6592499613761902\n",
            "Epoch [24], Batch [316], Train Loss: 0.7445406317710876\n",
            "Epoch [24], Batch [317], Train Loss: 0.8145595788955688\n",
            "Epoch [24], Batch [318], Train Loss: 0.9767323732376099\n",
            "Epoch [24], Batch [319], Train Loss: 0.9586381316184998\n",
            "Epoch [24], Batch [320], Train Loss: 0.6728842258453369\n",
            "Epoch [24], Batch [321], Train Loss: 0.787433922290802\n",
            "Epoch [24], Batch [322], Train Loss: 0.8134992718696594\n",
            "Epoch [24], Batch [323], Train Loss: 0.730818510055542\n",
            "Epoch [24], Batch [324], Train Loss: 0.8061595559120178\n",
            "Epoch [24], Batch [325], Train Loss: 0.8967145681381226\n",
            "Epoch [24], Batch [326], Train Loss: 0.990536093711853\n",
            "Epoch [24], Batch [327], Train Loss: 1.0603026151657104\n",
            "Epoch [24], Batch [328], Train Loss: 0.6418167948722839\n",
            "Epoch [24], Batch [329], Train Loss: 0.8390551805496216\n",
            "Epoch [24], Batch [330], Train Loss: 0.8762260675430298\n",
            "Epoch [24], Batch [331], Train Loss: 0.927409827709198\n",
            "Epoch [24], Batch [332], Train Loss: 0.798689067363739\n",
            "Epoch [24], Batch [333], Train Loss: 0.700458288192749\n",
            "Epoch [24], Batch [334], Train Loss: 0.6318808197975159\n",
            "Epoch [24], Batch [335], Train Loss: 0.6751276850700378\n",
            "Epoch [24], Batch [336], Train Loss: 0.7469583749771118\n",
            "Epoch [24], Batch [337], Train Loss: 0.9756386876106262\n",
            "Epoch [24], Batch [338], Train Loss: 1.0019949674606323\n",
            "Epoch [24], Batch [339], Train Loss: 0.8019253611564636\n",
            "Epoch [24], Batch [340], Train Loss: 0.6321411728858948\n",
            "Epoch [24], Batch [341], Train Loss: 0.7668635845184326\n",
            "Epoch [24], Batch [342], Train Loss: 0.7573242783546448\n",
            "Epoch [24], Batch [343], Train Loss: 0.8759042024612427\n",
            "Epoch [24], Batch [344], Train Loss: 0.9849651455879211\n",
            "Epoch [24], Batch [345], Train Loss: 1.2031493186950684\n",
            "Epoch [24], Batch [346], Train Loss: 0.79572993516922\n",
            "Epoch [24], Batch [347], Train Loss: 0.7262169122695923\n",
            "Epoch [24], Batch [348], Train Loss: 0.7611433863639832\n",
            "Epoch [24], Batch [349], Train Loss: 1.0101683139801025\n",
            "Epoch [24], Batch [350], Train Loss: 0.685518205165863\n",
            "Epoch [24], Batch [351], Train Loss: 0.8239202499389648\n",
            "Epoch [24], Batch [352], Train Loss: 0.7853440642356873\n",
            "Epoch [24], Batch [353], Train Loss: 0.7542328834533691\n",
            "Epoch [24], Batch [354], Train Loss: 0.6848185658454895\n",
            "Epoch [24], Batch [355], Train Loss: 0.8346038460731506\n",
            "Epoch [24], Batch [356], Train Loss: 0.7341136932373047\n",
            "Epoch [24], Batch [357], Train Loss: 0.7159788012504578\n",
            "Epoch [24], Batch [358], Train Loss: 0.6443842053413391\n",
            "Epoch [24], Batch [359], Train Loss: 0.6070924401283264\n",
            "Epoch [24], Batch [360], Train Loss: 0.6682640314102173\n",
            "Epoch [24], Batch [361], Train Loss: 0.7856874465942383\n",
            "Epoch [24], Batch [362], Train Loss: 0.6583884954452515\n",
            "Epoch [24], Batch [363], Train Loss: 0.8798161745071411\n",
            "Epoch [24], Batch [364], Train Loss: 0.7588672637939453\n",
            "Epoch [24], Batch [365], Train Loss: 0.7993031740188599\n",
            "Epoch [24], Batch [366], Train Loss: 0.8684645891189575\n",
            "Epoch [24], Batch [367], Train Loss: 0.8459343314170837\n",
            "Epoch [24], Batch [368], Train Loss: 0.8185035586357117\n",
            "Epoch [24], Batch [369], Train Loss: 0.6064459681510925\n",
            "Epoch [24], Batch [370], Train Loss: 0.7976605296134949\n",
            "Epoch [24], Batch [371], Train Loss: 0.7279192209243774\n",
            "Epoch [24], Batch [372], Train Loss: 0.7244299650192261\n",
            "Epoch [24], Batch [373], Train Loss: 0.8343968987464905\n",
            "Epoch [24], Batch [374], Train Loss: 0.7255605459213257\n",
            "Epoch [24], Batch [375], Train Loss: 0.6605496406555176\n",
            "Epoch [24], Batch [376], Train Loss: 0.8965862989425659\n",
            "Epoch [24], Batch [377], Train Loss: 0.8588374257087708\n",
            "Epoch [24], Batch [378], Train Loss: 0.5741601586341858\n",
            "Epoch [24], Batch [379], Train Loss: 0.6881705522537231\n",
            "Epoch [24], Batch [380], Train Loss: 0.7813951969146729\n",
            "Epoch [24], Batch [381], Train Loss: 0.7542367577552795\n",
            "Epoch [24], Batch [382], Train Loss: 0.6215790510177612\n",
            "Epoch [24], Batch [383], Train Loss: 0.8264015913009644\n",
            "Epoch [24], Batch [384], Train Loss: 0.8470604419708252\n",
            "Epoch [24], Batch [385], Train Loss: 0.6853472590446472\n",
            "Epoch [24], Batch [386], Train Loss: 0.8106667995452881\n",
            "Epoch [24], Batch [387], Train Loss: 0.5368233323097229\n",
            "Epoch [24], Batch [388], Train Loss: 0.7588685750961304\n",
            "Epoch [24], Batch [389], Train Loss: 0.5810811519622803\n",
            "Epoch [24], Batch [390], Train Loss: 1.0217920541763306\n",
            "Epoch [24], Batch [391], Train Loss: 0.7679774165153503\n",
            "Epoch [24], Batch [392], Train Loss: 0.7581812143325806\n",
            "Epoch [24], Batch [393], Train Loss: 0.8916234970092773\n",
            "Epoch [24], Batch [394], Train Loss: 0.7510560154914856\n",
            "Epoch [24], Batch [395], Train Loss: 0.5944472551345825\n",
            "Epoch [24], Batch [396], Train Loss: 0.7579414248466492\n",
            "Epoch [24], Batch [397], Train Loss: 0.670257568359375\n",
            "Epoch [24], Batch [398], Train Loss: 0.6869146823883057\n",
            "Epoch [24], Batch [399], Train Loss: 0.6915006041526794\n",
            "Epoch [24], Batch [400], Train Loss: 0.8542045950889587\n",
            "Epoch [24], Batch [401], Train Loss: 0.9432341456413269\n",
            "Epoch [24], Batch [402], Train Loss: 0.9252804517745972\n",
            "Epoch [24], Batch [403], Train Loss: 0.6671391725540161\n",
            "Epoch [24], Batch [404], Train Loss: 0.866809070110321\n",
            "Epoch [24], Batch [405], Train Loss: 0.6990594863891602\n",
            "Epoch [24], Batch [406], Train Loss: 0.8457114696502686\n",
            "Epoch [24], Batch [407], Train Loss: 0.8426065444946289\n",
            "Epoch [24], Batch [408], Train Loss: 0.8176919221878052\n",
            "Epoch [24], Batch [409], Train Loss: 0.6960620284080505\n",
            "Epoch [24], Batch [410], Train Loss: 0.6643760204315186\n",
            "Epoch [24], Batch [411], Train Loss: 0.7584223747253418\n",
            "Epoch [24], Batch [412], Train Loss: 0.7233250737190247\n",
            "Epoch [24], Batch [413], Train Loss: 0.6066298484802246\n",
            "Epoch [24], Batch [414], Train Loss: 0.755456805229187\n",
            "Epoch [24], Batch [415], Train Loss: 1.0295952558517456\n",
            "Epoch [24], Batch [416], Train Loss: 1.1775258779525757\n",
            "Epoch [24], Batch [417], Train Loss: 0.7253114581108093\n",
            "Epoch [24], Batch [418], Train Loss: 0.7014297842979431\n",
            "Epoch [24], Batch [419], Train Loss: 0.8059219717979431\n",
            "Epoch [24], Batch [420], Train Loss: 0.9233817458152771\n",
            "Epoch [24], Batch [421], Train Loss: 0.6850449442863464\n",
            "Epoch [24], Batch [422], Train Loss: 0.7645274996757507\n",
            "Epoch [24], Batch [423], Train Loss: 0.6245652437210083\n",
            "Epoch [24], Batch [424], Train Loss: 0.8560208082199097\n",
            "Epoch [24], Batch [425], Train Loss: 0.8027557134628296\n",
            "Epoch [24], Batch [426], Train Loss: 0.8124483823776245\n",
            "Epoch [24], Batch [427], Train Loss: 0.7108376026153564\n",
            "Epoch [24], Batch [428], Train Loss: 0.5693914890289307\n",
            "Epoch [24], Batch [429], Train Loss: 0.7558453679084778\n",
            "Epoch [24], Batch [430], Train Loss: 0.8000286817550659\n",
            "Epoch [24], Batch [431], Train Loss: 0.8149454593658447\n",
            "Epoch [24], Batch [432], Train Loss: 0.7340952157974243\n",
            "Epoch [24], Batch [433], Train Loss: 0.7716980576515198\n",
            "Epoch [24], Batch [434], Train Loss: 0.6733962893486023\n",
            "Epoch [24], Batch [435], Train Loss: 0.7698454260826111\n",
            "Epoch [24], Batch [436], Train Loss: 0.8821741938591003\n",
            "Epoch [24], Batch [437], Train Loss: 0.7549657225608826\n",
            "Epoch [24], Batch [438], Train Loss: 1.2145098447799683\n",
            "Epoch [24], Batch [439], Train Loss: 0.9055318832397461\n",
            "Epoch [24], Batch [440], Train Loss: 0.7328089475631714\n",
            "Epoch [24], Batch [441], Train Loss: 0.7817954421043396\n",
            "Epoch [24], Batch [442], Train Loss: 0.6100988984107971\n",
            "Epoch [24], Batch [443], Train Loss: 0.7298780679702759\n",
            "Epoch [24], Batch [444], Train Loss: 0.7769120335578918\n",
            "Epoch [24], Batch [445], Train Loss: 0.9730280637741089\n",
            "Epoch [24], Batch [446], Train Loss: 0.7194657325744629\n",
            "Epoch [24], Batch [447], Train Loss: 0.8391842246055603\n",
            "Epoch [24], Batch [448], Train Loss: 0.931430995464325\n",
            "Epoch [24], Batch [449], Train Loss: 0.7766025066375732\n",
            "Epoch [24], Batch [450], Train Loss: 0.6473075151443481\n",
            "Epoch [24], Batch [451], Train Loss: 0.8933541178703308\n",
            "Epoch [24], Batch [452], Train Loss: 0.903315007686615\n",
            "Epoch [24], Batch [453], Train Loss: 0.8222514390945435\n",
            "Epoch [24], Batch [454], Train Loss: 0.8596566915512085\n",
            "Epoch [24], Batch [455], Train Loss: 0.7898895144462585\n",
            "Epoch [24], Batch [456], Train Loss: 0.7367267608642578\n",
            "Epoch [24], Batch [457], Train Loss: 0.706269383430481\n",
            "Epoch [24], Batch [458], Train Loss: 0.5108075141906738\n",
            "Epoch [24], Batch [459], Train Loss: 0.7609393000602722\n",
            "Epoch [24], Batch [460], Train Loss: 0.8014658689498901\n",
            "Epoch [24], Batch [461], Train Loss: 0.8651142120361328\n",
            "Epoch [24], Batch [462], Train Loss: 0.8194183111190796\n",
            "Epoch [24], Batch [463], Train Loss: 0.743735134601593\n",
            "Epoch [24], Batch [464], Train Loss: 0.5335196256637573\n",
            "Epoch [24], Batch [465], Train Loss: 0.7799031138420105\n",
            "Epoch [24], Batch [466], Train Loss: 0.6205923557281494\n",
            "Epoch [24], Batch [467], Train Loss: 0.7135138511657715\n",
            "Epoch [24], Batch [468], Train Loss: 0.8304294347763062\n",
            "Epoch [24], Batch [469], Train Loss: 0.7960687875747681\n",
            "Epoch [24], Batch [470], Train Loss: 0.7997118234634399\n",
            "Epoch [24], Batch [471], Train Loss: 0.8581175208091736\n",
            "Epoch [24], Batch [472], Train Loss: 0.5903131365776062\n",
            "Epoch [24], Batch [473], Train Loss: 0.8028148412704468\n",
            "Epoch [24], Batch [474], Train Loss: 0.7276952266693115\n",
            "Epoch [24], Batch [475], Train Loss: 0.746544361114502\n",
            "Epoch [24], Batch [476], Train Loss: 0.8909564018249512\n",
            "Epoch [24], Batch [477], Train Loss: 0.6799682974815369\n",
            "Epoch [24], Batch [478], Train Loss: 0.6908982992172241\n",
            "Epoch [24], Batch [479], Train Loss: 0.8995780944824219\n",
            "Epoch [24], Batch [480], Train Loss: 0.6823443174362183\n",
            "Epoch [24], Batch [481], Train Loss: 0.7598580718040466\n",
            "Epoch [24], Batch [482], Train Loss: 0.8185080885887146\n",
            "Epoch [24], Batch [483], Train Loss: 0.8000439405441284\n",
            "Epoch [24], Batch [484], Train Loss: 0.723999559879303\n",
            "Epoch [24], Batch [485], Train Loss: 0.9268408417701721\n",
            "Epoch [24], Batch [486], Train Loss: 0.5998095870018005\n",
            "Epoch [24], Batch [487], Train Loss: 0.6936383247375488\n",
            "Epoch [24], Batch [488], Train Loss: 0.7949708700180054\n",
            "Epoch [24], Batch [489], Train Loss: 0.6864133477210999\n",
            "Epoch [24], Batch [490], Train Loss: 0.8251820802688599\n",
            "Epoch [24], Batch [491], Train Loss: 0.6575493812561035\n",
            "Epoch [24], Batch [492], Train Loss: 0.7719013094902039\n",
            "Epoch [24], Batch [493], Train Loss: 0.7989482283592224\n",
            "Epoch [24], Batch [494], Train Loss: 0.8776025772094727\n",
            "Epoch [24], Batch [495], Train Loss: 0.9232878088951111\n",
            "Epoch [24], Batch [496], Train Loss: 0.7216339707374573\n",
            "Epoch [24], Batch [497], Train Loss: 0.760758101940155\n",
            "Epoch [24], Batch [498], Train Loss: 0.7529985904693604\n",
            "Epoch [24], Batch [499], Train Loss: 0.7660930752754211\n",
            "Epoch [24], Batch [500], Train Loss: 0.726450502872467\n",
            "Epoch [24], Batch [501], Train Loss: 0.9789440035820007\n",
            "Epoch [24], Batch [502], Train Loss: 0.9402313828468323\n",
            "Epoch [24], Batch [503], Train Loss: 0.800301194190979\n",
            "Epoch [24], Batch [504], Train Loss: 0.6794967651367188\n",
            "Epoch [24], Batch [505], Train Loss: 0.8960406184196472\n",
            "Epoch [24], Batch [506], Train Loss: 0.6110907196998596\n",
            "Epoch [24], Batch [507], Train Loss: 0.7005695104598999\n",
            "Epoch [24], Batch [508], Train Loss: 0.7408726811408997\n",
            "Epoch [24], Batch [509], Train Loss: 0.6994959115982056\n",
            "Epoch [24], Batch [510], Train Loss: 0.5613206624984741\n",
            "Epoch [24], Batch [511], Train Loss: 0.8713541626930237\n",
            "Epoch [24], Batch [512], Train Loss: 0.9270445108413696\n",
            "Epoch [24], Batch [513], Train Loss: 0.7556686401367188\n",
            "Epoch [24], Batch [514], Train Loss: 0.6604530811309814\n",
            "Epoch [24], Batch [515], Train Loss: 0.8384963274002075\n",
            "Epoch [24], Batch [516], Train Loss: 0.5868356227874756\n",
            "Epoch [24], Batch [517], Train Loss: 0.7303668260574341\n",
            "Epoch [24], Batch [518], Train Loss: 0.5840628743171692\n",
            "Epoch [24], Batch [519], Train Loss: 0.7970066070556641\n",
            "Epoch [24], Batch [520], Train Loss: 0.5979354977607727\n",
            "Epoch [24], Batch [521], Train Loss: 0.6552129983901978\n",
            "Epoch [24], Batch [522], Train Loss: 0.7130818367004395\n",
            "Epoch [24], Batch [523], Train Loss: 0.951643705368042\n",
            "Epoch [24], Batch [524], Train Loss: 0.5986524820327759\n",
            "Epoch [24], Batch [525], Train Loss: 0.9139598608016968\n",
            "Epoch [24], Batch [526], Train Loss: 0.8005446195602417\n",
            "Epoch [24], Batch [527], Train Loss: 0.6034054756164551\n",
            "Epoch [24], Batch [528], Train Loss: 0.8297209739685059\n",
            "Epoch [24], Batch [529], Train Loss: 0.9663492441177368\n",
            "Epoch [24], Batch [530], Train Loss: 0.8127471208572388\n",
            "Epoch [24], Batch [531], Train Loss: 0.8626258969306946\n",
            "Epoch [24], Batch [532], Train Loss: 0.7483304738998413\n",
            "Epoch [24], Batch [533], Train Loss: 0.8992651104927063\n",
            "Epoch [24], Batch [534], Train Loss: 0.9248461127281189\n",
            "Epoch [24], Batch [535], Train Loss: 0.9297333359718323\n",
            "Epoch [24], Batch [536], Train Loss: 1.1652801036834717\n",
            "Epoch [24], Batch [537], Train Loss: 0.8635438084602356\n",
            "Epoch [24], Batch [538], Train Loss: 0.7862303256988525\n",
            "Epoch [24], Batch [539], Train Loss: 0.9349757432937622\n",
            "Epoch [24], Batch [540], Train Loss: 0.8589996099472046\n",
            "Epoch [24], Batch [541], Train Loss: 0.8853075504302979\n",
            "Epoch [24], Batch [542], Train Loss: 0.7030118107795715\n",
            "Epoch [24], Batch [543], Train Loss: 0.7789903879165649\n",
            "Epoch [24], Batch [544], Train Loss: 0.8679525852203369\n",
            "Epoch [24], Batch [545], Train Loss: 0.8804778456687927\n",
            "Epoch [24], Batch [546], Train Loss: 0.5940870046615601\n",
            "Epoch [24], Batch [547], Train Loss: 0.70798259973526\n",
            "Epoch [24], Batch [548], Train Loss: 0.8320426940917969\n",
            "Epoch [24], Batch [549], Train Loss: 0.6333252191543579\n",
            "Epoch [24], Batch [550], Train Loss: 0.62986820936203\n",
            "Epoch [24], Batch [551], Train Loss: 0.794030487537384\n",
            "Epoch [24], Batch [552], Train Loss: 1.1363606452941895\n",
            "Epoch [24], Batch [553], Train Loss: 0.866702675819397\n",
            "Epoch [24], Batch [554], Train Loss: 0.7825443148612976\n",
            "Epoch [24], Batch [555], Train Loss: 0.7059682011604309\n",
            "Epoch [24], Batch [556], Train Loss: 0.7165971398353577\n",
            "Epoch [24], Batch [557], Train Loss: 0.731573760509491\n",
            "Epoch [24], Batch [558], Train Loss: 0.5662924647331238\n",
            "Epoch [24], Batch [559], Train Loss: 0.7911339402198792\n",
            "Epoch [24], Batch [560], Train Loss: 0.7686954140663147\n",
            "Epoch [24], Batch [561], Train Loss: 0.7262908816337585\n",
            "Epoch [24], Batch [562], Train Loss: 0.7340367436408997\n",
            "Epoch [24], Batch [563], Train Loss: 0.669177234172821\n",
            "Epoch [24], Batch [564], Train Loss: 1.0252865552902222\n",
            "Epoch [24], Batch [565], Train Loss: 0.7005458474159241\n",
            "Epoch [24], Batch [566], Train Loss: 0.7722991704940796\n",
            "Epoch [24], Batch [567], Train Loss: 0.7704119682312012\n",
            "Epoch [24], Batch [568], Train Loss: 0.7312838435173035\n",
            "Epoch [24], Batch [569], Train Loss: 0.8291730880737305\n",
            "Epoch [24], Batch [570], Train Loss: 0.6520991921424866\n",
            "Epoch [24], Batch [571], Train Loss: 0.8291481733322144\n",
            "Epoch [24], Batch [572], Train Loss: 0.7408558130264282\n",
            "Epoch [24], Batch [573], Train Loss: 0.7430626153945923\n",
            "Epoch [24], Batch [574], Train Loss: 0.6680315136909485\n",
            "Epoch [24], Batch [575], Train Loss: 0.5936444401741028\n",
            "Epoch [24], Batch [576], Train Loss: 1.0846686363220215\n",
            "Epoch [24], Batch [577], Train Loss: 0.7700068950653076\n",
            "Epoch [24], Batch [578], Train Loss: 0.8020721673965454\n",
            "Epoch [24], Batch [579], Train Loss: 0.8625295162200928\n",
            "Epoch [24], Batch [580], Train Loss: 0.6709660291671753\n",
            "Epoch [24], Batch [581], Train Loss: 0.7394933700561523\n",
            "Epoch [24], Batch [582], Train Loss: 0.8632581830024719\n",
            "Epoch [24], Batch [583], Train Loss: 0.8817753791809082\n",
            "Epoch [24], Batch [584], Train Loss: 0.8464628458023071\n",
            "Epoch [24], Batch [585], Train Loss: 0.6203527450561523\n",
            "Epoch [24], Batch [586], Train Loss: 0.8343675136566162\n",
            "Epoch [24], Batch [587], Train Loss: 0.6968399882316589\n",
            "Epoch [24], Batch [588], Train Loss: 0.6505639553070068\n",
            "Epoch [24], Batch [589], Train Loss: 0.7057979106903076\n",
            "Epoch [24], Batch [590], Train Loss: 0.6998335719108582\n",
            "Epoch [24], Batch [591], Train Loss: 0.8065884113311768\n",
            "Epoch [24], Batch [592], Train Loss: 0.7732176780700684\n",
            "Epoch [24], Batch [593], Train Loss: 0.6359440088272095\n",
            "Epoch [24], Batch [594], Train Loss: 0.6761921644210815\n",
            "Epoch [24], Batch [595], Train Loss: 0.8194535970687866\n",
            "Epoch [24], Batch [596], Train Loss: 0.8359501361846924\n",
            "Epoch [24], Batch [597], Train Loss: 0.6941938400268555\n",
            "Epoch [24], Batch [598], Train Loss: 0.8555219173431396\n",
            "Epoch [24], Batch [599], Train Loss: 0.8187915682792664\n",
            "Epoch [24], Batch [600], Train Loss: 0.8239673376083374\n",
            "Epoch [24], Batch [601], Train Loss: 0.7570452690124512\n",
            "Epoch [24], Batch [602], Train Loss: 0.7319464087486267\n",
            "Epoch [24], Batch [603], Train Loss: 0.8704839944839478\n",
            "Epoch [24], Batch [604], Train Loss: 0.877142608165741\n",
            "Epoch [24], Batch [605], Train Loss: 0.714608907699585\n",
            "Epoch [24], Batch [606], Train Loss: 0.9623725414276123\n",
            "Epoch [24], Batch [607], Train Loss: 0.9941121339797974\n",
            "Epoch [24], Batch [608], Train Loss: 0.6794942617416382\n",
            "Epoch [24], Batch [609], Train Loss: 0.9218467473983765\n",
            "Epoch [24], Batch [610], Train Loss: 1.1603578329086304\n",
            "Epoch [24], Batch [611], Train Loss: 0.8579338788986206\n",
            "Epoch [24], Batch [612], Train Loss: 0.6564973592758179\n",
            "Epoch [24], Batch [613], Train Loss: 0.7776640057563782\n",
            "Epoch [24], Batch [614], Train Loss: 0.6659924983978271\n",
            "Epoch [24], Batch [615], Train Loss: 0.8938661813735962\n",
            "Epoch [24], Batch [616], Train Loss: 0.821678102016449\n",
            "Epoch [24], Batch [617], Train Loss: 0.8976991772651672\n",
            "Epoch [24], Batch [618], Train Loss: 0.86869215965271\n",
            "Epoch [24], Batch [619], Train Loss: 0.872214138507843\n",
            "Epoch [24], Batch [620], Train Loss: 0.7216678857803345\n",
            "Epoch [24], Batch [621], Train Loss: 0.8048409819602966\n",
            "Epoch [24], Batch [622], Train Loss: 0.7722326517105103\n",
            "Epoch [24], Batch [623], Train Loss: 0.6322010159492493\n",
            "Epoch [24], Batch [624], Train Loss: 0.947602391242981\n",
            "Epoch [24], Batch [625], Train Loss: 0.7350571155548096\n",
            "Epoch [24], Batch [626], Train Loss: 0.8809303045272827\n",
            "Epoch [24], Batch [627], Train Loss: 0.7631847262382507\n",
            "Epoch [24], Batch [628], Train Loss: 0.8741734623908997\n",
            "Epoch [24], Batch [629], Train Loss: 0.6955546140670776\n",
            "Epoch [24], Batch [630], Train Loss: 0.7178575396537781\n",
            "Epoch [24], Batch [631], Train Loss: 0.9246315360069275\n",
            "Epoch [24], Batch [632], Train Loss: 0.8085190653800964\n",
            "Epoch [24], Batch [633], Train Loss: 0.7792500257492065\n",
            "Epoch [24], Batch [634], Train Loss: 0.6464570760726929\n",
            "Epoch [24], Batch [635], Train Loss: 0.7282684445381165\n",
            "Epoch [24], Batch [636], Train Loss: 0.6271331310272217\n",
            "Epoch [24], Batch [637], Train Loss: 0.6537061333656311\n",
            "Epoch [24], Batch [638], Train Loss: 0.8782341480255127\n",
            "Epoch [24], Batch [639], Train Loss: 0.698962390422821\n",
            "Epoch [24], Batch [640], Train Loss: 0.651236891746521\n",
            "Epoch [24], Batch [641], Train Loss: 0.783214807510376\n",
            "Epoch [24], Batch [642], Train Loss: 0.8331716060638428\n",
            "Epoch [24], Batch [643], Train Loss: 0.6796180009841919\n",
            "Epoch [24], Batch [644], Train Loss: 0.6183524131774902\n",
            "Epoch [24], Batch [645], Train Loss: 0.951894223690033\n",
            "Epoch [24], Batch [646], Train Loss: 0.7381720542907715\n",
            "Epoch [24], Batch [647], Train Loss: 0.6984065771102905\n",
            "Epoch [24], Batch [648], Train Loss: 0.6350991129875183\n",
            "Epoch [24], Batch [649], Train Loss: 1.4860014915466309\n",
            "Epoch [24], Batch [650], Train Loss: 0.7365471124649048\n",
            "Epoch [24], Batch [651], Train Loss: 0.5831541419029236\n",
            "Epoch [24], Batch [652], Train Loss: 0.7378920316696167\n",
            "Epoch [24], Batch [653], Train Loss: 0.5883364677429199\n",
            "Epoch [24], Batch [654], Train Loss: 0.8871816396713257\n",
            "Epoch [24], Batch [655], Train Loss: 0.8669153451919556\n",
            "Epoch [24], Batch [656], Train Loss: 0.6822901368141174\n",
            "Epoch [24], Batch [657], Train Loss: 0.804487943649292\n",
            "Epoch [24], Batch [658], Train Loss: 0.7312878370285034\n",
            "Epoch [24], Batch [659], Train Loss: 0.8371620178222656\n",
            "Epoch [24], Batch [660], Train Loss: 0.828630268573761\n",
            "Epoch [24], Batch [661], Train Loss: 0.7900510430335999\n",
            "Epoch [24], Batch [662], Train Loss: 1.013127326965332\n",
            "Epoch [24], Batch [663], Train Loss: 0.6170753240585327\n",
            "Epoch [24], Batch [664], Train Loss: 0.7825533151626587\n",
            "Epoch [24], Batch [665], Train Loss: 0.7928937077522278\n",
            "Epoch [24], Batch [666], Train Loss: 0.7802221775054932\n",
            "Epoch [24], Batch [667], Train Loss: 0.8446154594421387\n",
            "Epoch [24], Batch [668], Train Loss: 0.8242946863174438\n",
            "Epoch [24], Batch [669], Train Loss: 0.6817928552627563\n",
            "Epoch [24], Batch [670], Train Loss: 0.6338534355163574\n",
            "Epoch [24], Batch [671], Train Loss: 1.050632119178772\n",
            "Epoch [24], Batch [672], Train Loss: 0.8008416295051575\n",
            "Epoch [24], Batch [673], Train Loss: 0.8409916758537292\n",
            "Epoch [24], Batch [674], Train Loss: 0.520989716053009\n",
            "Epoch [24], Batch [675], Train Loss: 0.7928767204284668\n",
            "Epoch [24], Batch [676], Train Loss: 0.8604139089584351\n",
            "Epoch [24], Batch [677], Train Loss: 0.7348189353942871\n",
            "Epoch [24], Batch [678], Train Loss: 0.861420214176178\n",
            "Epoch [24], Batch [679], Train Loss: 0.7643073201179504\n",
            "Epoch [24], Batch [680], Train Loss: 0.6792416572570801\n",
            "Epoch [24], Batch [681], Train Loss: 1.068116545677185\n",
            "Epoch [24], Batch [682], Train Loss: 0.7043375372886658\n",
            "Epoch [24], Batch [683], Train Loss: 0.8043485283851624\n",
            "Epoch [24], Batch [684], Train Loss: 0.7799820899963379\n",
            "Epoch [24], Batch [685], Train Loss: 0.8885315656661987\n",
            "Epoch [24], Batch [686], Train Loss: 0.7340497970581055\n",
            "Epoch [24], Batch [687], Train Loss: 0.8421821594238281\n",
            "Epoch [24], Batch [688], Train Loss: 0.9536135792732239\n",
            "Epoch [24], Batch [689], Train Loss: 0.7321004867553711\n",
            "Epoch [24], Batch [690], Train Loss: 0.7093358635902405\n",
            "Epoch [24], Batch [691], Train Loss: 0.7474647760391235\n",
            "Epoch [24], Batch [692], Train Loss: 0.9482796788215637\n",
            "Epoch [24], Batch [693], Train Loss: 0.7297261953353882\n",
            "Epoch [24], Batch [694], Train Loss: 0.871335506439209\n",
            "Epoch [24], Batch [695], Train Loss: 0.6422948241233826\n",
            "Epoch [24], Batch [696], Train Loss: 0.6618680953979492\n",
            "Epoch [24], Batch [697], Train Loss: 0.5178524255752563\n",
            "Epoch [24], Batch [698], Train Loss: 0.7903004288673401\n",
            "Epoch [24], Batch [699], Train Loss: 0.7681020498275757\n",
            "Epoch [24], Batch [700], Train Loss: 0.6709801554679871\n",
            "Epoch [24], Batch [701], Train Loss: 0.7940474152565002\n",
            "Epoch [24], Batch [702], Train Loss: 0.813445508480072\n",
            "Epoch [24], Batch [703], Train Loss: 0.6242130994796753\n",
            "Epoch [24], Batch [704], Train Loss: 0.5381328463554382\n",
            "Epoch [24], Batch [705], Train Loss: 0.5927801132202148\n",
            "Epoch [24], Batch [706], Train Loss: 0.7897544503211975\n",
            "Epoch [24], Batch [707], Train Loss: 0.5608487129211426\n",
            "Epoch [24], Batch [708], Train Loss: 0.8793951272964478\n",
            "Epoch [24], Batch [709], Train Loss: 0.7024071216583252\n",
            "Epoch [24], Batch [710], Train Loss: 0.6002574563026428\n",
            "Epoch [24], Batch [711], Train Loss: 0.7476357221603394\n",
            "Epoch [24], Batch [712], Train Loss: 0.7112557888031006\n",
            "Epoch [24], Batch [713], Train Loss: 0.6604291200637817\n",
            "Epoch [24], Batch [714], Train Loss: 0.8873035907745361\n",
            "Epoch [24], Batch [715], Train Loss: 0.8619297742843628\n",
            "Epoch [24], Batch [716], Train Loss: 1.2123357057571411\n",
            "Epoch [24], Batch [717], Train Loss: 0.7572580575942993\n",
            "Epoch [24], Batch [718], Train Loss: 0.865027666091919\n",
            "Epoch [24], Batch [719], Train Loss: 0.8990278244018555\n",
            "Epoch [24], Batch [720], Train Loss: 0.6931322813034058\n",
            "Epoch [24], Batch [721], Train Loss: 0.8496188521385193\n",
            "Epoch [24], Batch [722], Train Loss: 0.8175930976867676\n",
            "Epoch [24], Batch [723], Train Loss: 0.639124870300293\n",
            "Epoch [24], Batch [724], Train Loss: 0.7525175213813782\n",
            "Epoch [24], Batch [725], Train Loss: 0.6201820373535156\n",
            "Epoch [24], Batch [726], Train Loss: 0.6022157073020935\n",
            "Epoch [24], Batch [727], Train Loss: 0.7909043431282043\n",
            "Epoch [24], Batch [728], Train Loss: 0.769445538520813\n",
            "Epoch [24], Batch [729], Train Loss: 0.6798978447914124\n",
            "Epoch [24], Batch [730], Train Loss: 0.730412483215332\n",
            "Epoch [24], Batch [731], Train Loss: 0.7558393478393555\n",
            "Epoch [24], Batch [732], Train Loss: 0.7687491178512573\n",
            "Epoch [24], Batch [733], Train Loss: 0.8184588551521301\n",
            "Epoch [24], Batch [734], Train Loss: 0.7500067353248596\n",
            "Epoch [24], Batch [735], Train Loss: 0.5360060930252075\n",
            "Epoch [24], Batch [736], Train Loss: 0.7938443422317505\n",
            "Epoch [24], Batch [737], Train Loss: 0.7436076402664185\n",
            "Epoch [24], Batch [738], Train Loss: 0.9257001280784607\n",
            "Epoch [24], Batch [739], Train Loss: 0.913989782333374\n",
            "Epoch [24], Batch [740], Train Loss: 0.8191041350364685\n",
            "Epoch [24], Batch [741], Train Loss: 0.8139277696609497\n",
            "Epoch [24], Batch [742], Train Loss: 0.7636166214942932\n",
            "Epoch [24], Batch [743], Train Loss: 0.752230703830719\n",
            "Epoch [24], Batch [744], Train Loss: 0.75185227394104\n",
            "Epoch [24], Batch [745], Train Loss: 0.938055157661438\n",
            "Epoch [24], Batch [746], Train Loss: 0.6600449085235596\n",
            "Epoch [24], Batch [747], Train Loss: 0.7319698333740234\n",
            "Epoch [24], Batch [748], Train Loss: 0.7990022897720337\n",
            "Epoch [24], Batch [749], Train Loss: 0.7603850960731506\n",
            "Epoch [24], Batch [750], Train Loss: 0.8127574324607849\n",
            "Epoch [24], Batch [751], Train Loss: 0.7673236131668091\n",
            "Epoch [24], Batch [752], Train Loss: 0.8658548593521118\n",
            "Epoch [24], Batch [753], Train Loss: 0.9717825651168823\n",
            "Epoch [24], Batch [754], Train Loss: 0.9626626968383789\n",
            "Epoch [24], Batch [755], Train Loss: 0.9366683959960938\n",
            "Epoch [24], Batch [756], Train Loss: 0.6047806739807129\n",
            "Epoch [24], Batch [757], Train Loss: 0.7805258631706238\n",
            "Epoch [24], Batch [758], Train Loss: 0.9138820171356201\n",
            "Epoch [24], Batch [759], Train Loss: 0.6794069409370422\n",
            "Epoch [24], Batch [760], Train Loss: 0.8394728899002075\n",
            "Epoch [24], Batch [761], Train Loss: 0.7139356732368469\n",
            "Epoch [24], Batch [762], Train Loss: 0.7633581161499023\n",
            "Epoch [24], Batch [763], Train Loss: 0.7430302500724792\n",
            "Epoch [24], Batch [764], Train Loss: 0.7409213185310364\n",
            "Epoch [24], Batch [765], Train Loss: 0.6614755392074585\n",
            "Epoch [24], Batch [766], Train Loss: 0.7105804085731506\n",
            "Epoch [24], Batch [767], Train Loss: 0.5634675025939941\n",
            "Epoch [24], Batch [768], Train Loss: 0.8045620918273926\n",
            "Epoch [24], Batch [769], Train Loss: 0.9048700928688049\n",
            "Epoch [24], Batch [770], Train Loss: 0.8769420385360718\n",
            "Epoch [24], Batch [771], Train Loss: 0.8162227869033813\n",
            "Epoch [24], Batch [772], Train Loss: 1.071366548538208\n",
            "Epoch [24], Batch [773], Train Loss: 0.6683926582336426\n",
            "Epoch [24], Batch [774], Train Loss: 0.8289844393730164\n",
            "Epoch [24], Batch [775], Train Loss: 0.7883144021034241\n",
            "Epoch [24], Batch [776], Train Loss: 0.737289547920227\n",
            "Epoch [24], Batch [777], Train Loss: 0.757805585861206\n",
            "Epoch [24], Batch [778], Train Loss: 0.8034396767616272\n",
            "Epoch [24], Batch [779], Train Loss: 0.8825293183326721\n",
            "Epoch [24], Batch [780], Train Loss: 0.5854899883270264\n",
            "Epoch [24], Batch [781], Train Loss: 0.832366943359375\n",
            "Epoch [24], Batch [782], Train Loss: 0.5855017304420471\n",
            "Epoch [24], Batch [783], Train Loss: 0.763183057308197\n",
            "Epoch [24], Batch [784], Train Loss: 0.8914304971694946\n",
            "Epoch [24], Batch [785], Train Loss: 0.7023226618766785\n",
            "Epoch [24], Batch [786], Train Loss: 0.7471208572387695\n",
            "Epoch [24], Batch [787], Train Loss: 0.6744261980056763\n",
            "Epoch [24], Batch [788], Train Loss: 0.8838402032852173\n",
            "Epoch [24], Batch [789], Train Loss: 0.5922548770904541\n",
            "Epoch [24], Batch [790], Train Loss: 0.5572914481163025\n",
            "Epoch [24], Batch [791], Train Loss: 0.7926605343818665\n",
            "Epoch [24], Batch [792], Train Loss: 0.8259524703025818\n",
            "Epoch [24], Batch [793], Train Loss: 0.7659140825271606\n",
            "Epoch [24], Batch [794], Train Loss: 0.7133669257164001\n",
            "Epoch [24], Batch [795], Train Loss: 0.7920866012573242\n",
            "Epoch [24], Batch [796], Train Loss: 0.7226914763450623\n",
            "Epoch [24], Batch [797], Train Loss: 0.7966050505638123\n",
            "Epoch [24], Batch [798], Train Loss: 0.6885113716125488\n",
            "Epoch [24], Batch [799], Train Loss: 0.7062712907791138\n",
            "Epoch [24], Batch [800], Train Loss: 0.810583770275116\n",
            "Epoch [24], Batch [801], Train Loss: 0.5683498978614807\n",
            "Epoch [24], Batch [802], Train Loss: 0.7125859260559082\n",
            "Epoch [24], Batch [803], Train Loss: 0.7666512727737427\n",
            "Epoch [24], Batch [804], Train Loss: 0.5870165824890137\n",
            "Epoch [24], Batch [805], Train Loss: 0.603370189666748\n",
            "Epoch [24], Batch [806], Train Loss: 0.7690267562866211\n",
            "Epoch [24], Batch [807], Train Loss: 0.768668532371521\n",
            "Epoch [24], Batch [808], Train Loss: 0.7482074499130249\n",
            "Epoch [24], Batch [809], Train Loss: 0.7363178133964539\n",
            "Epoch [24], Batch [810], Train Loss: 0.7411388158798218\n",
            "Epoch [24], Batch [811], Train Loss: 0.985075056552887\n",
            "Epoch [24], Batch [812], Train Loss: 1.0789471864700317\n",
            "Epoch [24], Batch [813], Train Loss: 0.7216930389404297\n",
            "Epoch [24], Batch [814], Train Loss: 0.7877585887908936\n",
            "Epoch [24], Batch [815], Train Loss: 0.9629948139190674\n",
            "Epoch [24], Batch [816], Train Loss: 0.9906119704246521\n",
            "Epoch [24], Batch [817], Train Loss: 0.7690633535385132\n",
            "Epoch [24], Batch [818], Train Loss: 0.5517663359642029\n",
            "Epoch [24], Batch [819], Train Loss: 0.62935870885849\n",
            "Epoch [24], Batch [820], Train Loss: 0.5840188264846802\n",
            "Epoch [24], Batch [821], Train Loss: 0.7049053907394409\n",
            "Epoch [24], Batch [822], Train Loss: 0.8488970398902893\n",
            "Epoch [24], Batch [823], Train Loss: 0.8308066129684448\n",
            "Epoch [24], Batch [824], Train Loss: 0.9706590175628662\n",
            "Epoch [24], Batch [825], Train Loss: 0.889158308506012\n",
            "Epoch [24], Batch [826], Train Loss: 0.9616440534591675\n",
            "Epoch [24], Batch [827], Train Loss: 0.8102691769599915\n",
            "Epoch [24], Batch [828], Train Loss: 0.8562138080596924\n",
            "Epoch [24], Batch [829], Train Loss: 0.6954428553581238\n",
            "Epoch [24], Batch [830], Train Loss: 0.8243585228919983\n",
            "Epoch [24], Batch [831], Train Loss: 0.8743547797203064\n",
            "Epoch [24], Batch [832], Train Loss: 0.7777275443077087\n",
            "Epoch [24], Batch [833], Train Loss: 0.8635561466217041\n",
            "Epoch [24], Batch [834], Train Loss: 1.1161377429962158\n",
            "Epoch [24], Batch [835], Train Loss: 0.7500931024551392\n",
            "Epoch [24], Batch [836], Train Loss: 0.9009233117103577\n",
            "Epoch [24], Batch [837], Train Loss: 0.7172657251358032\n",
            "Epoch [24], Batch [838], Train Loss: 0.7411977648735046\n",
            "Epoch [24], Batch [839], Train Loss: 0.9152047634124756\n",
            "Epoch [24], Batch [840], Train Loss: 0.8329941034317017\n",
            "Epoch [24], Batch [841], Train Loss: 0.8186690211296082\n",
            "Epoch [24], Batch [842], Train Loss: 0.8315353393554688\n",
            "Epoch [24], Batch [843], Train Loss: 0.8594432473182678\n",
            "Epoch [24], Batch [844], Train Loss: 0.8336512446403503\n",
            "Epoch [24], Batch [845], Train Loss: 0.9795513153076172\n",
            "Epoch [24], Batch [846], Train Loss: 0.863381564617157\n",
            "Epoch [24], Batch [847], Train Loss: 0.9132810235023499\n",
            "Epoch [24], Batch [848], Train Loss: 0.8204900026321411\n",
            "Epoch [24], Batch [849], Train Loss: 0.687622606754303\n",
            "Epoch [24], Batch [850], Train Loss: 0.7878210544586182\n",
            "Epoch [24], Batch [851], Train Loss: 0.8522042036056519\n",
            "Epoch [24], Batch [852], Train Loss: 0.7640085816383362\n",
            "Epoch [24], Batch [853], Train Loss: 0.787295937538147\n",
            "Epoch [24], Batch [854], Train Loss: 0.8932240009307861\n",
            "Epoch [24], Batch [855], Train Loss: 0.8289464712142944\n",
            "Epoch [24], Batch [856], Train Loss: 0.7579845190048218\n",
            "Epoch [24], Batch [857], Train Loss: 0.7642027139663696\n",
            "Epoch [24], Batch [858], Train Loss: 0.7305355668067932\n",
            "Epoch [24], Batch [859], Train Loss: 0.7225542068481445\n",
            "Epoch [24], Batch [860], Train Loss: 0.7497337460517883\n",
            "Epoch [24], Batch [861], Train Loss: 0.8085305690765381\n",
            "Epoch [24], Batch [862], Train Loss: 0.710931122303009\n",
            "Epoch [24], Batch [863], Train Loss: 0.6868129968643188\n",
            "Epoch [24], Batch [864], Train Loss: 0.830203652381897\n",
            "Epoch [24], Batch [865], Train Loss: 0.688438355922699\n",
            "Epoch [24], Batch [866], Train Loss: 0.6666381359100342\n",
            "Epoch [24], Batch [867], Train Loss: 0.7158575057983398\n",
            "Epoch [24], Batch [868], Train Loss: 0.8472748398780823\n",
            "Epoch [24], Batch [869], Train Loss: 0.8202043771743774\n",
            "Epoch [24], Batch [870], Train Loss: 0.5127888321876526\n",
            "Epoch [24], Batch [871], Train Loss: 1.0314037799835205\n",
            "Epoch [24], Batch [872], Train Loss: 0.6724709868431091\n",
            "Epoch [24], Batch [873], Train Loss: 0.8647015690803528\n",
            "Epoch [24], Batch [874], Train Loss: 0.622384786605835\n",
            "Epoch [24], Batch [875], Train Loss: 0.8562898635864258\n",
            "Epoch [24], Batch [876], Train Loss: 0.8215243816375732\n",
            "Epoch [24], Batch [877], Train Loss: 0.7745134830474854\n",
            "Epoch [24], Batch [878], Train Loss: 0.7760432958602905\n",
            "Epoch [24], Batch [879], Train Loss: 0.9111146330833435\n",
            "Epoch [24], Batch [880], Train Loss: 0.7230525612831116\n",
            "Epoch [24], Batch [881], Train Loss: 0.6323087215423584\n",
            "Epoch [24], Batch [882], Train Loss: 0.679479718208313\n",
            "Epoch [24], Batch [883], Train Loss: 0.7291544079780579\n",
            "Epoch [24], Batch [884], Train Loss: 0.6931852698326111\n",
            "Epoch [24], Batch [885], Train Loss: 0.6490637063980103\n",
            "Epoch [24], Batch [886], Train Loss: 0.780556857585907\n",
            "Epoch [24], Batch [887], Train Loss: 0.6998511552810669\n",
            "Epoch [24], Batch [888], Train Loss: 0.8164363503456116\n",
            "Epoch [24], Batch [889], Train Loss: 0.6858609914779663\n",
            "Epoch [24], Batch [890], Train Loss: 0.6794501543045044\n",
            "Epoch [24], Batch [891], Train Loss: 0.8277602195739746\n",
            "Epoch [24], Batch [892], Train Loss: 0.6490612030029297\n",
            "Epoch [24], Batch [893], Train Loss: 0.8471216559410095\n",
            "Epoch [24], Batch [894], Train Loss: 0.8262783288955688\n",
            "Epoch [24], Batch [895], Train Loss: 0.8857042193412781\n",
            "Epoch [24], Batch [896], Train Loss: 0.7208939790725708\n",
            "Epoch [24], Batch [897], Train Loss: 0.8322305679321289\n",
            "Epoch [24], Batch [898], Train Loss: 0.6546213626861572\n",
            "Epoch [24], Batch [899], Train Loss: 0.7910451292991638\n",
            "Epoch [24], Batch [900], Train Loss: 0.6264076232910156\n",
            "Epoch [24], Batch [901], Train Loss: 0.7386588454246521\n",
            "Epoch [24], Batch [902], Train Loss: 0.8734046816825867\n",
            "Epoch [24], Batch [903], Train Loss: 0.7731914520263672\n",
            "Epoch [24], Batch [904], Train Loss: 0.9293847680091858\n",
            "Epoch [24], Batch [905], Train Loss: 0.7335763573646545\n",
            "Epoch [24], Batch [906], Train Loss: 0.6931746602058411\n",
            "Epoch [24], Batch [907], Train Loss: 0.8600694537162781\n",
            "Epoch [24], Batch [908], Train Loss: 0.7682154178619385\n",
            "Epoch [24], Batch [909], Train Loss: 0.7297548055648804\n",
            "Epoch [24], Batch [910], Train Loss: 0.8857738375663757\n",
            "Epoch [24], Batch [911], Train Loss: 0.718948483467102\n",
            "Epoch [24], Batch [912], Train Loss: 0.7537072896957397\n",
            "Epoch [24], Batch [913], Train Loss: 1.0404832363128662\n",
            "Epoch [24], Batch [914], Train Loss: 0.6216776967048645\n",
            "Epoch [24], Batch [915], Train Loss: 0.8051307797431946\n",
            "Epoch [24], Batch [916], Train Loss: 0.8174784183502197\n",
            "Epoch [24], Batch [917], Train Loss: 0.6454564929008484\n",
            "Epoch [24], Batch [918], Train Loss: 0.9848387241363525\n",
            "Epoch [24], Batch [919], Train Loss: 0.847149133682251\n",
            "Epoch [24], Batch [920], Train Loss: 0.6529033184051514\n",
            "Epoch [24], Batch [921], Train Loss: 0.840041995048523\n",
            "Epoch [24], Batch [922], Train Loss: 0.9445123672485352\n",
            "Epoch [24], Batch [923], Train Loss: 0.7159298062324524\n",
            "Epoch [24], Batch [924], Train Loss: 0.7255061268806458\n",
            "Epoch [24], Batch [925], Train Loss: 0.8032961487770081\n",
            "Epoch [24], Batch [926], Train Loss: 0.8306788206100464\n",
            "Epoch [24], Batch [927], Train Loss: 0.8544967174530029\n",
            "Epoch [24], Batch [928], Train Loss: 0.8001659512519836\n",
            "Epoch [24], Batch [929], Train Loss: 0.7435992360115051\n",
            "Epoch [24], Batch [930], Train Loss: 0.8061435222625732\n",
            "Epoch [24], Batch [931], Train Loss: 0.7761601805686951\n",
            "Epoch [24], Batch [932], Train Loss: 0.6663817167282104\n",
            "Epoch [24], Batch [933], Train Loss: 0.8219988942146301\n",
            "Epoch [24], Batch [934], Train Loss: 0.7310953140258789\n",
            "Epoch [24], Batch [935], Train Loss: 0.7191537618637085\n",
            "Epoch [24], Batch [936], Train Loss: 0.6085011959075928\n",
            "Epoch [24], Batch [937], Train Loss: 0.693792998790741\n",
            "Epoch [24], Batch [938], Train Loss: 0.8921250104904175\n",
            "Accuracy of train set: 0.7082666666666667\n",
            "Epoch [25], Batch [1], Loss: 0.7717088460922241\n",
            "Epoch [25], Batch [2], Loss: 0.59391188621521\n",
            "Epoch [25], Batch [3], Loss: 0.6638227105140686\n",
            "Epoch [25], Batch [4], Loss: 0.7131292819976807\n",
            "Epoch [25], Batch [5], Loss: 0.7157500982284546\n",
            "Epoch [25], Batch [6], Loss: 0.7053964734077454\n",
            "Epoch [25], Batch [7], Loss: 0.7532961368560791\n",
            "Epoch [25], Batch [8], Loss: 0.8129020929336548\n",
            "Epoch [25], Batch [9], Loss: 0.7749795317649841\n",
            "Epoch [25], Batch [10], Loss: 0.9069962501525879\n",
            "Epoch [25], Batch [11], Loss: 0.9188323020935059\n",
            "Epoch [25], Batch [12], Loss: 0.7787869572639465\n",
            "Epoch [25], Batch [13], Loss: 0.604513943195343\n",
            "Epoch [25], Batch [14], Loss: 0.6452773809432983\n",
            "Epoch [25], Batch [15], Loss: 0.7008557319641113\n",
            "Epoch [25], Batch [16], Loss: 0.9007400274276733\n",
            "Epoch [25], Batch [17], Loss: 0.7030325531959534\n",
            "Epoch [25], Batch [18], Loss: 0.9501941204071045\n",
            "Epoch [25], Batch [19], Loss: 0.8348561525344849\n",
            "Epoch [25], Batch [20], Loss: 0.8805795907974243\n",
            "Epoch [25], Batch [21], Loss: 0.9258120059967041\n",
            "Epoch [25], Batch [22], Loss: 0.7668499946594238\n",
            "Epoch [25], Batch [23], Loss: 0.6731593608856201\n",
            "Epoch [25], Batch [24], Loss: 0.6864631175994873\n",
            "Epoch [25], Batch [25], Loss: 0.7252794504165649\n",
            "Epoch [25], Batch [26], Loss: 0.8133519291877747\n",
            "Epoch [25], Batch [27], Loss: 0.737657368183136\n",
            "Epoch [25], Batch [28], Loss: 0.8426166772842407\n",
            "Epoch [25], Batch [29], Loss: 0.8483773469924927\n",
            "Epoch [25], Batch [30], Loss: 0.4552437961101532\n",
            "Epoch [25], Batch [31], Loss: 0.8881640434265137\n",
            "Epoch [25], Batch [32], Loss: 0.6630568504333496\n",
            "Epoch [25], Batch [33], Loss: 0.8620837926864624\n",
            "Epoch [25], Batch [34], Loss: 0.6871909499168396\n",
            "Epoch [25], Batch [35], Loss: 0.6577562689781189\n",
            "Epoch [25], Batch [36], Loss: 0.6875284910202026\n",
            "Epoch [25], Batch [37], Loss: 0.650495707988739\n",
            "Epoch [25], Batch [38], Loss: 0.7118762135505676\n",
            "Epoch [25], Batch [39], Loss: 0.7869929075241089\n",
            "Epoch [25], Batch [40], Loss: 0.7450194358825684\n",
            "Epoch [25], Batch [41], Loss: 0.8151063323020935\n",
            "Epoch [25], Batch [42], Loss: 0.918865978717804\n",
            "Epoch [25], Batch [43], Loss: 0.9319833517074585\n",
            "Epoch [25], Batch [44], Loss: 0.6553477048873901\n",
            "Epoch [25], Batch [45], Loss: 0.9028435945510864\n",
            "Epoch [25], Batch [46], Loss: 1.0597883462905884\n",
            "Epoch [25], Batch [47], Loss: 0.7918875813484192\n",
            "Epoch [25], Batch [48], Loss: 0.9897220134735107\n",
            "Epoch [25], Batch [49], Loss: 0.8688681125640869\n",
            "Epoch [25], Batch [50], Loss: 0.7061013579368591\n",
            "Epoch [25], Batch [51], Loss: 1.1118782758712769\n",
            "Epoch [25], Batch [52], Loss: 0.890425980091095\n",
            "Epoch [25], Batch [53], Loss: 0.7013134360313416\n",
            "Epoch [25], Batch [54], Loss: 0.8862637281417847\n",
            "Epoch [25], Batch [55], Loss: 1.0635825395584106\n",
            "Epoch [25], Batch [56], Loss: 0.767059326171875\n",
            "Epoch [25], Batch [57], Loss: 0.8730828166007996\n",
            "Epoch [25], Batch [58], Loss: 0.9355112314224243\n",
            "Epoch [25], Batch [59], Loss: 0.7894802093505859\n",
            "Epoch [25], Batch [60], Loss: 0.9586941003799438\n",
            "Epoch [25], Batch [61], Loss: 0.77571702003479\n",
            "Epoch [25], Batch [62], Loss: 1.1552882194519043\n",
            "Epoch [25], Batch [63], Loss: 0.7638400197029114\n",
            "Epoch [25], Batch [64], Loss: 0.9669171571731567\n",
            "Epoch [25], Batch [65], Loss: 0.962785542011261\n",
            "Epoch [25], Batch [66], Loss: 0.7887314558029175\n",
            "Epoch [25], Batch [67], Loss: 0.8629467487335205\n",
            "Epoch [25], Batch [68], Loss: 0.624576985836029\n",
            "Epoch [25], Batch [69], Loss: 0.8700989484786987\n",
            "Epoch [25], Batch [70], Loss: 0.5329998135566711\n",
            "Epoch [25], Batch [71], Loss: 0.5674238204956055\n",
            "Epoch [25], Batch [72], Loss: 0.5356144309043884\n",
            "Epoch [25], Batch [73], Loss: 0.7913055419921875\n",
            "Epoch [25], Batch [74], Loss: 0.8849426507949829\n",
            "Epoch [25], Batch [75], Loss: 0.7131154537200928\n",
            "Epoch [25], Batch [76], Loss: 0.8808388710021973\n",
            "Epoch [25], Batch [77], Loss: 0.827613890171051\n",
            "Epoch [25], Batch [78], Loss: 0.6882575750350952\n",
            "Epoch [25], Batch [79], Loss: 1.0479233264923096\n",
            "Epoch [25], Batch [80], Loss: 0.8614840507507324\n",
            "Epoch [25], Batch [81], Loss: 0.9955824017524719\n",
            "Epoch [25], Batch [82], Loss: 0.7399876713752747\n",
            "Epoch [25], Batch [83], Loss: 0.7055762410163879\n",
            "Epoch [25], Batch [84], Loss: 0.7359024882316589\n",
            "Epoch [25], Batch [85], Loss: 0.7216565608978271\n",
            "Epoch [25], Batch [86], Loss: 0.8519617319107056\n",
            "Epoch [25], Batch [87], Loss: 0.9482889771461487\n",
            "Epoch [25], Batch [88], Loss: 0.9702150821685791\n",
            "Epoch [25], Batch [89], Loss: 0.8869017958641052\n",
            "Epoch [25], Batch [90], Loss: 0.7508150339126587\n",
            "Epoch [25], Batch [91], Loss: 0.7831910848617554\n",
            "Epoch [25], Batch [92], Loss: 0.6523439884185791\n",
            "Epoch [25], Batch [93], Loss: 0.5606439113616943\n",
            "Epoch [25], Batch [94], Loss: 0.8886735439300537\n",
            "Epoch [25], Batch [95], Loss: 0.6677645444869995\n",
            "Epoch [25], Batch [96], Loss: 0.8697009086608887\n",
            "Epoch [25], Batch [97], Loss: 0.6876263618469238\n",
            "Epoch [25], Batch [98], Loss: 0.7957167625427246\n",
            "Epoch [25], Batch [99], Loss: 0.977485716342926\n",
            "Epoch [25], Batch [100], Loss: 0.6448709964752197\n",
            "Epoch [25], Batch [101], Loss: 0.8040149211883545\n",
            "Epoch [25], Batch [102], Loss: 0.812404990196228\n",
            "Epoch [25], Batch [103], Loss: 0.7035530805587769\n",
            "Epoch [25], Batch [104], Loss: 0.756641149520874\n",
            "Epoch [25], Batch [105], Loss: 1.080391764640808\n",
            "Epoch [25], Batch [106], Loss: 0.6004154086112976\n",
            "Epoch [25], Batch [107], Loss: 0.8368455767631531\n",
            "Epoch [25], Batch [108], Loss: 0.754940927028656\n",
            "Epoch [25], Batch [109], Loss: 0.8529672026634216\n",
            "Epoch [25], Batch [110], Loss: 0.7872471213340759\n",
            "Epoch [25], Batch [111], Loss: 0.7181602120399475\n",
            "Epoch [25], Batch [112], Loss: 0.560330331325531\n",
            "Epoch [25], Batch [113], Loss: 0.8785830736160278\n",
            "Epoch [25], Batch [114], Loss: 0.7783527374267578\n",
            "Epoch [25], Batch [115], Loss: 0.7378408908843994\n",
            "Epoch [25], Batch [116], Loss: 0.7479750514030457\n",
            "Epoch [25], Batch [117], Loss: 0.7467237710952759\n",
            "Epoch [25], Batch [118], Loss: 0.7488831281661987\n",
            "Epoch [25], Batch [119], Loss: 0.7778973579406738\n",
            "Epoch [25], Batch [120], Loss: 0.701640784740448\n",
            "Epoch [25], Batch [121], Loss: 0.7645853161811829\n",
            "Epoch [25], Batch [122], Loss: 0.6481785774230957\n",
            "Epoch [25], Batch [123], Loss: 0.8078134059906006\n",
            "Epoch [25], Batch [124], Loss: 1.1138505935668945\n",
            "Epoch [25], Batch [125], Loss: 0.8162446022033691\n",
            "Epoch [25], Batch [126], Loss: 0.8524425029754639\n",
            "Epoch [25], Batch [127], Loss: 0.6340306997299194\n",
            "Epoch [25], Batch [128], Loss: 0.8258368968963623\n",
            "Epoch [25], Batch [129], Loss: 0.8604137301445007\n",
            "Epoch [25], Batch [130], Loss: 0.8427144289016724\n",
            "Epoch [25], Batch [131], Loss: 0.6318187117576599\n",
            "Epoch [25], Batch [132], Loss: 0.6194166541099548\n",
            "Epoch [25], Batch [133], Loss: 0.6774585247039795\n",
            "Epoch [25], Batch [134], Loss: 0.7756830453872681\n",
            "Epoch [25], Batch [135], Loss: 0.762322187423706\n",
            "Epoch [25], Batch [136], Loss: 0.6911042332649231\n",
            "Epoch [25], Batch [137], Loss: 0.9654159545898438\n",
            "Epoch [25], Batch [138], Loss: 0.7244361042976379\n",
            "Epoch [25], Batch [139], Loss: 0.8130767345428467\n",
            "Epoch [25], Batch [140], Loss: 0.8519347310066223\n",
            "Epoch [25], Batch [141], Loss: 0.6686894297599792\n",
            "Epoch [25], Batch [142], Loss: 0.847763180732727\n",
            "Epoch [25], Batch [143], Loss: 0.8816734552383423\n",
            "Epoch [25], Batch [144], Loss: 0.8844267725944519\n",
            "Epoch [25], Batch [145], Loss: 0.8120701909065247\n",
            "Epoch [25], Batch [146], Loss: 0.7859492301940918\n",
            "Epoch [25], Batch [147], Loss: 0.7846372127532959\n",
            "Epoch [25], Batch [148], Loss: 0.6296805143356323\n",
            "Epoch [25], Batch [149], Loss: 0.9755035638809204\n",
            "Epoch [25], Batch [150], Loss: 1.0049481391906738\n",
            "Epoch [25], Batch [151], Loss: 1.053709864616394\n",
            "Epoch [25], Batch [152], Loss: 0.6027982234954834\n",
            "Epoch [25], Batch [153], Loss: 0.6704596281051636\n",
            "Epoch [25], Batch [154], Loss: 0.6863725185394287\n",
            "Epoch [25], Batch [155], Loss: 1.1042110919952393\n",
            "Epoch [25], Batch [156], Loss: 0.7626133561134338\n",
            "Epoch [25], Batch [157], Loss: 0.6861423254013062\n",
            "Accuracy of None set: 0.7069\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "train_model([train_loader, test_loader], num_epochs=25, learning_rate=0.005)"
      ],
      "id": "2ec4bdd2"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ceb5783f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fc76677-1ea1-4914-ddb4-7def96ada152"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final test accuracy: 0.7069\n"
          ]
        }
      ],
      "source": [
        "print(f'Final test accuracy: {test_accuracies[-1]}')"
      ],
      "id": "ceb5783f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5e128ed"
      },
      "source": [
        "## Visualization of the labels and predictions\n",
        "\n",
        "In this section, you should visual one image from each class and show both the actual label and the predicted label for that image."
      ],
      "id": "a5e128ed"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "6c0b79fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "outputId": "9e6618d3-c1bf-4c00-e02a-e7206c99cdfa"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "## FILL HERE\n",
        "\n",
        "def visualize_predictions(loader):\n",
        "\n",
        "  true_labels = []\n",
        "  predicted_labels = []\n",
        "  added_labels = []\n",
        "\n",
        "  for images, labels in loader:\n",
        "\n",
        "    images = images.to(device)\n",
        "    true_labels = labels.to(device).cpu().numpy()\n",
        "\n",
        "    outputs = model(images, parameters)\n",
        "    predicted_labels = outputs.argmax(dim=-1).cpu().numpy()\n",
        "\n",
        "    for image, true_label, pred_label in zip(images, true_labels, predicted_labels):\n",
        "\n",
        "      if(true_label not in added_labels):\n",
        "\n",
        "        added_labels.append(true_label)\n",
        "\n",
        "        plt.subplot(2, 5, true_label+1)\n",
        "        plt.imshow(image.squeeze(), cmap='gray')\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.title(f\"Actual: {FashionMNIST.classes[true_label]}\\nPredicted: {FashionMNIST.classes[pred_label]}\", fontsize=7)\n",
        "\n",
        "        if (len(added_labels) == num_classes):\n",
        "          break\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "visualize_predictions(test_loader)\n"
      ],
      "id": "6c0b79fd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQcZ9ka3SuYS"
      },
      "source": [
        "First of all we implement a function performing the visualization and then call the function on the test loader. \n",
        "\n",
        "The function iterates over the loader, stores the true labels, calculates the output of the model for the input images and decides the predicted labels based on the argmax of the outputs in the same way as before.\n",
        "\n",
        "In the next step, for each image and its corresponding true and predicted label (using \"zip\" command), we plot the image with the labels written above. \n",
        "\n",
        "This process continues until we gathered one image for each class. "
      ],
      "id": "UQcZ9ka3SuYS"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}